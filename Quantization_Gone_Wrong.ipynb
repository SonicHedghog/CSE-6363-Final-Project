{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNFFvla6dHf6RfabS8/8Hsz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bde9bbe5c9743cbb6ac995414d92870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51c191ca2630444fb2d9cbb0c6f4b2d6",
              "IPY_MODEL_c5d07241edb34790b8d3b4ef8f02bc8d",
              "IPY_MODEL_37acfbd1090447aea88c9337de9520e3"
            ],
            "layout": "IPY_MODEL_755bb639a74f4fddbaba555c3d737e52"
          }
        },
        "51c191ca2630444fb2d9cbb0c6f4b2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_850d4fe63c5e498ea82533f93b5b2e55",
            "placeholder": "​",
            "style": "IPY_MODEL_f7ae5d2d5a1c443caf68386cca0b3b72",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c5d07241edb34790b8d3b4ef8f02bc8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401a882d7e474f5c9d9a0d5f13f969af",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35e12d1c8f144a9bacc94c2fdc050384",
            "value": 2
          }
        },
        "37acfbd1090447aea88c9337de9520e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dffc32816342417fa7b3166e62cd191e",
            "placeholder": "​",
            "style": "IPY_MODEL_5aa08bca20c6481090dcfc01787172ca",
            "value": " 2/2 [00:02&lt;00:00,  1.03it/s]"
          }
        },
        "755bb639a74f4fddbaba555c3d737e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "850d4fe63c5e498ea82533f93b5b2e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ae5d2d5a1c443caf68386cca0b3b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "401a882d7e474f5c9d9a0d5f13f969af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e12d1c8f144a9bacc94c2fdc050384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dffc32816342417fa7b3166e62cd191e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa08bca20c6481090dcfc01787172ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2da904b281a54783a09060819660bd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4c40dc5ad6a4d2da5add0f3dfab9b43",
              "IPY_MODEL_eaa490da16b747e78d112041f1f1205b",
              "IPY_MODEL_1abcea551a734f8eb7f19444b30e2f42"
            ],
            "layout": "IPY_MODEL_8301b35f833e49cfa2ee6795629978bf"
          }
        },
        "c4c40dc5ad6a4d2da5add0f3dfab9b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc89514bc48a48cd82fbb025a1aef352",
            "placeholder": "​",
            "style": "IPY_MODEL_26c2ec7a86bb464a9eb833c30a45e9dd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "eaa490da16b747e78d112041f1f1205b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5d629beda964fec8b09891a8266e923",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49db806bb4eb428f9b7c7eb56edeb3e7",
            "value": 2
          }
        },
        "1abcea551a734f8eb7f19444b30e2f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3626151f924b46cc8d292ab4e83fdfd8",
            "placeholder": "​",
            "style": "IPY_MODEL_f613a12c19574134989b6e2fe4319ce6",
            "value": " 2/2 [00:01&lt;00:00,  1.33it/s]"
          }
        },
        "8301b35f833e49cfa2ee6795629978bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc89514bc48a48cd82fbb025a1aef352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c2ec7a86bb464a9eb833c30a45e9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5d629beda964fec8b09891a8266e923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49db806bb4eb428f9b7c7eb56edeb3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3626151f924b46cc8d292ab4e83fdfd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f613a12c19574134989b6e2fe4319ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SonicHedghog/CSE-6363-Final-Project/blob/main/Quantization_Gone_Wrong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hho6Zuxu_frC",
        "outputId": "1ade05e1-1b53-40c7-b133-51f72755d898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum[exporters-gpu]\n",
            "  Downloading optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (4.54.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (0.34.1)\n",
            "Collecting onnx (from optimum[exporters-gpu])\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime-gpu (from optimum[exporters-gpu])\n",
            "  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[exporters-gpu]) (5.29.5)\n",
            "Collecting transformers>=4.29 (from optimum[exporters-gpu])\n",
            "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[exporters-gpu]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum[exporters-gpu])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[exporters-gpu]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum[exporters-gpu]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters-gpu]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters-gpu]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[exporters-gpu]) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime-gpu->optimum[exporters-gpu])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu->optimum[exporters-gpu]) (25.2.10)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->optimum[exporters-gpu])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[exporters-gpu]) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[exporters-gpu]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[exporters-gpu]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[exporters-gpu]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[exporters-gpu]) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.27.0-py3-none-any.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime-gpu, nvidia-cusolver-cu12, transformers, optimum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 onnxruntime-gpu-1.22.0 optimum-1.27.0 transformers-4.53.3\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade optimum[exporters-gpu]\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "class LocalGemmaChatBot:\n",
        "    \"\"\"\n",
        "    A chatbot that plays Rock, Paper, Scissors using a locally-run Gemma 2 9B model.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the LocalGemmaChatBot.\n",
        "        This will download the model (if not cached) and load it into memory.\n",
        "        \"\"\"\n",
        "        print(\"Initializing local Gemma model...\")\n",
        "        print(\"This may take a while and will download gigabytes of data the first time.\")\n",
        "\n",
        "        # The model ID for the instruction-tuned Gemma 2 9B model\n",
        "        self.model_id = \"./rps-gemma2-quantized\"\n",
        "        self.dtype = torch.bfloat16 # Use bfloat16 for better performance\n",
        "\n",
        "        # Load the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "\n",
        "        # Load the model\n",
        "        # device_map=\"auto\" will automatically use a GPU if it's available\n",
        "        self.game_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model_id, # Use the in-memory quantized model\n",
        "            tokenizer=self.tokenizer,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # The system prompt that defines the bot's behavior\n",
        "        self.system_prompt = \"\"\"You are a rock-paper-scissors game.\n",
        "\n",
        "Please do the following:\n",
        "1. First, check if the user wants to exit the game (they might say \"exit\", \"quit\", \"stop\", \"bye\", \"goodbye\", or similar)\n",
        "- If they want to exit, respond with exactly: \"EXIT_GAME_TOKEN\"\n",
        "2. Validate the user's choice (rock, paper, or scissors).\n",
        "- If the user enters an invalid choice (not rock/paper/scissors and not wanting to exit), respond with an error message asking them to choose rock, paper, scissors, or exit.\n",
        "3. If it's a valid game choice, choose your own move randomly (rock, paper, or scissors)\n",
        "4. Determine who wins based on the rules:\n",
        "- Rock beats scissors\n",
        "- Paper beats rock\n",
        "- Scissors beats paper\n",
        "- Same choice = tie\n",
        "\n",
        "For valid game moves, format your response like this:\n",
        "My choice: [your choice]\n",
        "Result: [who won and why]\n",
        "\"\"\"\n",
        "        # We store the conversation history, starting with the system prompt\n",
        "        self.chat_history = [{\"role\": \"user\", \"content\": self.system_prompt}, {\"role\": \"model\", \"content\": \"I am ready to play Rock, Paper, Scissors! What is your choice?\"}]\n",
        "        print(\"\\nModel ready!\")\n",
        "\n",
        "\n",
        "    def get_local_gemma_response(self, user_prompt):\n",
        "        \"\"\"\n",
        "        Generates a response from the local Gemma model.\n",
        "\n",
        "        Args:\n",
        "            user_prompt (str): The user's input.\n",
        "\n",
        "        Returns:\n",
        "            str: The model's response.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Add the user's new message to the history\n",
        "            self.chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "            # Apply the chat template to format the history for the model\n",
        "            # This turns the list of roles/content into a single string the model understands\n",
        "            prompt_for_model = self.tokenizer.apply_chat_template(\n",
        "                self.chat_history,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            # Tokenize the formatted prompt and move it to the model's device (CPU/GPU)\n",
        "            inputs = self.tokenizer.encode(prompt_for_model, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.model.device)\n",
        "\n",
        "            # Generate a response\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=150 # Limit the length of the response\n",
        "            )\n",
        "\n",
        "            # Decode the response, but only the new part\n",
        "            response_text = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "            # Add the model's response to the history for the next turn\n",
        "            self.chat_history.append({\"role\": \"model\", \"content\": response_text})\n",
        "\n",
        "            return response_text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting local Gemma response: {e}\")\n",
        "            return \"I'm having trouble thinking. Let's try again!\"\n",
        "\n",
        "    def play_game(self, user_choice):\n",
        "      \"\"\"\n",
        "      Function to interact with the finetuned model using the correct chat format.\n",
        "      \"\"\"\n",
        "      # Create a structured list of messages. This is the correct way.\n",
        "      self.messages = [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f\"{self.system_prompt}\\n\\nYour choice is: {user_choice}\"\n",
        "          }\n",
        "      ]\n",
        "\n",
        "      # The pipeline will automatically apply the chat template to 'messages'\n",
        "      sequences = self.game_pipeline(\n",
        "          self.messages,\n",
        "          max_new_tokens=50,\n",
        "          do_sample=False,\n",
        "          num_return_sequences=1,\n",
        "      )\n",
        "\n",
        "      # The output format from the pipeline is slightly different\n",
        "      # It returns the full conversation including the prompt\n",
        "      full_conversation = sequences[0]['generated_text']\n",
        "      # The model's response is the last message in the list\n",
        "      model_response = full_conversation[-1]['content']\n",
        "      return model_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bot = LocalGemmaChatBot()\n",
        "# bot.play()\n",
        "\n",
        "while True:\n",
        "    player_move = input(\"\\nYour move: \").strip().lower()\n",
        "\n",
        "    response = bot.play_game(player_move)\n",
        "\n",
        "    if \"EXIT_GAME_TOKEN\" in response:\n",
        "        print(\"\\nBot: Thanks for playing! Goodbye.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\nBot:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875,
          "referenced_widgets": [
            "3bde9bbe5c9743cbb6ac995414d92870",
            "51c191ca2630444fb2d9cbb0c6f4b2d6",
            "c5d07241edb34790b8d3b4ef8f02bc8d",
            "37acfbd1090447aea88c9337de9520e3",
            "755bb639a74f4fddbaba555c3d737e52",
            "850d4fe63c5e498ea82533f93b5b2e55",
            "f7ae5d2d5a1c443caf68386cca0b3b72",
            "401a882d7e474f5c9d9a0d5f13f969af",
            "35e12d1c8f144a9bacc94c2fdc050384",
            "dffc32816342417fa7b3166e62cd191e",
            "5aa08bca20c6481090dcfc01787172ca"
          ]
        },
        "id": "wuCLO244AnCU",
        "outputId": "e9de3558-1665-4134-da76-1c0dfa9a6eba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing local Gemma model...\n",
            "This may take a while and will download gigabytes of data the first time.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bde9bbe5c9743cbb6ac995414d92870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./rps-gemma2-quantized were not used when initializing Gemma3ForConditionalGeneration: ['lm_head.input_scale', 'lm_head.output_scale', 'lm_head.weight._data._data', 'lm_head.weight._scale', 'lm_head.weight._shift', 'model.language_model.layers.0.mlp.down_proj.input_scale', 'model.language_model.layers.0.mlp.down_proj.output_scale', 'model.language_model.layers.0.mlp.down_proj.weight._data._data', 'model.language_model.layers.0.mlp.down_proj.weight._scale', 'model.language_model.layers.0.mlp.down_proj.weight._shift', 'model.language_model.layers.0.mlp.gate_proj.input_scale', 'model.language_model.layers.0.mlp.gate_proj.output_scale', 'model.language_model.layers.0.mlp.gate_proj.weight._data._data', 'model.language_model.layers.0.mlp.gate_proj.weight._scale', 'model.language_model.layers.0.mlp.gate_proj.weight._shift', 'model.language_model.layers.0.mlp.up_proj.input_scale', 'model.language_model.layers.0.mlp.up_proj.output_scale', 'model.language_model.layers.0.mlp.up_proj.weight._data._data', 'model.language_model.layers.0.mlp.up_proj.weight._scale', 'model.language_model.layers.0.mlp.up_proj.weight._shift', 'model.language_model.layers.0.self_attn.k_proj.input_scale', 'model.language_model.layers.0.self_attn.k_proj.output_scale', 'model.language_model.layers.0.self_attn.k_proj.weight._data._data', 'model.language_model.layers.0.self_attn.k_proj.weight._scale', 'model.language_model.layers.0.self_attn.k_proj.weight._shift', 'model.language_model.layers.0.self_attn.o_proj.input_scale', 'model.language_model.layers.0.self_attn.o_proj.output_scale', 'model.language_model.layers.0.self_attn.o_proj.weight._data._data', 'model.language_model.layers.0.self_attn.o_proj.weight._scale', 'model.language_model.layers.0.self_attn.o_proj.weight._shift', 'model.language_model.layers.0.self_attn.q_proj.input_scale', 'model.language_model.layers.0.self_attn.q_proj.output_scale', 'model.language_model.layers.0.self_attn.q_proj.weight._data._data', 'model.language_model.layers.0.self_attn.q_proj.weight._scale', 'model.language_model.layers.0.self_attn.q_proj.weight._shift', 'model.language_model.layers.0.self_attn.v_proj.input_scale', 'model.language_model.layers.0.self_attn.v_proj.output_scale', 'model.language_model.layers.0.self_attn.v_proj.weight._data._data', 'model.language_model.layers.0.self_attn.v_proj.weight._scale', 'model.language_model.layers.0.self_attn.v_proj.weight._shift', 'model.language_model.layers.1.mlp.down_proj.input_scale', 'model.language_model.layers.1.mlp.down_proj.output_scale', 'model.language_model.layers.1.mlp.down_proj.weight._data._data', 'model.language_model.layers.1.mlp.down_proj.weight._scale', 'model.language_model.layers.1.mlp.down_proj.weight._shift', 'model.language_model.layers.1.mlp.gate_proj.input_scale', 'model.language_model.layers.1.mlp.gate_proj.output_scale', 'model.language_model.layers.1.mlp.gate_proj.weight._data._data', 'model.language_model.layers.1.mlp.gate_proj.weight._scale', 'model.language_model.layers.1.mlp.gate_proj.weight._shift', 'model.language_model.layers.1.mlp.up_proj.input_scale', 'model.language_model.layers.1.mlp.up_proj.output_scale', 'model.language_model.layers.1.mlp.up_proj.weight._data._data', 'model.language_model.layers.1.mlp.up_proj.weight._scale', 'model.language_model.layers.1.mlp.up_proj.weight._shift', 'model.language_model.layers.1.self_attn.k_proj.input_scale', 'model.language_model.layers.1.self_attn.k_proj.output_scale', 'model.language_model.layers.1.self_attn.k_proj.weight._data._data', 'model.language_model.layers.1.self_attn.k_proj.weight._scale', 'model.language_model.layers.1.self_attn.k_proj.weight._shift', 'model.language_model.layers.1.self_attn.o_proj.input_scale', 'model.language_model.layers.1.self_attn.o_proj.output_scale', 'model.language_model.layers.1.self_attn.o_proj.weight._data._data', 'model.language_model.layers.1.self_attn.o_proj.weight._scale', 'model.language_model.layers.1.self_attn.o_proj.weight._shift', 'model.language_model.layers.1.self_attn.q_proj.input_scale', 'model.language_model.layers.1.self_attn.q_proj.output_scale', 'model.language_model.layers.1.self_attn.q_proj.weight._data._data', 'model.language_model.layers.1.self_attn.q_proj.weight._scale', 'model.language_model.layers.1.self_attn.q_proj.weight._shift', 'model.language_model.layers.1.self_attn.v_proj.input_scale', 'model.language_model.layers.1.self_attn.v_proj.output_scale', 'model.language_model.layers.1.self_attn.v_proj.weight._data._data', 'model.language_model.layers.1.self_attn.v_proj.weight._scale', 'model.language_model.layers.1.self_attn.v_proj.weight._shift', 'model.language_model.layers.10.mlp.down_proj.input_scale', 'model.language_model.layers.10.mlp.down_proj.output_scale', 'model.language_model.layers.10.mlp.down_proj.weight._data._data', 'model.language_model.layers.10.mlp.down_proj.weight._scale', 'model.language_model.layers.10.mlp.down_proj.weight._shift', 'model.language_model.layers.10.mlp.gate_proj.input_scale', 'model.language_model.layers.10.mlp.gate_proj.output_scale', 'model.language_model.layers.10.mlp.gate_proj.weight._data._data', 'model.language_model.layers.10.mlp.gate_proj.weight._scale', 'model.language_model.layers.10.mlp.gate_proj.weight._shift', 'model.language_model.layers.10.mlp.up_proj.input_scale', 'model.language_model.layers.10.mlp.up_proj.output_scale', 'model.language_model.layers.10.mlp.up_proj.weight._data._data', 'model.language_model.layers.10.mlp.up_proj.weight._scale', 'model.language_model.layers.10.mlp.up_proj.weight._shift', 'model.language_model.layers.10.self_attn.k_proj.input_scale', 'model.language_model.layers.10.self_attn.k_proj.output_scale', 'model.language_model.layers.10.self_attn.k_proj.weight._data._data', 'model.language_model.layers.10.self_attn.k_proj.weight._scale', 'model.language_model.layers.10.self_attn.k_proj.weight._shift', 'model.language_model.layers.10.self_attn.o_proj.input_scale', 'model.language_model.layers.10.self_attn.o_proj.output_scale', 'model.language_model.layers.10.self_attn.o_proj.weight._data._data', 'model.language_model.layers.10.self_attn.o_proj.weight._scale', 'model.language_model.layers.10.self_attn.o_proj.weight._shift', 'model.language_model.layers.10.self_attn.q_proj.input_scale', 'model.language_model.layers.10.self_attn.q_proj.output_scale', 'model.language_model.layers.10.self_attn.q_proj.weight._data._data', 'model.language_model.layers.10.self_attn.q_proj.weight._scale', 'model.language_model.layers.10.self_attn.q_proj.weight._shift', 'model.language_model.layers.10.self_attn.v_proj.input_scale', 'model.language_model.layers.10.self_attn.v_proj.output_scale', 'model.language_model.layers.10.self_attn.v_proj.weight._data._data', 'model.language_model.layers.10.self_attn.v_proj.weight._scale', 'model.language_model.layers.10.self_attn.v_proj.weight._shift', 'model.language_model.layers.11.mlp.down_proj.input_scale', 'model.language_model.layers.11.mlp.down_proj.output_scale', 'model.language_model.layers.11.mlp.down_proj.weight._data._data', 'model.language_model.layers.11.mlp.down_proj.weight._scale', 'model.language_model.layers.11.mlp.down_proj.weight._shift', 'model.language_model.layers.11.mlp.gate_proj.input_scale', 'model.language_model.layers.11.mlp.gate_proj.output_scale', 'model.language_model.layers.11.mlp.gate_proj.weight._data._data', 'model.language_model.layers.11.mlp.gate_proj.weight._scale', 'model.language_model.layers.11.mlp.gate_proj.weight._shift', 'model.language_model.layers.11.mlp.up_proj.input_scale', 'model.language_model.layers.11.mlp.up_proj.output_scale', 'model.language_model.layers.11.mlp.up_proj.weight._data._data', 'model.language_model.layers.11.mlp.up_proj.weight._scale', 'model.language_model.layers.11.mlp.up_proj.weight._shift', 'model.language_model.layers.11.self_attn.k_proj.input_scale', 'model.language_model.layers.11.self_attn.k_proj.output_scale', 'model.language_model.layers.11.self_attn.k_proj.weight._data._data', 'model.language_model.layers.11.self_attn.k_proj.weight._scale', 'model.language_model.layers.11.self_attn.k_proj.weight._shift', 'model.language_model.layers.11.self_attn.o_proj.input_scale', 'model.language_model.layers.11.self_attn.o_proj.output_scale', 'model.language_model.layers.11.self_attn.o_proj.weight._data._data', 'model.language_model.layers.11.self_attn.o_proj.weight._scale', 'model.language_model.layers.11.self_attn.o_proj.weight._shift', 'model.language_model.layers.11.self_attn.q_proj.input_scale', 'model.language_model.layers.11.self_attn.q_proj.output_scale', 'model.language_model.layers.11.self_attn.q_proj.weight._data._data', 'model.language_model.layers.11.self_attn.q_proj.weight._scale', 'model.language_model.layers.11.self_attn.q_proj.weight._shift', 'model.language_model.layers.11.self_attn.v_proj.input_scale', 'model.language_model.layers.11.self_attn.v_proj.output_scale', 'model.language_model.layers.11.self_attn.v_proj.weight._data._data', 'model.language_model.layers.11.self_attn.v_proj.weight._scale', 'model.language_model.layers.11.self_attn.v_proj.weight._shift', 'model.language_model.layers.12.mlp.down_proj.input_scale', 'model.language_model.layers.12.mlp.down_proj.output_scale', 'model.language_model.layers.12.mlp.down_proj.weight._data._data', 'model.language_model.layers.12.mlp.down_proj.weight._scale', 'model.language_model.layers.12.mlp.down_proj.weight._shift', 'model.language_model.layers.12.mlp.gate_proj.input_scale', 'model.language_model.layers.12.mlp.gate_proj.output_scale', 'model.language_model.layers.12.mlp.gate_proj.weight._data._data', 'model.language_model.layers.12.mlp.gate_proj.weight._scale', 'model.language_model.layers.12.mlp.gate_proj.weight._shift', 'model.language_model.layers.12.mlp.up_proj.input_scale', 'model.language_model.layers.12.mlp.up_proj.output_scale', 'model.language_model.layers.12.mlp.up_proj.weight._data._data', 'model.language_model.layers.12.mlp.up_proj.weight._scale', 'model.language_model.layers.12.mlp.up_proj.weight._shift', 'model.language_model.layers.12.self_attn.k_proj.input_scale', 'model.language_model.layers.12.self_attn.k_proj.output_scale', 'model.language_model.layers.12.self_attn.k_proj.weight._data._data', 'model.language_model.layers.12.self_attn.k_proj.weight._scale', 'model.language_model.layers.12.self_attn.k_proj.weight._shift', 'model.language_model.layers.12.self_attn.o_proj.input_scale', 'model.language_model.layers.12.self_attn.o_proj.output_scale', 'model.language_model.layers.12.self_attn.o_proj.weight._data._data', 'model.language_model.layers.12.self_attn.o_proj.weight._scale', 'model.language_model.layers.12.self_attn.o_proj.weight._shift', 'model.language_model.layers.12.self_attn.q_proj.input_scale', 'model.language_model.layers.12.self_attn.q_proj.output_scale', 'model.language_model.layers.12.self_attn.q_proj.weight._data._data', 'model.language_model.layers.12.self_attn.q_proj.weight._scale', 'model.language_model.layers.12.self_attn.q_proj.weight._shift', 'model.language_model.layers.12.self_attn.v_proj.input_scale', 'model.language_model.layers.12.self_attn.v_proj.output_scale', 'model.language_model.layers.12.self_attn.v_proj.weight._data._data', 'model.language_model.layers.12.self_attn.v_proj.weight._scale', 'model.language_model.layers.12.self_attn.v_proj.weight._shift', 'model.language_model.layers.13.mlp.down_proj.input_scale', 'model.language_model.layers.13.mlp.down_proj.output_scale', 'model.language_model.layers.13.mlp.down_proj.weight._data._data', 'model.language_model.layers.13.mlp.down_proj.weight._scale', 'model.language_model.layers.13.mlp.down_proj.weight._shift', 'model.language_model.layers.13.mlp.gate_proj.input_scale', 'model.language_model.layers.13.mlp.gate_proj.output_scale', 'model.language_model.layers.13.mlp.gate_proj.weight._data._data', 'model.language_model.layers.13.mlp.gate_proj.weight._scale', 'model.language_model.layers.13.mlp.gate_proj.weight._shift', 'model.language_model.layers.13.mlp.up_proj.input_scale', 'model.language_model.layers.13.mlp.up_proj.output_scale', 'model.language_model.layers.13.mlp.up_proj.weight._data._data', 'model.language_model.layers.13.mlp.up_proj.weight._scale', 'model.language_model.layers.13.mlp.up_proj.weight._shift', 'model.language_model.layers.13.self_attn.k_proj.input_scale', 'model.language_model.layers.13.self_attn.k_proj.output_scale', 'model.language_model.layers.13.self_attn.k_proj.weight._data._data', 'model.language_model.layers.13.self_attn.k_proj.weight._scale', 'model.language_model.layers.13.self_attn.k_proj.weight._shift', 'model.language_model.layers.13.self_attn.o_proj.input_scale', 'model.language_model.layers.13.self_attn.o_proj.output_scale', 'model.language_model.layers.13.self_attn.o_proj.weight._data._data', 'model.language_model.layers.13.self_attn.o_proj.weight._scale', 'model.language_model.layers.13.self_attn.o_proj.weight._shift', 'model.language_model.layers.13.self_attn.q_proj.input_scale', 'model.language_model.layers.13.self_attn.q_proj.output_scale', 'model.language_model.layers.13.self_attn.q_proj.weight._data._data', 'model.language_model.layers.13.self_attn.q_proj.weight._scale', 'model.language_model.layers.13.self_attn.q_proj.weight._shift', 'model.language_model.layers.13.self_attn.v_proj.input_scale', 'model.language_model.layers.13.self_attn.v_proj.output_scale', 'model.language_model.layers.13.self_attn.v_proj.weight._data._data', 'model.language_model.layers.13.self_attn.v_proj.weight._scale', 'model.language_model.layers.13.self_attn.v_proj.weight._shift', 'model.language_model.layers.14.mlp.down_proj.input_scale', 'model.language_model.layers.14.mlp.down_proj.output_scale', 'model.language_model.layers.14.mlp.down_proj.weight._data._data', 'model.language_model.layers.14.mlp.down_proj.weight._scale', 'model.language_model.layers.14.mlp.down_proj.weight._shift', 'model.language_model.layers.14.mlp.gate_proj.input_scale', 'model.language_model.layers.14.mlp.gate_proj.output_scale', 'model.language_model.layers.14.mlp.gate_proj.weight._data._data', 'model.language_model.layers.14.mlp.gate_proj.weight._scale', 'model.language_model.layers.14.mlp.gate_proj.weight._shift', 'model.language_model.layers.14.mlp.up_proj.input_scale', 'model.language_model.layers.14.mlp.up_proj.output_scale', 'model.language_model.layers.14.mlp.up_proj.weight._data._data', 'model.language_model.layers.14.mlp.up_proj.weight._scale', 'model.language_model.layers.14.mlp.up_proj.weight._shift', 'model.language_model.layers.14.self_attn.k_proj.input_scale', 'model.language_model.layers.14.self_attn.k_proj.output_scale', 'model.language_model.layers.14.self_attn.k_proj.weight._data._data', 'model.language_model.layers.14.self_attn.k_proj.weight._scale', 'model.language_model.layers.14.self_attn.k_proj.weight._shift', 'model.language_model.layers.14.self_attn.o_proj.input_scale', 'model.language_model.layers.14.self_attn.o_proj.output_scale', 'model.language_model.layers.14.self_attn.o_proj.weight._data._data', 'model.language_model.layers.14.self_attn.o_proj.weight._scale', 'model.language_model.layers.14.self_attn.o_proj.weight._shift', 'model.language_model.layers.14.self_attn.q_proj.input_scale', 'model.language_model.layers.14.self_attn.q_proj.output_scale', 'model.language_model.layers.14.self_attn.q_proj.weight._data._data', 'model.language_model.layers.14.self_attn.q_proj.weight._scale', 'model.language_model.layers.14.self_attn.q_proj.weight._shift', 'model.language_model.layers.14.self_attn.v_proj.input_scale', 'model.language_model.layers.14.self_attn.v_proj.output_scale', 'model.language_model.layers.14.self_attn.v_proj.weight._data._data', 'model.language_model.layers.14.self_attn.v_proj.weight._scale', 'model.language_model.layers.14.self_attn.v_proj.weight._shift', 'model.language_model.layers.15.mlp.down_proj.input_scale', 'model.language_model.layers.15.mlp.down_proj.output_scale', 'model.language_model.layers.15.mlp.down_proj.weight._data._data', 'model.language_model.layers.15.mlp.down_proj.weight._scale', 'model.language_model.layers.15.mlp.down_proj.weight._shift', 'model.language_model.layers.15.mlp.gate_proj.input_scale', 'model.language_model.layers.15.mlp.gate_proj.output_scale', 'model.language_model.layers.15.mlp.gate_proj.weight._data._data', 'model.language_model.layers.15.mlp.gate_proj.weight._scale', 'model.language_model.layers.15.mlp.gate_proj.weight._shift', 'model.language_model.layers.15.mlp.up_proj.input_scale', 'model.language_model.layers.15.mlp.up_proj.output_scale', 'model.language_model.layers.15.mlp.up_proj.weight._data._data', 'model.language_model.layers.15.mlp.up_proj.weight._scale', 'model.language_model.layers.15.mlp.up_proj.weight._shift', 'model.language_model.layers.15.self_attn.k_proj.input_scale', 'model.language_model.layers.15.self_attn.k_proj.output_scale', 'model.language_model.layers.15.self_attn.k_proj.weight._data._data', 'model.language_model.layers.15.self_attn.k_proj.weight._scale', 'model.language_model.layers.15.self_attn.k_proj.weight._shift', 'model.language_model.layers.15.self_attn.o_proj.input_scale', 'model.language_model.layers.15.self_attn.o_proj.output_scale', 'model.language_model.layers.15.self_attn.o_proj.weight._data._data', 'model.language_model.layers.15.self_attn.o_proj.weight._scale', 'model.language_model.layers.15.self_attn.o_proj.weight._shift', 'model.language_model.layers.15.self_attn.q_proj.input_scale', 'model.language_model.layers.15.self_attn.q_proj.output_scale', 'model.language_model.layers.15.self_attn.q_proj.weight._data._data', 'model.language_model.layers.15.self_attn.q_proj.weight._scale', 'model.language_model.layers.15.self_attn.q_proj.weight._shift', 'model.language_model.layers.15.self_attn.v_proj.input_scale', 'model.language_model.layers.15.self_attn.v_proj.output_scale', 'model.language_model.layers.15.self_attn.v_proj.weight._data._data', 'model.language_model.layers.15.self_attn.v_proj.weight._scale', 'model.language_model.layers.15.self_attn.v_proj.weight._shift', 'model.language_model.layers.16.mlp.down_proj.input_scale', 'model.language_model.layers.16.mlp.down_proj.output_scale', 'model.language_model.layers.16.mlp.down_proj.weight._data._data', 'model.language_model.layers.16.mlp.down_proj.weight._scale', 'model.language_model.layers.16.mlp.down_proj.weight._shift', 'model.language_model.layers.16.mlp.gate_proj.input_scale', 'model.language_model.layers.16.mlp.gate_proj.output_scale', 'model.language_model.layers.16.mlp.gate_proj.weight._data._data', 'model.language_model.layers.16.mlp.gate_proj.weight._scale', 'model.language_model.layers.16.mlp.gate_proj.weight._shift', 'model.language_model.layers.16.mlp.up_proj.input_scale', 'model.language_model.layers.16.mlp.up_proj.output_scale', 'model.language_model.layers.16.mlp.up_proj.weight._data._data', 'model.language_model.layers.16.mlp.up_proj.weight._scale', 'model.language_model.layers.16.mlp.up_proj.weight._shift', 'model.language_model.layers.16.self_attn.k_proj.input_scale', 'model.language_model.layers.16.self_attn.k_proj.output_scale', 'model.language_model.layers.16.self_attn.k_proj.weight._data._data', 'model.language_model.layers.16.self_attn.k_proj.weight._scale', 'model.language_model.layers.16.self_attn.k_proj.weight._shift', 'model.language_model.layers.16.self_attn.o_proj.input_scale', 'model.language_model.layers.16.self_attn.o_proj.output_scale', 'model.language_model.layers.16.self_attn.o_proj.weight._data._data', 'model.language_model.layers.16.self_attn.o_proj.weight._scale', 'model.language_model.layers.16.self_attn.o_proj.weight._shift', 'model.language_model.layers.16.self_attn.q_proj.input_scale', 'model.language_model.layers.16.self_attn.q_proj.output_scale', 'model.language_model.layers.16.self_attn.q_proj.weight._data._data', 'model.language_model.layers.16.self_attn.q_proj.weight._scale', 'model.language_model.layers.16.self_attn.q_proj.weight._shift', 'model.language_model.layers.16.self_attn.v_proj.input_scale', 'model.language_model.layers.16.self_attn.v_proj.output_scale', 'model.language_model.layers.16.self_attn.v_proj.weight._data._data', 'model.language_model.layers.16.self_attn.v_proj.weight._scale', 'model.language_model.layers.16.self_attn.v_proj.weight._shift', 'model.language_model.layers.17.mlp.down_proj.input_scale', 'model.language_model.layers.17.mlp.down_proj.output_scale', 'model.language_model.layers.17.mlp.down_proj.weight._data._data', 'model.language_model.layers.17.mlp.down_proj.weight._scale', 'model.language_model.layers.17.mlp.down_proj.weight._shift', 'model.language_model.layers.17.mlp.gate_proj.input_scale', 'model.language_model.layers.17.mlp.gate_proj.output_scale', 'model.language_model.layers.17.mlp.gate_proj.weight._data._data', 'model.language_model.layers.17.mlp.gate_proj.weight._scale', 'model.language_model.layers.17.mlp.gate_proj.weight._shift', 'model.language_model.layers.17.mlp.up_proj.input_scale', 'model.language_model.layers.17.mlp.up_proj.output_scale', 'model.language_model.layers.17.mlp.up_proj.weight._data._data', 'model.language_model.layers.17.mlp.up_proj.weight._scale', 'model.language_model.layers.17.mlp.up_proj.weight._shift', 'model.language_model.layers.17.self_attn.k_proj.input_scale', 'model.language_model.layers.17.self_attn.k_proj.output_scale', 'model.language_model.layers.17.self_attn.k_proj.weight._data._data', 'model.language_model.layers.17.self_attn.k_proj.weight._scale', 'model.language_model.layers.17.self_attn.k_proj.weight._shift', 'model.language_model.layers.17.self_attn.o_proj.input_scale', 'model.language_model.layers.17.self_attn.o_proj.output_scale', 'model.language_model.layers.17.self_attn.o_proj.weight._data._data', 'model.language_model.layers.17.self_attn.o_proj.weight._scale', 'model.language_model.layers.17.self_attn.o_proj.weight._shift', 'model.language_model.layers.17.self_attn.q_proj.input_scale', 'model.language_model.layers.17.self_attn.q_proj.output_scale', 'model.language_model.layers.17.self_attn.q_proj.weight._data._data', 'model.language_model.layers.17.self_attn.q_proj.weight._scale', 'model.language_model.layers.17.self_attn.q_proj.weight._shift', 'model.language_model.layers.17.self_attn.v_proj.input_scale', 'model.language_model.layers.17.self_attn.v_proj.output_scale', 'model.language_model.layers.17.self_attn.v_proj.weight._data._data', 'model.language_model.layers.17.self_attn.v_proj.weight._scale', 'model.language_model.layers.17.self_attn.v_proj.weight._shift', 'model.language_model.layers.18.mlp.down_proj.input_scale', 'model.language_model.layers.18.mlp.down_proj.output_scale', 'model.language_model.layers.18.mlp.down_proj.weight._data._data', 'model.language_model.layers.18.mlp.down_proj.weight._scale', 'model.language_model.layers.18.mlp.down_proj.weight._shift', 'model.language_model.layers.18.mlp.gate_proj.input_scale', 'model.language_model.layers.18.mlp.gate_proj.output_scale', 'model.language_model.layers.18.mlp.gate_proj.weight._data._data', 'model.language_model.layers.18.mlp.gate_proj.weight._scale', 'model.language_model.layers.18.mlp.gate_proj.weight._shift', 'model.language_model.layers.18.mlp.up_proj.input_scale', 'model.language_model.layers.18.mlp.up_proj.output_scale', 'model.language_model.layers.18.mlp.up_proj.weight._data._data', 'model.language_model.layers.18.mlp.up_proj.weight._scale', 'model.language_model.layers.18.mlp.up_proj.weight._shift', 'model.language_model.layers.18.self_attn.k_proj.input_scale', 'model.language_model.layers.18.self_attn.k_proj.output_scale', 'model.language_model.layers.18.self_attn.k_proj.weight._data._data', 'model.language_model.layers.18.self_attn.k_proj.weight._scale', 'model.language_model.layers.18.self_attn.k_proj.weight._shift', 'model.language_model.layers.18.self_attn.o_proj.input_scale', 'model.language_model.layers.18.self_attn.o_proj.output_scale', 'model.language_model.layers.18.self_attn.o_proj.weight._data._data', 'model.language_model.layers.18.self_attn.o_proj.weight._scale', 'model.language_model.layers.18.self_attn.o_proj.weight._shift', 'model.language_model.layers.18.self_attn.q_proj.input_scale', 'model.language_model.layers.18.self_attn.q_proj.output_scale', 'model.language_model.layers.18.self_attn.q_proj.weight._data._data', 'model.language_model.layers.18.self_attn.q_proj.weight._scale', 'model.language_model.layers.18.self_attn.q_proj.weight._shift', 'model.language_model.layers.18.self_attn.v_proj.input_scale', 'model.language_model.layers.18.self_attn.v_proj.output_scale', 'model.language_model.layers.18.self_attn.v_proj.weight._data._data', 'model.language_model.layers.18.self_attn.v_proj.weight._scale', 'model.language_model.layers.18.self_attn.v_proj.weight._shift', 'model.language_model.layers.19.mlp.down_proj.input_scale', 'model.language_model.layers.19.mlp.down_proj.output_scale', 'model.language_model.layers.19.mlp.down_proj.weight._data._data', 'model.language_model.layers.19.mlp.down_proj.weight._scale', 'model.language_model.layers.19.mlp.down_proj.weight._shift', 'model.language_model.layers.19.mlp.gate_proj.input_scale', 'model.language_model.layers.19.mlp.gate_proj.output_scale', 'model.language_model.layers.19.mlp.gate_proj.weight._data._data', 'model.language_model.layers.19.mlp.gate_proj.weight._scale', 'model.language_model.layers.19.mlp.gate_proj.weight._shift', 'model.language_model.layers.19.mlp.up_proj.input_scale', 'model.language_model.layers.19.mlp.up_proj.output_scale', 'model.language_model.layers.19.mlp.up_proj.weight._data._data', 'model.language_model.layers.19.mlp.up_proj.weight._scale', 'model.language_model.layers.19.mlp.up_proj.weight._shift', 'model.language_model.layers.19.self_attn.k_proj.input_scale', 'model.language_model.layers.19.self_attn.k_proj.output_scale', 'model.language_model.layers.19.self_attn.k_proj.weight._data._data', 'model.language_model.layers.19.self_attn.k_proj.weight._scale', 'model.language_model.layers.19.self_attn.k_proj.weight._shift', 'model.language_model.layers.19.self_attn.o_proj.input_scale', 'model.language_model.layers.19.self_attn.o_proj.output_scale', 'model.language_model.layers.19.self_attn.o_proj.weight._data._data', 'model.language_model.layers.19.self_attn.o_proj.weight._scale', 'model.language_model.layers.19.self_attn.o_proj.weight._shift', 'model.language_model.layers.19.self_attn.q_proj.input_scale', 'model.language_model.layers.19.self_attn.q_proj.output_scale', 'model.language_model.layers.19.self_attn.q_proj.weight._data._data', 'model.language_model.layers.19.self_attn.q_proj.weight._scale', 'model.language_model.layers.19.self_attn.q_proj.weight._shift', 'model.language_model.layers.19.self_attn.v_proj.input_scale', 'model.language_model.layers.19.self_attn.v_proj.output_scale', 'model.language_model.layers.19.self_attn.v_proj.weight._data._data', 'model.language_model.layers.19.self_attn.v_proj.weight._scale', 'model.language_model.layers.19.self_attn.v_proj.weight._shift', 'model.language_model.layers.2.mlp.down_proj.input_scale', 'model.language_model.layers.2.mlp.down_proj.output_scale', 'model.language_model.layers.2.mlp.down_proj.weight._data._data', 'model.language_model.layers.2.mlp.down_proj.weight._scale', 'model.language_model.layers.2.mlp.down_proj.weight._shift', 'model.language_model.layers.2.mlp.gate_proj.input_scale', 'model.language_model.layers.2.mlp.gate_proj.output_scale', 'model.language_model.layers.2.mlp.gate_proj.weight._data._data', 'model.language_model.layers.2.mlp.gate_proj.weight._scale', 'model.language_model.layers.2.mlp.gate_proj.weight._shift', 'model.language_model.layers.2.mlp.up_proj.input_scale', 'model.language_model.layers.2.mlp.up_proj.output_scale', 'model.language_model.layers.2.mlp.up_proj.weight._data._data', 'model.language_model.layers.2.mlp.up_proj.weight._scale', 'model.language_model.layers.2.mlp.up_proj.weight._shift', 'model.language_model.layers.2.self_attn.k_proj.input_scale', 'model.language_model.layers.2.self_attn.k_proj.output_scale', 'model.language_model.layers.2.self_attn.k_proj.weight._data._data', 'model.language_model.layers.2.self_attn.k_proj.weight._scale', 'model.language_model.layers.2.self_attn.k_proj.weight._shift', 'model.language_model.layers.2.self_attn.o_proj.input_scale', 'model.language_model.layers.2.self_attn.o_proj.output_scale', 'model.language_model.layers.2.self_attn.o_proj.weight._data._data', 'model.language_model.layers.2.self_attn.o_proj.weight._scale', 'model.language_model.layers.2.self_attn.o_proj.weight._shift', 'model.language_model.layers.2.self_attn.q_proj.input_scale', 'model.language_model.layers.2.self_attn.q_proj.output_scale', 'model.language_model.layers.2.self_attn.q_proj.weight._data._data', 'model.language_model.layers.2.self_attn.q_proj.weight._scale', 'model.language_model.layers.2.self_attn.q_proj.weight._shift', 'model.language_model.layers.2.self_attn.v_proj.input_scale', 'model.language_model.layers.2.self_attn.v_proj.output_scale', 'model.language_model.layers.2.self_attn.v_proj.weight._data._data', 'model.language_model.layers.2.self_attn.v_proj.weight._scale', 'model.language_model.layers.2.self_attn.v_proj.weight._shift', 'model.language_model.layers.20.mlp.down_proj.input_scale', 'model.language_model.layers.20.mlp.down_proj.output_scale', 'model.language_model.layers.20.mlp.down_proj.weight._data._data', 'model.language_model.layers.20.mlp.down_proj.weight._scale', 'model.language_model.layers.20.mlp.down_proj.weight._shift', 'model.language_model.layers.20.mlp.gate_proj.input_scale', 'model.language_model.layers.20.mlp.gate_proj.output_scale', 'model.language_model.layers.20.mlp.gate_proj.weight._data._data', 'model.language_model.layers.20.mlp.gate_proj.weight._scale', 'model.language_model.layers.20.mlp.gate_proj.weight._shift', 'model.language_model.layers.20.mlp.up_proj.input_scale', 'model.language_model.layers.20.mlp.up_proj.output_scale', 'model.language_model.layers.20.mlp.up_proj.weight._data._data', 'model.language_model.layers.20.mlp.up_proj.weight._scale', 'model.language_model.layers.20.mlp.up_proj.weight._shift', 'model.language_model.layers.20.self_attn.k_proj.input_scale', 'model.language_model.layers.20.self_attn.k_proj.output_scale', 'model.language_model.layers.20.self_attn.k_proj.weight._data._data', 'model.language_model.layers.20.self_attn.k_proj.weight._scale', 'model.language_model.layers.20.self_attn.k_proj.weight._shift', 'model.language_model.layers.20.self_attn.o_proj.input_scale', 'model.language_model.layers.20.self_attn.o_proj.output_scale', 'model.language_model.layers.20.self_attn.o_proj.weight._data._data', 'model.language_model.layers.20.self_attn.o_proj.weight._scale', 'model.language_model.layers.20.self_attn.o_proj.weight._shift', 'model.language_model.layers.20.self_attn.q_proj.input_scale', 'model.language_model.layers.20.self_attn.q_proj.output_scale', 'model.language_model.layers.20.self_attn.q_proj.weight._data._data', 'model.language_model.layers.20.self_attn.q_proj.weight._scale', 'model.language_model.layers.20.self_attn.q_proj.weight._shift', 'model.language_model.layers.20.self_attn.v_proj.input_scale', 'model.language_model.layers.20.self_attn.v_proj.output_scale', 'model.language_model.layers.20.self_attn.v_proj.weight._data._data', 'model.language_model.layers.20.self_attn.v_proj.weight._scale', 'model.language_model.layers.20.self_attn.v_proj.weight._shift', 'model.language_model.layers.21.mlp.down_proj.input_scale', 'model.language_model.layers.21.mlp.down_proj.output_scale', 'model.language_model.layers.21.mlp.down_proj.weight._data._data', 'model.language_model.layers.21.mlp.down_proj.weight._scale', 'model.language_model.layers.21.mlp.down_proj.weight._shift', 'model.language_model.layers.21.mlp.gate_proj.input_scale', 'model.language_model.layers.21.mlp.gate_proj.output_scale', 'model.language_model.layers.21.mlp.gate_proj.weight._data._data', 'model.language_model.layers.21.mlp.gate_proj.weight._scale', 'model.language_model.layers.21.mlp.gate_proj.weight._shift', 'model.language_model.layers.21.mlp.up_proj.input_scale', 'model.language_model.layers.21.mlp.up_proj.output_scale', 'model.language_model.layers.21.mlp.up_proj.weight._data._data', 'model.language_model.layers.21.mlp.up_proj.weight._scale', 'model.language_model.layers.21.mlp.up_proj.weight._shift', 'model.language_model.layers.21.self_attn.k_proj.input_scale', 'model.language_model.layers.21.self_attn.k_proj.output_scale', 'model.language_model.layers.21.self_attn.k_proj.weight._data._data', 'model.language_model.layers.21.self_attn.k_proj.weight._scale', 'model.language_model.layers.21.self_attn.k_proj.weight._shift', 'model.language_model.layers.21.self_attn.o_proj.input_scale', 'model.language_model.layers.21.self_attn.o_proj.output_scale', 'model.language_model.layers.21.self_attn.o_proj.weight._data._data', 'model.language_model.layers.21.self_attn.o_proj.weight._scale', 'model.language_model.layers.21.self_attn.o_proj.weight._shift', 'model.language_model.layers.21.self_attn.q_proj.input_scale', 'model.language_model.layers.21.self_attn.q_proj.output_scale', 'model.language_model.layers.21.self_attn.q_proj.weight._data._data', 'model.language_model.layers.21.self_attn.q_proj.weight._scale', 'model.language_model.layers.21.self_attn.q_proj.weight._shift', 'model.language_model.layers.21.self_attn.v_proj.input_scale', 'model.language_model.layers.21.self_attn.v_proj.output_scale', 'model.language_model.layers.21.self_attn.v_proj.weight._data._data', 'model.language_model.layers.21.self_attn.v_proj.weight._scale', 'model.language_model.layers.21.self_attn.v_proj.weight._shift', 'model.language_model.layers.22.mlp.down_proj.input_scale', 'model.language_model.layers.22.mlp.down_proj.output_scale', 'model.language_model.layers.22.mlp.down_proj.weight._data._data', 'model.language_model.layers.22.mlp.down_proj.weight._scale', 'model.language_model.layers.22.mlp.down_proj.weight._shift', 'model.language_model.layers.22.mlp.gate_proj.input_scale', 'model.language_model.layers.22.mlp.gate_proj.output_scale', 'model.language_model.layers.22.mlp.gate_proj.weight._data._data', 'model.language_model.layers.22.mlp.gate_proj.weight._scale', 'model.language_model.layers.22.mlp.gate_proj.weight._shift', 'model.language_model.layers.22.mlp.up_proj.input_scale', 'model.language_model.layers.22.mlp.up_proj.output_scale', 'model.language_model.layers.22.mlp.up_proj.weight._data._data', 'model.language_model.layers.22.mlp.up_proj.weight._scale', 'model.language_model.layers.22.mlp.up_proj.weight._shift', 'model.language_model.layers.22.self_attn.k_proj.input_scale', 'model.language_model.layers.22.self_attn.k_proj.output_scale', 'model.language_model.layers.22.self_attn.k_proj.weight._data._data', 'model.language_model.layers.22.self_attn.k_proj.weight._scale', 'model.language_model.layers.22.self_attn.k_proj.weight._shift', 'model.language_model.layers.22.self_attn.o_proj.input_scale', 'model.language_model.layers.22.self_attn.o_proj.output_scale', 'model.language_model.layers.22.self_attn.o_proj.weight._data._data', 'model.language_model.layers.22.self_attn.o_proj.weight._scale', 'model.language_model.layers.22.self_attn.o_proj.weight._shift', 'model.language_model.layers.22.self_attn.q_proj.input_scale', 'model.language_model.layers.22.self_attn.q_proj.output_scale', 'model.language_model.layers.22.self_attn.q_proj.weight._data._data', 'model.language_model.layers.22.self_attn.q_proj.weight._scale', 'model.language_model.layers.22.self_attn.q_proj.weight._shift', 'model.language_model.layers.22.self_attn.v_proj.input_scale', 'model.language_model.layers.22.self_attn.v_proj.output_scale', 'model.language_model.layers.22.self_attn.v_proj.weight._data._data', 'model.language_model.layers.22.self_attn.v_proj.weight._scale', 'model.language_model.layers.22.self_attn.v_proj.weight._shift', 'model.language_model.layers.23.mlp.down_proj.input_scale', 'model.language_model.layers.23.mlp.down_proj.output_scale', 'model.language_model.layers.23.mlp.down_proj.weight._data._data', 'model.language_model.layers.23.mlp.down_proj.weight._scale', 'model.language_model.layers.23.mlp.down_proj.weight._shift', 'model.language_model.layers.23.mlp.gate_proj.input_scale', 'model.language_model.layers.23.mlp.gate_proj.output_scale', 'model.language_model.layers.23.mlp.gate_proj.weight._data._data', 'model.language_model.layers.23.mlp.gate_proj.weight._scale', 'model.language_model.layers.23.mlp.gate_proj.weight._shift', 'model.language_model.layers.23.mlp.up_proj.input_scale', 'model.language_model.layers.23.mlp.up_proj.output_scale', 'model.language_model.layers.23.mlp.up_proj.weight._data._data', 'model.language_model.layers.23.mlp.up_proj.weight._scale', 'model.language_model.layers.23.mlp.up_proj.weight._shift', 'model.language_model.layers.23.self_attn.k_proj.input_scale', 'model.language_model.layers.23.self_attn.k_proj.output_scale', 'model.language_model.layers.23.self_attn.k_proj.weight._data._data', 'model.language_model.layers.23.self_attn.k_proj.weight._scale', 'model.language_model.layers.23.self_attn.k_proj.weight._shift', 'model.language_model.layers.23.self_attn.o_proj.input_scale', 'model.language_model.layers.23.self_attn.o_proj.output_scale', 'model.language_model.layers.23.self_attn.o_proj.weight._data._data', 'model.language_model.layers.23.self_attn.o_proj.weight._scale', 'model.language_model.layers.23.self_attn.o_proj.weight._shift', 'model.language_model.layers.23.self_attn.q_proj.input_scale', 'model.language_model.layers.23.self_attn.q_proj.output_scale', 'model.language_model.layers.23.self_attn.q_proj.weight._data._data', 'model.language_model.layers.23.self_attn.q_proj.weight._scale', 'model.language_model.layers.23.self_attn.q_proj.weight._shift', 'model.language_model.layers.23.self_attn.v_proj.input_scale', 'model.language_model.layers.23.self_attn.v_proj.output_scale', 'model.language_model.layers.23.self_attn.v_proj.weight._data._data', 'model.language_model.layers.23.self_attn.v_proj.weight._scale', 'model.language_model.layers.23.self_attn.v_proj.weight._shift', 'model.language_model.layers.24.mlp.down_proj.input_scale', 'model.language_model.layers.24.mlp.down_proj.output_scale', 'model.language_model.layers.24.mlp.down_proj.weight._data._data', 'model.language_model.layers.24.mlp.down_proj.weight._scale', 'model.language_model.layers.24.mlp.down_proj.weight._shift', 'model.language_model.layers.24.mlp.gate_proj.input_scale', 'model.language_model.layers.24.mlp.gate_proj.output_scale', 'model.language_model.layers.24.mlp.gate_proj.weight._data._data', 'model.language_model.layers.24.mlp.gate_proj.weight._scale', 'model.language_model.layers.24.mlp.gate_proj.weight._shift', 'model.language_model.layers.24.mlp.up_proj.input_scale', 'model.language_model.layers.24.mlp.up_proj.output_scale', 'model.language_model.layers.24.mlp.up_proj.weight._data._data', 'model.language_model.layers.24.mlp.up_proj.weight._scale', 'model.language_model.layers.24.mlp.up_proj.weight._shift', 'model.language_model.layers.24.self_attn.k_proj.input_scale', 'model.language_model.layers.24.self_attn.k_proj.output_scale', 'model.language_model.layers.24.self_attn.k_proj.weight._data._data', 'model.language_model.layers.24.self_attn.k_proj.weight._scale', 'model.language_model.layers.24.self_attn.k_proj.weight._shift', 'model.language_model.layers.24.self_attn.o_proj.input_scale', 'model.language_model.layers.24.self_attn.o_proj.output_scale', 'model.language_model.layers.24.self_attn.o_proj.weight._data._data', 'model.language_model.layers.24.self_attn.o_proj.weight._scale', 'model.language_model.layers.24.self_attn.o_proj.weight._shift', 'model.language_model.layers.24.self_attn.q_proj.input_scale', 'model.language_model.layers.24.self_attn.q_proj.output_scale', 'model.language_model.layers.24.self_attn.q_proj.weight._data._data', 'model.language_model.layers.24.self_attn.q_proj.weight._scale', 'model.language_model.layers.24.self_attn.q_proj.weight._shift', 'model.language_model.layers.24.self_attn.v_proj.input_scale', 'model.language_model.layers.24.self_attn.v_proj.output_scale', 'model.language_model.layers.24.self_attn.v_proj.weight._data._data', 'model.language_model.layers.24.self_attn.v_proj.weight._scale', 'model.language_model.layers.24.self_attn.v_proj.weight._shift', 'model.language_model.layers.25.mlp.down_proj.input_scale', 'model.language_model.layers.25.mlp.down_proj.output_scale', 'model.language_model.layers.25.mlp.down_proj.weight._data._data', 'model.language_model.layers.25.mlp.down_proj.weight._scale', 'model.language_model.layers.25.mlp.down_proj.weight._shift', 'model.language_model.layers.25.mlp.gate_proj.input_scale', 'model.language_model.layers.25.mlp.gate_proj.output_scale', 'model.language_model.layers.25.mlp.gate_proj.weight._data._data', 'model.language_model.layers.25.mlp.gate_proj.weight._scale', 'model.language_model.layers.25.mlp.gate_proj.weight._shift', 'model.language_model.layers.25.mlp.up_proj.input_scale', 'model.language_model.layers.25.mlp.up_proj.output_scale', 'model.language_model.layers.25.mlp.up_proj.weight._data._data', 'model.language_model.layers.25.mlp.up_proj.weight._scale', 'model.language_model.layers.25.mlp.up_proj.weight._shift', 'model.language_model.layers.25.self_attn.k_proj.input_scale', 'model.language_model.layers.25.self_attn.k_proj.output_scale', 'model.language_model.layers.25.self_attn.k_proj.weight._data._data', 'model.language_model.layers.25.self_attn.k_proj.weight._scale', 'model.language_model.layers.25.self_attn.k_proj.weight._shift', 'model.language_model.layers.25.self_attn.o_proj.input_scale', 'model.language_model.layers.25.self_attn.o_proj.output_scale', 'model.language_model.layers.25.self_attn.o_proj.weight._data._data', 'model.language_model.layers.25.self_attn.o_proj.weight._scale', 'model.language_model.layers.25.self_attn.o_proj.weight._shift', 'model.language_model.layers.25.self_attn.q_proj.input_scale', 'model.language_model.layers.25.self_attn.q_proj.output_scale', 'model.language_model.layers.25.self_attn.q_proj.weight._data._data', 'model.language_model.layers.25.self_attn.q_proj.weight._scale', 'model.language_model.layers.25.self_attn.q_proj.weight._shift', 'model.language_model.layers.25.self_attn.v_proj.input_scale', 'model.language_model.layers.25.self_attn.v_proj.output_scale', 'model.language_model.layers.25.self_attn.v_proj.weight._data._data', 'model.language_model.layers.25.self_attn.v_proj.weight._scale', 'model.language_model.layers.25.self_attn.v_proj.weight._shift', 'model.language_model.layers.26.mlp.down_proj.input_scale', 'model.language_model.layers.26.mlp.down_proj.output_scale', 'model.language_model.layers.26.mlp.down_proj.weight._data._data', 'model.language_model.layers.26.mlp.down_proj.weight._scale', 'model.language_model.layers.26.mlp.down_proj.weight._shift', 'model.language_model.layers.26.mlp.gate_proj.input_scale', 'model.language_model.layers.26.mlp.gate_proj.output_scale', 'model.language_model.layers.26.mlp.gate_proj.weight._data._data', 'model.language_model.layers.26.mlp.gate_proj.weight._scale', 'model.language_model.layers.26.mlp.gate_proj.weight._shift', 'model.language_model.layers.26.mlp.up_proj.input_scale', 'model.language_model.layers.26.mlp.up_proj.output_scale', 'model.language_model.layers.26.mlp.up_proj.weight._data._data', 'model.language_model.layers.26.mlp.up_proj.weight._scale', 'model.language_model.layers.26.mlp.up_proj.weight._shift', 'model.language_model.layers.26.self_attn.k_proj.input_scale', 'model.language_model.layers.26.self_attn.k_proj.output_scale', 'model.language_model.layers.26.self_attn.k_proj.weight._data._data', 'model.language_model.layers.26.self_attn.k_proj.weight._scale', 'model.language_model.layers.26.self_attn.k_proj.weight._shift', 'model.language_model.layers.26.self_attn.o_proj.input_scale', 'model.language_model.layers.26.self_attn.o_proj.output_scale', 'model.language_model.layers.26.self_attn.o_proj.weight._data._data', 'model.language_model.layers.26.self_attn.o_proj.weight._scale', 'model.language_model.layers.26.self_attn.o_proj.weight._shift', 'model.language_model.layers.26.self_attn.q_proj.input_scale', 'model.language_model.layers.26.self_attn.q_proj.output_scale', 'model.language_model.layers.26.self_attn.q_proj.weight._data._data', 'model.language_model.layers.26.self_attn.q_proj.weight._scale', 'model.language_model.layers.26.self_attn.q_proj.weight._shift', 'model.language_model.layers.26.self_attn.v_proj.input_scale', 'model.language_model.layers.26.self_attn.v_proj.output_scale', 'model.language_model.layers.26.self_attn.v_proj.weight._data._data', 'model.language_model.layers.26.self_attn.v_proj.weight._scale', 'model.language_model.layers.26.self_attn.v_proj.weight._shift', 'model.language_model.layers.27.mlp.down_proj.input_scale', 'model.language_model.layers.27.mlp.down_proj.output_scale', 'model.language_model.layers.27.mlp.down_proj.weight._data._data', 'model.language_model.layers.27.mlp.down_proj.weight._scale', 'model.language_model.layers.27.mlp.down_proj.weight._shift', 'model.language_model.layers.27.mlp.gate_proj.input_scale', 'model.language_model.layers.27.mlp.gate_proj.output_scale', 'model.language_model.layers.27.mlp.gate_proj.weight._data._data', 'model.language_model.layers.27.mlp.gate_proj.weight._scale', 'model.language_model.layers.27.mlp.gate_proj.weight._shift', 'model.language_model.layers.27.mlp.up_proj.input_scale', 'model.language_model.layers.27.mlp.up_proj.output_scale', 'model.language_model.layers.27.mlp.up_proj.weight._data._data', 'model.language_model.layers.27.mlp.up_proj.weight._scale', 'model.language_model.layers.27.mlp.up_proj.weight._shift', 'model.language_model.layers.27.self_attn.k_proj.input_scale', 'model.language_model.layers.27.self_attn.k_proj.output_scale', 'model.language_model.layers.27.self_attn.k_proj.weight._data._data', 'model.language_model.layers.27.self_attn.k_proj.weight._scale', 'model.language_model.layers.27.self_attn.k_proj.weight._shift', 'model.language_model.layers.27.self_attn.o_proj.input_scale', 'model.language_model.layers.27.self_attn.o_proj.output_scale', 'model.language_model.layers.27.self_attn.o_proj.weight._data._data', 'model.language_model.layers.27.self_attn.o_proj.weight._scale', 'model.language_model.layers.27.self_attn.o_proj.weight._shift', 'model.language_model.layers.27.self_attn.q_proj.input_scale', 'model.language_model.layers.27.self_attn.q_proj.output_scale', 'model.language_model.layers.27.self_attn.q_proj.weight._data._data', 'model.language_model.layers.27.self_attn.q_proj.weight._scale', 'model.language_model.layers.27.self_attn.q_proj.weight._shift', 'model.language_model.layers.27.self_attn.v_proj.input_scale', 'model.language_model.layers.27.self_attn.v_proj.output_scale', 'model.language_model.layers.27.self_attn.v_proj.weight._data._data', 'model.language_model.layers.27.self_attn.v_proj.weight._scale', 'model.language_model.layers.27.self_attn.v_proj.weight._shift', 'model.language_model.layers.28.mlp.down_proj.input_scale', 'model.language_model.layers.28.mlp.down_proj.output_scale', 'model.language_model.layers.28.mlp.down_proj.weight._data._data', 'model.language_model.layers.28.mlp.down_proj.weight._scale', 'model.language_model.layers.28.mlp.down_proj.weight._shift', 'model.language_model.layers.28.mlp.gate_proj.input_scale', 'model.language_model.layers.28.mlp.gate_proj.output_scale', 'model.language_model.layers.28.mlp.gate_proj.weight._data._data', 'model.language_model.layers.28.mlp.gate_proj.weight._scale', 'model.language_model.layers.28.mlp.gate_proj.weight._shift', 'model.language_model.layers.28.mlp.up_proj.input_scale', 'model.language_model.layers.28.mlp.up_proj.output_scale', 'model.language_model.layers.28.mlp.up_proj.weight._data._data', 'model.language_model.layers.28.mlp.up_proj.weight._scale', 'model.language_model.layers.28.mlp.up_proj.weight._shift', 'model.language_model.layers.28.self_attn.k_proj.input_scale', 'model.language_model.layers.28.self_attn.k_proj.output_scale', 'model.language_model.layers.28.self_attn.k_proj.weight._data._data', 'model.language_model.layers.28.self_attn.k_proj.weight._scale', 'model.language_model.layers.28.self_attn.k_proj.weight._shift', 'model.language_model.layers.28.self_attn.o_proj.input_scale', 'model.language_model.layers.28.self_attn.o_proj.output_scale', 'model.language_model.layers.28.self_attn.o_proj.weight._data._data', 'model.language_model.layers.28.self_attn.o_proj.weight._scale', 'model.language_model.layers.28.self_attn.o_proj.weight._shift', 'model.language_model.layers.28.self_attn.q_proj.input_scale', 'model.language_model.layers.28.self_attn.q_proj.output_scale', 'model.language_model.layers.28.self_attn.q_proj.weight._data._data', 'model.language_model.layers.28.self_attn.q_proj.weight._scale', 'model.language_model.layers.28.self_attn.q_proj.weight._shift', 'model.language_model.layers.28.self_attn.v_proj.input_scale', 'model.language_model.layers.28.self_attn.v_proj.output_scale', 'model.language_model.layers.28.self_attn.v_proj.weight._data._data', 'model.language_model.layers.28.self_attn.v_proj.weight._scale', 'model.language_model.layers.28.self_attn.v_proj.weight._shift', 'model.language_model.layers.29.mlp.down_proj.input_scale', 'model.language_model.layers.29.mlp.down_proj.output_scale', 'model.language_model.layers.29.mlp.down_proj.weight._data._data', 'model.language_model.layers.29.mlp.down_proj.weight._scale', 'model.language_model.layers.29.mlp.down_proj.weight._shift', 'model.language_model.layers.29.mlp.gate_proj.input_scale', 'model.language_model.layers.29.mlp.gate_proj.output_scale', 'model.language_model.layers.29.mlp.gate_proj.weight._data._data', 'model.language_model.layers.29.mlp.gate_proj.weight._scale', 'model.language_model.layers.29.mlp.gate_proj.weight._shift', 'model.language_model.layers.29.mlp.up_proj.input_scale', 'model.language_model.layers.29.mlp.up_proj.output_scale', 'model.language_model.layers.29.mlp.up_proj.weight._data._data', 'model.language_model.layers.29.mlp.up_proj.weight._scale', 'model.language_model.layers.29.mlp.up_proj.weight._shift', 'model.language_model.layers.29.self_attn.k_proj.input_scale', 'model.language_model.layers.29.self_attn.k_proj.output_scale', 'model.language_model.layers.29.self_attn.k_proj.weight._data._data', 'model.language_model.layers.29.self_attn.k_proj.weight._scale', 'model.language_model.layers.29.self_attn.k_proj.weight._shift', 'model.language_model.layers.29.self_attn.o_proj.input_scale', 'model.language_model.layers.29.self_attn.o_proj.output_scale', 'model.language_model.layers.29.self_attn.o_proj.weight._data._data', 'model.language_model.layers.29.self_attn.o_proj.weight._scale', 'model.language_model.layers.29.self_attn.o_proj.weight._shift', 'model.language_model.layers.29.self_attn.q_proj.input_scale', 'model.language_model.layers.29.self_attn.q_proj.output_scale', 'model.language_model.layers.29.self_attn.q_proj.weight._data._data', 'model.language_model.layers.29.self_attn.q_proj.weight._scale', 'model.language_model.layers.29.self_attn.q_proj.weight._shift', 'model.language_model.layers.29.self_attn.v_proj.input_scale', 'model.language_model.layers.29.self_attn.v_proj.output_scale', 'model.language_model.layers.29.self_attn.v_proj.weight._data._data', 'model.language_model.layers.29.self_attn.v_proj.weight._scale', 'model.language_model.layers.29.self_attn.v_proj.weight._shift', 'model.language_model.layers.3.mlp.down_proj.input_scale', 'model.language_model.layers.3.mlp.down_proj.output_scale', 'model.language_model.layers.3.mlp.down_proj.weight._data._data', 'model.language_model.layers.3.mlp.down_proj.weight._scale', 'model.language_model.layers.3.mlp.down_proj.weight._shift', 'model.language_model.layers.3.mlp.gate_proj.input_scale', 'model.language_model.layers.3.mlp.gate_proj.output_scale', 'model.language_model.layers.3.mlp.gate_proj.weight._data._data', 'model.language_model.layers.3.mlp.gate_proj.weight._scale', 'model.language_model.layers.3.mlp.gate_proj.weight._shift', 'model.language_model.layers.3.mlp.up_proj.input_scale', 'model.language_model.layers.3.mlp.up_proj.output_scale', 'model.language_model.layers.3.mlp.up_proj.weight._data._data', 'model.language_model.layers.3.mlp.up_proj.weight._scale', 'model.language_model.layers.3.mlp.up_proj.weight._shift', 'model.language_model.layers.3.self_attn.k_proj.input_scale', 'model.language_model.layers.3.self_attn.k_proj.output_scale', 'model.language_model.layers.3.self_attn.k_proj.weight._data._data', 'model.language_model.layers.3.self_attn.k_proj.weight._scale', 'model.language_model.layers.3.self_attn.k_proj.weight._shift', 'model.language_model.layers.3.self_attn.o_proj.input_scale', 'model.language_model.layers.3.self_attn.o_proj.output_scale', 'model.language_model.layers.3.self_attn.o_proj.weight._data._data', 'model.language_model.layers.3.self_attn.o_proj.weight._scale', 'model.language_model.layers.3.self_attn.o_proj.weight._shift', 'model.language_model.layers.3.self_attn.q_proj.input_scale', 'model.language_model.layers.3.self_attn.q_proj.output_scale', 'model.language_model.layers.3.self_attn.q_proj.weight._data._data', 'model.language_model.layers.3.self_attn.q_proj.weight._scale', 'model.language_model.layers.3.self_attn.q_proj.weight._shift', 'model.language_model.layers.3.self_attn.v_proj.input_scale', 'model.language_model.layers.3.self_attn.v_proj.output_scale', 'model.language_model.layers.3.self_attn.v_proj.weight._data._data', 'model.language_model.layers.3.self_attn.v_proj.weight._scale', 'model.language_model.layers.3.self_attn.v_proj.weight._shift', 'model.language_model.layers.30.mlp.down_proj.input_scale', 'model.language_model.layers.30.mlp.down_proj.output_scale', 'model.language_model.layers.30.mlp.down_proj.weight._data._data', 'model.language_model.layers.30.mlp.down_proj.weight._scale', 'model.language_model.layers.30.mlp.down_proj.weight._shift', 'model.language_model.layers.30.mlp.gate_proj.input_scale', 'model.language_model.layers.30.mlp.gate_proj.output_scale', 'model.language_model.layers.30.mlp.gate_proj.weight._data._data', 'model.language_model.layers.30.mlp.gate_proj.weight._scale', 'model.language_model.layers.30.mlp.gate_proj.weight._shift', 'model.language_model.layers.30.mlp.up_proj.input_scale', 'model.language_model.layers.30.mlp.up_proj.output_scale', 'model.language_model.layers.30.mlp.up_proj.weight._data._data', 'model.language_model.layers.30.mlp.up_proj.weight._scale', 'model.language_model.layers.30.mlp.up_proj.weight._shift', 'model.language_model.layers.30.self_attn.k_proj.input_scale', 'model.language_model.layers.30.self_attn.k_proj.output_scale', 'model.language_model.layers.30.self_attn.k_proj.weight._data._data', 'model.language_model.layers.30.self_attn.k_proj.weight._scale', 'model.language_model.layers.30.self_attn.k_proj.weight._shift', 'model.language_model.layers.30.self_attn.o_proj.input_scale', 'model.language_model.layers.30.self_attn.o_proj.output_scale', 'model.language_model.layers.30.self_attn.o_proj.weight._data._data', 'model.language_model.layers.30.self_attn.o_proj.weight._scale', 'model.language_model.layers.30.self_attn.o_proj.weight._shift', 'model.language_model.layers.30.self_attn.q_proj.input_scale', 'model.language_model.layers.30.self_attn.q_proj.output_scale', 'model.language_model.layers.30.self_attn.q_proj.weight._data._data', 'model.language_model.layers.30.self_attn.q_proj.weight._scale', 'model.language_model.layers.30.self_attn.q_proj.weight._shift', 'model.language_model.layers.30.self_attn.v_proj.input_scale', 'model.language_model.layers.30.self_attn.v_proj.output_scale', 'model.language_model.layers.30.self_attn.v_proj.weight._data._data', 'model.language_model.layers.30.self_attn.v_proj.weight._scale', 'model.language_model.layers.30.self_attn.v_proj.weight._shift', 'model.language_model.layers.31.mlp.down_proj.input_scale', 'model.language_model.layers.31.mlp.down_proj.output_scale', 'model.language_model.layers.31.mlp.down_proj.weight._data._data', 'model.language_model.layers.31.mlp.down_proj.weight._scale', 'model.language_model.layers.31.mlp.down_proj.weight._shift', 'model.language_model.layers.31.mlp.gate_proj.input_scale', 'model.language_model.layers.31.mlp.gate_proj.output_scale', 'model.language_model.layers.31.mlp.gate_proj.weight._data._data', 'model.language_model.layers.31.mlp.gate_proj.weight._scale', 'model.language_model.layers.31.mlp.gate_proj.weight._shift', 'model.language_model.layers.31.mlp.up_proj.input_scale', 'model.language_model.layers.31.mlp.up_proj.output_scale', 'model.language_model.layers.31.mlp.up_proj.weight._data._data', 'model.language_model.layers.31.mlp.up_proj.weight._scale', 'model.language_model.layers.31.mlp.up_proj.weight._shift', 'model.language_model.layers.31.self_attn.k_proj.input_scale', 'model.language_model.layers.31.self_attn.k_proj.output_scale', 'model.language_model.layers.31.self_attn.k_proj.weight._data._data', 'model.language_model.layers.31.self_attn.k_proj.weight._scale', 'model.language_model.layers.31.self_attn.k_proj.weight._shift', 'model.language_model.layers.31.self_attn.o_proj.input_scale', 'model.language_model.layers.31.self_attn.o_proj.output_scale', 'model.language_model.layers.31.self_attn.o_proj.weight._data._data', 'model.language_model.layers.31.self_attn.o_proj.weight._scale', 'model.language_model.layers.31.self_attn.o_proj.weight._shift', 'model.language_model.layers.31.self_attn.q_proj.input_scale', 'model.language_model.layers.31.self_attn.q_proj.output_scale', 'model.language_model.layers.31.self_attn.q_proj.weight._data._data', 'model.language_model.layers.31.self_attn.q_proj.weight._scale', 'model.language_model.layers.31.self_attn.q_proj.weight._shift', 'model.language_model.layers.31.self_attn.v_proj.input_scale', 'model.language_model.layers.31.self_attn.v_proj.output_scale', 'model.language_model.layers.31.self_attn.v_proj.weight._data._data', 'model.language_model.layers.31.self_attn.v_proj.weight._scale', 'model.language_model.layers.31.self_attn.v_proj.weight._shift', 'model.language_model.layers.32.mlp.down_proj.input_scale', 'model.language_model.layers.32.mlp.down_proj.output_scale', 'model.language_model.layers.32.mlp.down_proj.weight._data._data', 'model.language_model.layers.32.mlp.down_proj.weight._scale', 'model.language_model.layers.32.mlp.down_proj.weight._shift', 'model.language_model.layers.32.mlp.gate_proj.input_scale', 'model.language_model.layers.32.mlp.gate_proj.output_scale', 'model.language_model.layers.32.mlp.gate_proj.weight._data._data', 'model.language_model.layers.32.mlp.gate_proj.weight._scale', 'model.language_model.layers.32.mlp.gate_proj.weight._shift', 'model.language_model.layers.32.mlp.up_proj.input_scale', 'model.language_model.layers.32.mlp.up_proj.output_scale', 'model.language_model.layers.32.mlp.up_proj.weight._data._data', 'model.language_model.layers.32.mlp.up_proj.weight._scale', 'model.language_model.layers.32.mlp.up_proj.weight._shift', 'model.language_model.layers.32.self_attn.k_proj.input_scale', 'model.language_model.layers.32.self_attn.k_proj.output_scale', 'model.language_model.layers.32.self_attn.k_proj.weight._data._data', 'model.language_model.layers.32.self_attn.k_proj.weight._scale', 'model.language_model.layers.32.self_attn.k_proj.weight._shift', 'model.language_model.layers.32.self_attn.o_proj.input_scale', 'model.language_model.layers.32.self_attn.o_proj.output_scale', 'model.language_model.layers.32.self_attn.o_proj.weight._data._data', 'model.language_model.layers.32.self_attn.o_proj.weight._scale', 'model.language_model.layers.32.self_attn.o_proj.weight._shift', 'model.language_model.layers.32.self_attn.q_proj.input_scale', 'model.language_model.layers.32.self_attn.q_proj.output_scale', 'model.language_model.layers.32.self_attn.q_proj.weight._data._data', 'model.language_model.layers.32.self_attn.q_proj.weight._scale', 'model.language_model.layers.32.self_attn.q_proj.weight._shift', 'model.language_model.layers.32.self_attn.v_proj.input_scale', 'model.language_model.layers.32.self_attn.v_proj.output_scale', 'model.language_model.layers.32.self_attn.v_proj.weight._data._data', 'model.language_model.layers.32.self_attn.v_proj.weight._scale', 'model.language_model.layers.32.self_attn.v_proj.weight._shift', 'model.language_model.layers.33.mlp.down_proj.input_scale', 'model.language_model.layers.33.mlp.down_proj.output_scale', 'model.language_model.layers.33.mlp.down_proj.weight._data._data', 'model.language_model.layers.33.mlp.down_proj.weight._scale', 'model.language_model.layers.33.mlp.down_proj.weight._shift', 'model.language_model.layers.33.mlp.gate_proj.input_scale', 'model.language_model.layers.33.mlp.gate_proj.output_scale', 'model.language_model.layers.33.mlp.gate_proj.weight._data._data', 'model.language_model.layers.33.mlp.gate_proj.weight._scale', 'model.language_model.layers.33.mlp.gate_proj.weight._shift', 'model.language_model.layers.33.mlp.up_proj.input_scale', 'model.language_model.layers.33.mlp.up_proj.output_scale', 'model.language_model.layers.33.mlp.up_proj.weight._data._data', 'model.language_model.layers.33.mlp.up_proj.weight._scale', 'model.language_model.layers.33.mlp.up_proj.weight._shift', 'model.language_model.layers.33.self_attn.k_proj.input_scale', 'model.language_model.layers.33.self_attn.k_proj.output_scale', 'model.language_model.layers.33.self_attn.k_proj.weight._data._data', 'model.language_model.layers.33.self_attn.k_proj.weight._scale', 'model.language_model.layers.33.self_attn.k_proj.weight._shift', 'model.language_model.layers.33.self_attn.o_proj.input_scale', 'model.language_model.layers.33.self_attn.o_proj.output_scale', 'model.language_model.layers.33.self_attn.o_proj.weight._data._data', 'model.language_model.layers.33.self_attn.o_proj.weight._scale', 'model.language_model.layers.33.self_attn.o_proj.weight._shift', 'model.language_model.layers.33.self_attn.q_proj.input_scale', 'model.language_model.layers.33.self_attn.q_proj.output_scale', 'model.language_model.layers.33.self_attn.q_proj.weight._data._data', 'model.language_model.layers.33.self_attn.q_proj.weight._scale', 'model.language_model.layers.33.self_attn.q_proj.weight._shift', 'model.language_model.layers.33.self_attn.v_proj.input_scale', 'model.language_model.layers.33.self_attn.v_proj.output_scale', 'model.language_model.layers.33.self_attn.v_proj.weight._data._data', 'model.language_model.layers.33.self_attn.v_proj.weight._scale', 'model.language_model.layers.33.self_attn.v_proj.weight._shift', 'model.language_model.layers.34.mlp.down_proj.input_scale', 'model.language_model.layers.34.mlp.down_proj.output_scale', 'model.language_model.layers.34.mlp.down_proj.weight._data._data', 'model.language_model.layers.34.mlp.down_proj.weight._scale', 'model.language_model.layers.34.mlp.down_proj.weight._shift', 'model.language_model.layers.34.mlp.gate_proj.input_scale', 'model.language_model.layers.34.mlp.gate_proj.output_scale', 'model.language_model.layers.34.mlp.gate_proj.weight._data._data', 'model.language_model.layers.34.mlp.gate_proj.weight._scale', 'model.language_model.layers.34.mlp.gate_proj.weight._shift', 'model.language_model.layers.34.mlp.up_proj.input_scale', 'model.language_model.layers.34.mlp.up_proj.output_scale', 'model.language_model.layers.34.mlp.up_proj.weight._data._data', 'model.language_model.layers.34.mlp.up_proj.weight._scale', 'model.language_model.layers.34.mlp.up_proj.weight._shift', 'model.language_model.layers.34.self_attn.k_proj.input_scale', 'model.language_model.layers.34.self_attn.k_proj.output_scale', 'model.language_model.layers.34.self_attn.k_proj.weight._data._data', 'model.language_model.layers.34.self_attn.k_proj.weight._scale', 'model.language_model.layers.34.self_attn.k_proj.weight._shift', 'model.language_model.layers.34.self_attn.o_proj.input_scale', 'model.language_model.layers.34.self_attn.o_proj.output_scale', 'model.language_model.layers.34.self_attn.o_proj.weight._data._data', 'model.language_model.layers.34.self_attn.o_proj.weight._scale', 'model.language_model.layers.34.self_attn.o_proj.weight._shift', 'model.language_model.layers.34.self_attn.q_proj.input_scale', 'model.language_model.layers.34.self_attn.q_proj.output_scale', 'model.language_model.layers.34.self_attn.q_proj.weight._data._data', 'model.language_model.layers.34.self_attn.q_proj.weight._scale', 'model.language_model.layers.34.self_attn.q_proj.weight._shift', 'model.language_model.layers.34.self_attn.v_proj.input_scale', 'model.language_model.layers.34.self_attn.v_proj.output_scale', 'model.language_model.layers.34.self_attn.v_proj.weight._data._data', 'model.language_model.layers.34.self_attn.v_proj.weight._scale', 'model.language_model.layers.34.self_attn.v_proj.weight._shift', 'model.language_model.layers.35.mlp.down_proj.input_scale', 'model.language_model.layers.35.mlp.down_proj.output_scale', 'model.language_model.layers.35.mlp.down_proj.weight._data._data', 'model.language_model.layers.35.mlp.down_proj.weight._scale', 'model.language_model.layers.35.mlp.down_proj.weight._shift', 'model.language_model.layers.35.mlp.gate_proj.input_scale', 'model.language_model.layers.35.mlp.gate_proj.output_scale', 'model.language_model.layers.35.mlp.gate_proj.weight._data._data', 'model.language_model.layers.35.mlp.gate_proj.weight._scale', 'model.language_model.layers.35.mlp.gate_proj.weight._shift', 'model.language_model.layers.35.mlp.up_proj.input_scale', 'model.language_model.layers.35.mlp.up_proj.output_scale', 'model.language_model.layers.35.mlp.up_proj.weight._data._data', 'model.language_model.layers.35.mlp.up_proj.weight._scale', 'model.language_model.layers.35.mlp.up_proj.weight._shift', 'model.language_model.layers.35.self_attn.k_proj.input_scale', 'model.language_model.layers.35.self_attn.k_proj.output_scale', 'model.language_model.layers.35.self_attn.k_proj.weight._data._data', 'model.language_model.layers.35.self_attn.k_proj.weight._scale', 'model.language_model.layers.35.self_attn.k_proj.weight._shift', 'model.language_model.layers.35.self_attn.o_proj.input_scale', 'model.language_model.layers.35.self_attn.o_proj.output_scale', 'model.language_model.layers.35.self_attn.o_proj.weight._data._data', 'model.language_model.layers.35.self_attn.o_proj.weight._scale', 'model.language_model.layers.35.self_attn.o_proj.weight._shift', 'model.language_model.layers.35.self_attn.q_proj.input_scale', 'model.language_model.layers.35.self_attn.q_proj.output_scale', 'model.language_model.layers.35.self_attn.q_proj.weight._data._data', 'model.language_model.layers.35.self_attn.q_proj.weight._scale', 'model.language_model.layers.35.self_attn.q_proj.weight._shift', 'model.language_model.layers.35.self_attn.v_proj.input_scale', 'model.language_model.layers.35.self_attn.v_proj.output_scale', 'model.language_model.layers.35.self_attn.v_proj.weight._data._data', 'model.language_model.layers.35.self_attn.v_proj.weight._scale', 'model.language_model.layers.35.self_attn.v_proj.weight._shift', 'model.language_model.layers.36.mlp.down_proj.input_scale', 'model.language_model.layers.36.mlp.down_proj.output_scale', 'model.language_model.layers.36.mlp.down_proj.weight._data._data', 'model.language_model.layers.36.mlp.down_proj.weight._scale', 'model.language_model.layers.36.mlp.down_proj.weight._shift', 'model.language_model.layers.36.mlp.gate_proj.input_scale', 'model.language_model.layers.36.mlp.gate_proj.output_scale', 'model.language_model.layers.36.mlp.gate_proj.weight._data._data', 'model.language_model.layers.36.mlp.gate_proj.weight._scale', 'model.language_model.layers.36.mlp.gate_proj.weight._shift', 'model.language_model.layers.36.mlp.up_proj.input_scale', 'model.language_model.layers.36.mlp.up_proj.output_scale', 'model.language_model.layers.36.mlp.up_proj.weight._data._data', 'model.language_model.layers.36.mlp.up_proj.weight._scale', 'model.language_model.layers.36.mlp.up_proj.weight._shift', 'model.language_model.layers.36.self_attn.k_proj.input_scale', 'model.language_model.layers.36.self_attn.k_proj.output_scale', 'model.language_model.layers.36.self_attn.k_proj.weight._data._data', 'model.language_model.layers.36.self_attn.k_proj.weight._scale', 'model.language_model.layers.36.self_attn.k_proj.weight._shift', 'model.language_model.layers.36.self_attn.o_proj.input_scale', 'model.language_model.layers.36.self_attn.o_proj.output_scale', 'model.language_model.layers.36.self_attn.o_proj.weight._data._data', 'model.language_model.layers.36.self_attn.o_proj.weight._scale', 'model.language_model.layers.36.self_attn.o_proj.weight._shift', 'model.language_model.layers.36.self_attn.q_proj.input_scale', 'model.language_model.layers.36.self_attn.q_proj.output_scale', 'model.language_model.layers.36.self_attn.q_proj.weight._data._data', 'model.language_model.layers.36.self_attn.q_proj.weight._scale', 'model.language_model.layers.36.self_attn.q_proj.weight._shift', 'model.language_model.layers.36.self_attn.v_proj.input_scale', 'model.language_model.layers.36.self_attn.v_proj.output_scale', 'model.language_model.layers.36.self_attn.v_proj.weight._data._data', 'model.language_model.layers.36.self_attn.v_proj.weight._scale', 'model.language_model.layers.36.self_attn.v_proj.weight._shift', 'model.language_model.layers.37.mlp.down_proj.input_scale', 'model.language_model.layers.37.mlp.down_proj.output_scale', 'model.language_model.layers.37.mlp.down_proj.weight._data._data', 'model.language_model.layers.37.mlp.down_proj.weight._scale', 'model.language_model.layers.37.mlp.down_proj.weight._shift', 'model.language_model.layers.37.mlp.gate_proj.input_scale', 'model.language_model.layers.37.mlp.gate_proj.output_scale', 'model.language_model.layers.37.mlp.gate_proj.weight._data._data', 'model.language_model.layers.37.mlp.gate_proj.weight._scale', 'model.language_model.layers.37.mlp.gate_proj.weight._shift', 'model.language_model.layers.37.mlp.up_proj.input_scale', 'model.language_model.layers.37.mlp.up_proj.output_scale', 'model.language_model.layers.37.mlp.up_proj.weight._data._data', 'model.language_model.layers.37.mlp.up_proj.weight._scale', 'model.language_model.layers.37.mlp.up_proj.weight._shift', 'model.language_model.layers.37.self_attn.k_proj.input_scale', 'model.language_model.layers.37.self_attn.k_proj.output_scale', 'model.language_model.layers.37.self_attn.k_proj.weight._data._data', 'model.language_model.layers.37.self_attn.k_proj.weight._scale', 'model.language_model.layers.37.self_attn.k_proj.weight._shift', 'model.language_model.layers.37.self_attn.o_proj.input_scale', 'model.language_model.layers.37.self_attn.o_proj.output_scale', 'model.language_model.layers.37.self_attn.o_proj.weight._data._data', 'model.language_model.layers.37.self_attn.o_proj.weight._scale', 'model.language_model.layers.37.self_attn.o_proj.weight._shift', 'model.language_model.layers.37.self_attn.q_proj.input_scale', 'model.language_model.layers.37.self_attn.q_proj.output_scale', 'model.language_model.layers.37.self_attn.q_proj.weight._data._data', 'model.language_model.layers.37.self_attn.q_proj.weight._scale', 'model.language_model.layers.37.self_attn.q_proj.weight._shift', 'model.language_model.layers.37.self_attn.v_proj.input_scale', 'model.language_model.layers.37.self_attn.v_proj.output_scale', 'model.language_model.layers.37.self_attn.v_proj.weight._data._data', 'model.language_model.layers.37.self_attn.v_proj.weight._scale', 'model.language_model.layers.37.self_attn.v_proj.weight._shift', 'model.language_model.layers.38.mlp.down_proj.input_scale', 'model.language_model.layers.38.mlp.down_proj.output_scale', 'model.language_model.layers.38.mlp.down_proj.weight._data._data', 'model.language_model.layers.38.mlp.down_proj.weight._scale', 'model.language_model.layers.38.mlp.down_proj.weight._shift', 'model.language_model.layers.38.mlp.gate_proj.input_scale', 'model.language_model.layers.38.mlp.gate_proj.output_scale', 'model.language_model.layers.38.mlp.gate_proj.weight._data._data', 'model.language_model.layers.38.mlp.gate_proj.weight._scale', 'model.language_model.layers.38.mlp.gate_proj.weight._shift', 'model.language_model.layers.38.mlp.up_proj.input_scale', 'model.language_model.layers.38.mlp.up_proj.output_scale', 'model.language_model.layers.38.mlp.up_proj.weight._data._data', 'model.language_model.layers.38.mlp.up_proj.weight._scale', 'model.language_model.layers.38.mlp.up_proj.weight._shift', 'model.language_model.layers.38.self_attn.k_proj.input_scale', 'model.language_model.layers.38.self_attn.k_proj.output_scale', 'model.language_model.layers.38.self_attn.k_proj.weight._data._data', 'model.language_model.layers.38.self_attn.k_proj.weight._scale', 'model.language_model.layers.38.self_attn.k_proj.weight._shift', 'model.language_model.layers.38.self_attn.o_proj.input_scale', 'model.language_model.layers.38.self_attn.o_proj.output_scale', 'model.language_model.layers.38.self_attn.o_proj.weight._data._data', 'model.language_model.layers.38.self_attn.o_proj.weight._scale', 'model.language_model.layers.38.self_attn.o_proj.weight._shift', 'model.language_model.layers.38.self_attn.q_proj.input_scale', 'model.language_model.layers.38.self_attn.q_proj.output_scale', 'model.language_model.layers.38.self_attn.q_proj.weight._data._data', 'model.language_model.layers.38.self_attn.q_proj.weight._scale', 'model.language_model.layers.38.self_attn.q_proj.weight._shift', 'model.language_model.layers.38.self_attn.v_proj.input_scale', 'model.language_model.layers.38.self_attn.v_proj.output_scale', 'model.language_model.layers.38.self_attn.v_proj.weight._data._data', 'model.language_model.layers.38.self_attn.v_proj.weight._scale', 'model.language_model.layers.38.self_attn.v_proj.weight._shift', 'model.language_model.layers.39.mlp.down_proj.input_scale', 'model.language_model.layers.39.mlp.down_proj.output_scale', 'model.language_model.layers.39.mlp.down_proj.weight._data._data', 'model.language_model.layers.39.mlp.down_proj.weight._scale', 'model.language_model.layers.39.mlp.down_proj.weight._shift', 'model.language_model.layers.39.mlp.gate_proj.input_scale', 'model.language_model.layers.39.mlp.gate_proj.output_scale', 'model.language_model.layers.39.mlp.gate_proj.weight._data._data', 'model.language_model.layers.39.mlp.gate_proj.weight._scale', 'model.language_model.layers.39.mlp.gate_proj.weight._shift', 'model.language_model.layers.39.mlp.up_proj.input_scale', 'model.language_model.layers.39.mlp.up_proj.output_scale', 'model.language_model.layers.39.mlp.up_proj.weight._data._data', 'model.language_model.layers.39.mlp.up_proj.weight._scale', 'model.language_model.layers.39.mlp.up_proj.weight._shift', 'model.language_model.layers.39.self_attn.k_proj.input_scale', 'model.language_model.layers.39.self_attn.k_proj.output_scale', 'model.language_model.layers.39.self_attn.k_proj.weight._data._data', 'model.language_model.layers.39.self_attn.k_proj.weight._scale', 'model.language_model.layers.39.self_attn.k_proj.weight._shift', 'model.language_model.layers.39.self_attn.o_proj.input_scale', 'model.language_model.layers.39.self_attn.o_proj.output_scale', 'model.language_model.layers.39.self_attn.o_proj.weight._data._data', 'model.language_model.layers.39.self_attn.o_proj.weight._scale', 'model.language_model.layers.39.self_attn.o_proj.weight._shift', 'model.language_model.layers.39.self_attn.q_proj.input_scale', 'model.language_model.layers.39.self_attn.q_proj.output_scale', 'model.language_model.layers.39.self_attn.q_proj.weight._data._data', 'model.language_model.layers.39.self_attn.q_proj.weight._scale', 'model.language_model.layers.39.self_attn.q_proj.weight._shift', 'model.language_model.layers.39.self_attn.v_proj.input_scale', 'model.language_model.layers.39.self_attn.v_proj.output_scale', 'model.language_model.layers.39.self_attn.v_proj.weight._data._data', 'model.language_model.layers.39.self_attn.v_proj.weight._scale', 'model.language_model.layers.39.self_attn.v_proj.weight._shift', 'model.language_model.layers.4.mlp.down_proj.input_scale', 'model.language_model.layers.4.mlp.down_proj.output_scale', 'model.language_model.layers.4.mlp.down_proj.weight._data._data', 'model.language_model.layers.4.mlp.down_proj.weight._scale', 'model.language_model.layers.4.mlp.down_proj.weight._shift', 'model.language_model.layers.4.mlp.gate_proj.input_scale', 'model.language_model.layers.4.mlp.gate_proj.output_scale', 'model.language_model.layers.4.mlp.gate_proj.weight._data._data', 'model.language_model.layers.4.mlp.gate_proj.weight._scale', 'model.language_model.layers.4.mlp.gate_proj.weight._shift', 'model.language_model.layers.4.mlp.up_proj.input_scale', 'model.language_model.layers.4.mlp.up_proj.output_scale', 'model.language_model.layers.4.mlp.up_proj.weight._data._data', 'model.language_model.layers.4.mlp.up_proj.weight._scale', 'model.language_model.layers.4.mlp.up_proj.weight._shift', 'model.language_model.layers.4.self_attn.k_proj.input_scale', 'model.language_model.layers.4.self_attn.k_proj.output_scale', 'model.language_model.layers.4.self_attn.k_proj.weight._data._data', 'model.language_model.layers.4.self_attn.k_proj.weight._scale', 'model.language_model.layers.4.self_attn.k_proj.weight._shift', 'model.language_model.layers.4.self_attn.o_proj.input_scale', 'model.language_model.layers.4.self_attn.o_proj.output_scale', 'model.language_model.layers.4.self_attn.o_proj.weight._data._data', 'model.language_model.layers.4.self_attn.o_proj.weight._scale', 'model.language_model.layers.4.self_attn.o_proj.weight._shift', 'model.language_model.layers.4.self_attn.q_proj.input_scale', 'model.language_model.layers.4.self_attn.q_proj.output_scale', 'model.language_model.layers.4.self_attn.q_proj.weight._data._data', 'model.language_model.layers.4.self_attn.q_proj.weight._scale', 'model.language_model.layers.4.self_attn.q_proj.weight._shift', 'model.language_model.layers.4.self_attn.v_proj.input_scale', 'model.language_model.layers.4.self_attn.v_proj.output_scale', 'model.language_model.layers.4.self_attn.v_proj.weight._data._data', 'model.language_model.layers.4.self_attn.v_proj.weight._scale', 'model.language_model.layers.4.self_attn.v_proj.weight._shift', 'model.language_model.layers.40.mlp.down_proj.input_scale', 'model.language_model.layers.40.mlp.down_proj.output_scale', 'model.language_model.layers.40.mlp.down_proj.weight._data._data', 'model.language_model.layers.40.mlp.down_proj.weight._scale', 'model.language_model.layers.40.mlp.down_proj.weight._shift', 'model.language_model.layers.40.mlp.gate_proj.input_scale', 'model.language_model.layers.40.mlp.gate_proj.output_scale', 'model.language_model.layers.40.mlp.gate_proj.weight._data._data', 'model.language_model.layers.40.mlp.gate_proj.weight._scale', 'model.language_model.layers.40.mlp.gate_proj.weight._shift', 'model.language_model.layers.40.mlp.up_proj.input_scale', 'model.language_model.layers.40.mlp.up_proj.output_scale', 'model.language_model.layers.40.mlp.up_proj.weight._data._data', 'model.language_model.layers.40.mlp.up_proj.weight._scale', 'model.language_model.layers.40.mlp.up_proj.weight._shift', 'model.language_model.layers.40.self_attn.k_proj.input_scale', 'model.language_model.layers.40.self_attn.k_proj.output_scale', 'model.language_model.layers.40.self_attn.k_proj.weight._data._data', 'model.language_model.layers.40.self_attn.k_proj.weight._scale', 'model.language_model.layers.40.self_attn.k_proj.weight._shift', 'model.language_model.layers.40.self_attn.o_proj.input_scale', 'model.language_model.layers.40.self_attn.o_proj.output_scale', 'model.language_model.layers.40.self_attn.o_proj.weight._data._data', 'model.language_model.layers.40.self_attn.o_proj.weight._scale', 'model.language_model.layers.40.self_attn.o_proj.weight._shift', 'model.language_model.layers.40.self_attn.q_proj.input_scale', 'model.language_model.layers.40.self_attn.q_proj.output_scale', 'model.language_model.layers.40.self_attn.q_proj.weight._data._data', 'model.language_model.layers.40.self_attn.q_proj.weight._scale', 'model.language_model.layers.40.self_attn.q_proj.weight._shift', 'model.language_model.layers.40.self_attn.v_proj.input_scale', 'model.language_model.layers.40.self_attn.v_proj.output_scale', 'model.language_model.layers.40.self_attn.v_proj.weight._data._data', 'model.language_model.layers.40.self_attn.v_proj.weight._scale', 'model.language_model.layers.40.self_attn.v_proj.weight._shift', 'model.language_model.layers.41.mlp.down_proj.input_scale', 'model.language_model.layers.41.mlp.down_proj.output_scale', 'model.language_model.layers.41.mlp.down_proj.weight._data._data', 'model.language_model.layers.41.mlp.down_proj.weight._scale', 'model.language_model.layers.41.mlp.down_proj.weight._shift', 'model.language_model.layers.41.mlp.gate_proj.input_scale', 'model.language_model.layers.41.mlp.gate_proj.output_scale', 'model.language_model.layers.41.mlp.gate_proj.weight._data._data', 'model.language_model.layers.41.mlp.gate_proj.weight._scale', 'model.language_model.layers.41.mlp.gate_proj.weight._shift', 'model.language_model.layers.41.mlp.up_proj.input_scale', 'model.language_model.layers.41.mlp.up_proj.output_scale', 'model.language_model.layers.41.mlp.up_proj.weight._data._data', 'model.language_model.layers.41.mlp.up_proj.weight._scale', 'model.language_model.layers.41.mlp.up_proj.weight._shift', 'model.language_model.layers.41.self_attn.k_proj.input_scale', 'model.language_model.layers.41.self_attn.k_proj.output_scale', 'model.language_model.layers.41.self_attn.k_proj.weight._data._data', 'model.language_model.layers.41.self_attn.k_proj.weight._scale', 'model.language_model.layers.41.self_attn.k_proj.weight._shift', 'model.language_model.layers.41.self_attn.o_proj.input_scale', 'model.language_model.layers.41.self_attn.o_proj.output_scale', 'model.language_model.layers.41.self_attn.o_proj.weight._data._data', 'model.language_model.layers.41.self_attn.o_proj.weight._scale', 'model.language_model.layers.41.self_attn.o_proj.weight._shift', 'model.language_model.layers.41.self_attn.q_proj.input_scale', 'model.language_model.layers.41.self_attn.q_proj.output_scale', 'model.language_model.layers.41.self_attn.q_proj.weight._data._data', 'model.language_model.layers.41.self_attn.q_proj.weight._scale', 'model.language_model.layers.41.self_attn.q_proj.weight._shift', 'model.language_model.layers.41.self_attn.v_proj.input_scale', 'model.language_model.layers.41.self_attn.v_proj.output_scale', 'model.language_model.layers.41.self_attn.v_proj.weight._data._data', 'model.language_model.layers.41.self_attn.v_proj.weight._scale', 'model.language_model.layers.41.self_attn.v_proj.weight._shift', 'model.language_model.layers.42.mlp.down_proj.input_scale', 'model.language_model.layers.42.mlp.down_proj.output_scale', 'model.language_model.layers.42.mlp.down_proj.weight._data._data', 'model.language_model.layers.42.mlp.down_proj.weight._scale', 'model.language_model.layers.42.mlp.down_proj.weight._shift', 'model.language_model.layers.42.mlp.gate_proj.input_scale', 'model.language_model.layers.42.mlp.gate_proj.output_scale', 'model.language_model.layers.42.mlp.gate_proj.weight._data._data', 'model.language_model.layers.42.mlp.gate_proj.weight._scale', 'model.language_model.layers.42.mlp.gate_proj.weight._shift', 'model.language_model.layers.42.mlp.up_proj.input_scale', 'model.language_model.layers.42.mlp.up_proj.output_scale', 'model.language_model.layers.42.mlp.up_proj.weight._data._data', 'model.language_model.layers.42.mlp.up_proj.weight._scale', 'model.language_model.layers.42.mlp.up_proj.weight._shift', 'model.language_model.layers.42.self_attn.k_proj.input_scale', 'model.language_model.layers.42.self_attn.k_proj.output_scale', 'model.language_model.layers.42.self_attn.k_proj.weight._data._data', 'model.language_model.layers.42.self_attn.k_proj.weight._scale', 'model.language_model.layers.42.self_attn.k_proj.weight._shift', 'model.language_model.layers.42.self_attn.o_proj.input_scale', 'model.language_model.layers.42.self_attn.o_proj.output_scale', 'model.language_model.layers.42.self_attn.o_proj.weight._data._data', 'model.language_model.layers.42.self_attn.o_proj.weight._scale', 'model.language_model.layers.42.self_attn.o_proj.weight._shift', 'model.language_model.layers.42.self_attn.q_proj.input_scale', 'model.language_model.layers.42.self_attn.q_proj.output_scale', 'model.language_model.layers.42.self_attn.q_proj.weight._data._data', 'model.language_model.layers.42.self_attn.q_proj.weight._scale', 'model.language_model.layers.42.self_attn.q_proj.weight._shift', 'model.language_model.layers.42.self_attn.v_proj.input_scale', 'model.language_model.layers.42.self_attn.v_proj.output_scale', 'model.language_model.layers.42.self_attn.v_proj.weight._data._data', 'model.language_model.layers.42.self_attn.v_proj.weight._scale', 'model.language_model.layers.42.self_attn.v_proj.weight._shift', 'model.language_model.layers.43.mlp.down_proj.input_scale', 'model.language_model.layers.43.mlp.down_proj.output_scale', 'model.language_model.layers.43.mlp.down_proj.weight._data._data', 'model.language_model.layers.43.mlp.down_proj.weight._scale', 'model.language_model.layers.43.mlp.down_proj.weight._shift', 'model.language_model.layers.43.mlp.gate_proj.input_scale', 'model.language_model.layers.43.mlp.gate_proj.output_scale', 'model.language_model.layers.43.mlp.gate_proj.weight._data._data', 'model.language_model.layers.43.mlp.gate_proj.weight._scale', 'model.language_model.layers.43.mlp.gate_proj.weight._shift', 'model.language_model.layers.43.mlp.up_proj.input_scale', 'model.language_model.layers.43.mlp.up_proj.output_scale', 'model.language_model.layers.43.mlp.up_proj.weight._data._data', 'model.language_model.layers.43.mlp.up_proj.weight._scale', 'model.language_model.layers.43.mlp.up_proj.weight._shift', 'model.language_model.layers.43.self_attn.k_proj.input_scale', 'model.language_model.layers.43.self_attn.k_proj.output_scale', 'model.language_model.layers.43.self_attn.k_proj.weight._data._data', 'model.language_model.layers.43.self_attn.k_proj.weight._scale', 'model.language_model.layers.43.self_attn.k_proj.weight._shift', 'model.language_model.layers.43.self_attn.o_proj.input_scale', 'model.language_model.layers.43.self_attn.o_proj.output_scale', 'model.language_model.layers.43.self_attn.o_proj.weight._data._data', 'model.language_model.layers.43.self_attn.o_proj.weight._scale', 'model.language_model.layers.43.self_attn.o_proj.weight._shift', 'model.language_model.layers.43.self_attn.q_proj.input_scale', 'model.language_model.layers.43.self_attn.q_proj.output_scale', 'model.language_model.layers.43.self_attn.q_proj.weight._data._data', 'model.language_model.layers.43.self_attn.q_proj.weight._scale', 'model.language_model.layers.43.self_attn.q_proj.weight._shift', 'model.language_model.layers.43.self_attn.v_proj.input_scale', 'model.language_model.layers.43.self_attn.v_proj.output_scale', 'model.language_model.layers.43.self_attn.v_proj.weight._data._data', 'model.language_model.layers.43.self_attn.v_proj.weight._scale', 'model.language_model.layers.43.self_attn.v_proj.weight._shift', 'model.language_model.layers.44.mlp.down_proj.input_scale', 'model.language_model.layers.44.mlp.down_proj.output_scale', 'model.language_model.layers.44.mlp.down_proj.weight._data._data', 'model.language_model.layers.44.mlp.down_proj.weight._scale', 'model.language_model.layers.44.mlp.down_proj.weight._shift', 'model.language_model.layers.44.mlp.gate_proj.input_scale', 'model.language_model.layers.44.mlp.gate_proj.output_scale', 'model.language_model.layers.44.mlp.gate_proj.weight._data._data', 'model.language_model.layers.44.mlp.gate_proj.weight._scale', 'model.language_model.layers.44.mlp.gate_proj.weight._shift', 'model.language_model.layers.44.mlp.up_proj.input_scale', 'model.language_model.layers.44.mlp.up_proj.output_scale', 'model.language_model.layers.44.mlp.up_proj.weight._data._data', 'model.language_model.layers.44.mlp.up_proj.weight._scale', 'model.language_model.layers.44.mlp.up_proj.weight._shift', 'model.language_model.layers.44.self_attn.k_proj.input_scale', 'model.language_model.layers.44.self_attn.k_proj.output_scale', 'model.language_model.layers.44.self_attn.k_proj.weight._data._data', 'model.language_model.layers.44.self_attn.k_proj.weight._scale', 'model.language_model.layers.44.self_attn.k_proj.weight._shift', 'model.language_model.layers.44.self_attn.o_proj.input_scale', 'model.language_model.layers.44.self_attn.o_proj.output_scale', 'model.language_model.layers.44.self_attn.o_proj.weight._data._data', 'model.language_model.layers.44.self_attn.o_proj.weight._scale', 'model.language_model.layers.44.self_attn.o_proj.weight._shift', 'model.language_model.layers.44.self_attn.q_proj.input_scale', 'model.language_model.layers.44.self_attn.q_proj.output_scale', 'model.language_model.layers.44.self_attn.q_proj.weight._data._data', 'model.language_model.layers.44.self_attn.q_proj.weight._scale', 'model.language_model.layers.44.self_attn.q_proj.weight._shift', 'model.language_model.layers.44.self_attn.v_proj.input_scale', 'model.language_model.layers.44.self_attn.v_proj.output_scale', 'model.language_model.layers.44.self_attn.v_proj.weight._data._data', 'model.language_model.layers.44.self_attn.v_proj.weight._scale', 'model.language_model.layers.44.self_attn.v_proj.weight._shift', 'model.language_model.layers.45.mlp.down_proj.input_scale', 'model.language_model.layers.45.mlp.down_proj.output_scale', 'model.language_model.layers.45.mlp.down_proj.weight._data._data', 'model.language_model.layers.45.mlp.down_proj.weight._scale', 'model.language_model.layers.45.mlp.down_proj.weight._shift', 'model.language_model.layers.45.mlp.gate_proj.input_scale', 'model.language_model.layers.45.mlp.gate_proj.output_scale', 'model.language_model.layers.45.mlp.gate_proj.weight._data._data', 'model.language_model.layers.45.mlp.gate_proj.weight._scale', 'model.language_model.layers.45.mlp.gate_proj.weight._shift', 'model.language_model.layers.45.mlp.up_proj.input_scale', 'model.language_model.layers.45.mlp.up_proj.output_scale', 'model.language_model.layers.45.mlp.up_proj.weight._data._data', 'model.language_model.layers.45.mlp.up_proj.weight._scale', 'model.language_model.layers.45.mlp.up_proj.weight._shift', 'model.language_model.layers.45.self_attn.k_proj.input_scale', 'model.language_model.layers.45.self_attn.k_proj.output_scale', 'model.language_model.layers.45.self_attn.k_proj.weight._data._data', 'model.language_model.layers.45.self_attn.k_proj.weight._scale', 'model.language_model.layers.45.self_attn.k_proj.weight._shift', 'model.language_model.layers.45.self_attn.o_proj.input_scale', 'model.language_model.layers.45.self_attn.o_proj.output_scale', 'model.language_model.layers.45.self_attn.o_proj.weight._data._data', 'model.language_model.layers.45.self_attn.o_proj.weight._scale', 'model.language_model.layers.45.self_attn.o_proj.weight._shift', 'model.language_model.layers.45.self_attn.q_proj.input_scale', 'model.language_model.layers.45.self_attn.q_proj.output_scale', 'model.language_model.layers.45.self_attn.q_proj.weight._data._data', 'model.language_model.layers.45.self_attn.q_proj.weight._scale', 'model.language_model.layers.45.self_attn.q_proj.weight._shift', 'model.language_model.layers.45.self_attn.v_proj.input_scale', 'model.language_model.layers.45.self_attn.v_proj.output_scale', 'model.language_model.layers.45.self_attn.v_proj.weight._data._data', 'model.language_model.layers.45.self_attn.v_proj.weight._scale', 'model.language_model.layers.45.self_attn.v_proj.weight._shift', 'model.language_model.layers.46.mlp.down_proj.input_scale', 'model.language_model.layers.46.mlp.down_proj.output_scale', 'model.language_model.layers.46.mlp.down_proj.weight._data._data', 'model.language_model.layers.46.mlp.down_proj.weight._scale', 'model.language_model.layers.46.mlp.down_proj.weight._shift', 'model.language_model.layers.46.mlp.gate_proj.input_scale', 'model.language_model.layers.46.mlp.gate_proj.output_scale', 'model.language_model.layers.46.mlp.gate_proj.weight._data._data', 'model.language_model.layers.46.mlp.gate_proj.weight._scale', 'model.language_model.layers.46.mlp.gate_proj.weight._shift', 'model.language_model.layers.46.mlp.up_proj.input_scale', 'model.language_model.layers.46.mlp.up_proj.output_scale', 'model.language_model.layers.46.mlp.up_proj.weight._data._data', 'model.language_model.layers.46.mlp.up_proj.weight._scale', 'model.language_model.layers.46.mlp.up_proj.weight._shift', 'model.language_model.layers.46.self_attn.k_proj.input_scale', 'model.language_model.layers.46.self_attn.k_proj.output_scale', 'model.language_model.layers.46.self_attn.k_proj.weight._data._data', 'model.language_model.layers.46.self_attn.k_proj.weight._scale', 'model.language_model.layers.46.self_attn.k_proj.weight._shift', 'model.language_model.layers.46.self_attn.o_proj.input_scale', 'model.language_model.layers.46.self_attn.o_proj.output_scale', 'model.language_model.layers.46.self_attn.o_proj.weight._data._data', 'model.language_model.layers.46.self_attn.o_proj.weight._scale', 'model.language_model.layers.46.self_attn.o_proj.weight._shift', 'model.language_model.layers.46.self_attn.q_proj.input_scale', 'model.language_model.layers.46.self_attn.q_proj.output_scale', 'model.language_model.layers.46.self_attn.q_proj.weight._data._data', 'model.language_model.layers.46.self_attn.q_proj.weight._scale', 'model.language_model.layers.46.self_attn.q_proj.weight._shift', 'model.language_model.layers.46.self_attn.v_proj.input_scale', 'model.language_model.layers.46.self_attn.v_proj.output_scale', 'model.language_model.layers.46.self_attn.v_proj.weight._data._data', 'model.language_model.layers.46.self_attn.v_proj.weight._scale', 'model.language_model.layers.46.self_attn.v_proj.weight._shift', 'model.language_model.layers.47.mlp.down_proj.input_scale', 'model.language_model.layers.47.mlp.down_proj.output_scale', 'model.language_model.layers.47.mlp.down_proj.weight._data._data', 'model.language_model.layers.47.mlp.down_proj.weight._scale', 'model.language_model.layers.47.mlp.down_proj.weight._shift', 'model.language_model.layers.47.mlp.gate_proj.input_scale', 'model.language_model.layers.47.mlp.gate_proj.output_scale', 'model.language_model.layers.47.mlp.gate_proj.weight._data._data', 'model.language_model.layers.47.mlp.gate_proj.weight._scale', 'model.language_model.layers.47.mlp.gate_proj.weight._shift', 'model.language_model.layers.47.mlp.up_proj.input_scale', 'model.language_model.layers.47.mlp.up_proj.output_scale', 'model.language_model.layers.47.mlp.up_proj.weight._data._data', 'model.language_model.layers.47.mlp.up_proj.weight._scale', 'model.language_model.layers.47.mlp.up_proj.weight._shift', 'model.language_model.layers.47.self_attn.k_proj.input_scale', 'model.language_model.layers.47.self_attn.k_proj.output_scale', 'model.language_model.layers.47.self_attn.k_proj.weight._data._data', 'model.language_model.layers.47.self_attn.k_proj.weight._scale', 'model.language_model.layers.47.self_attn.k_proj.weight._shift', 'model.language_model.layers.47.self_attn.o_proj.input_scale', 'model.language_model.layers.47.self_attn.o_proj.output_scale', 'model.language_model.layers.47.self_attn.o_proj.weight._data._data', 'model.language_model.layers.47.self_attn.o_proj.weight._scale', 'model.language_model.layers.47.self_attn.o_proj.weight._shift', 'model.language_model.layers.47.self_attn.q_proj.input_scale', 'model.language_model.layers.47.self_attn.q_proj.output_scale', 'model.language_model.layers.47.self_attn.q_proj.weight._data._data', 'model.language_model.layers.47.self_attn.q_proj.weight._scale', 'model.language_model.layers.47.self_attn.q_proj.weight._shift', 'model.language_model.layers.47.self_attn.v_proj.input_scale', 'model.language_model.layers.47.self_attn.v_proj.output_scale', 'model.language_model.layers.47.self_attn.v_proj.weight._data._data', 'model.language_model.layers.47.self_attn.v_proj.weight._scale', 'model.language_model.layers.47.self_attn.v_proj.weight._shift', 'model.language_model.layers.5.mlp.down_proj.input_scale', 'model.language_model.layers.5.mlp.down_proj.output_scale', 'model.language_model.layers.5.mlp.down_proj.weight._data._data', 'model.language_model.layers.5.mlp.down_proj.weight._scale', 'model.language_model.layers.5.mlp.down_proj.weight._shift', 'model.language_model.layers.5.mlp.gate_proj.input_scale', 'model.language_model.layers.5.mlp.gate_proj.output_scale', 'model.language_model.layers.5.mlp.gate_proj.weight._data._data', 'model.language_model.layers.5.mlp.gate_proj.weight._scale', 'model.language_model.layers.5.mlp.gate_proj.weight._shift', 'model.language_model.layers.5.mlp.up_proj.input_scale', 'model.language_model.layers.5.mlp.up_proj.output_scale', 'model.language_model.layers.5.mlp.up_proj.weight._data._data', 'model.language_model.layers.5.mlp.up_proj.weight._scale', 'model.language_model.layers.5.mlp.up_proj.weight._shift', 'model.language_model.layers.5.self_attn.k_proj.input_scale', 'model.language_model.layers.5.self_attn.k_proj.output_scale', 'model.language_model.layers.5.self_attn.k_proj.weight._data._data', 'model.language_model.layers.5.self_attn.k_proj.weight._scale', 'model.language_model.layers.5.self_attn.k_proj.weight._shift', 'model.language_model.layers.5.self_attn.o_proj.input_scale', 'model.language_model.layers.5.self_attn.o_proj.output_scale', 'model.language_model.layers.5.self_attn.o_proj.weight._data._data', 'model.language_model.layers.5.self_attn.o_proj.weight._scale', 'model.language_model.layers.5.self_attn.o_proj.weight._shift', 'model.language_model.layers.5.self_attn.q_proj.input_scale', 'model.language_model.layers.5.self_attn.q_proj.output_scale', 'model.language_model.layers.5.self_attn.q_proj.weight._data._data', 'model.language_model.layers.5.self_attn.q_proj.weight._scale', 'model.language_model.layers.5.self_attn.q_proj.weight._shift', 'model.language_model.layers.5.self_attn.v_proj.input_scale', 'model.language_model.layers.5.self_attn.v_proj.output_scale', 'model.language_model.layers.5.self_attn.v_proj.weight._data._data', 'model.language_model.layers.5.self_attn.v_proj.weight._scale', 'model.language_model.layers.5.self_attn.v_proj.weight._shift', 'model.language_model.layers.6.mlp.down_proj.input_scale', 'model.language_model.layers.6.mlp.down_proj.output_scale', 'model.language_model.layers.6.mlp.down_proj.weight._data._data', 'model.language_model.layers.6.mlp.down_proj.weight._scale', 'model.language_model.layers.6.mlp.down_proj.weight._shift', 'model.language_model.layers.6.mlp.gate_proj.input_scale', 'model.language_model.layers.6.mlp.gate_proj.output_scale', 'model.language_model.layers.6.mlp.gate_proj.weight._data._data', 'model.language_model.layers.6.mlp.gate_proj.weight._scale', 'model.language_model.layers.6.mlp.gate_proj.weight._shift', 'model.language_model.layers.6.mlp.up_proj.input_scale', 'model.language_model.layers.6.mlp.up_proj.output_scale', 'model.language_model.layers.6.mlp.up_proj.weight._data._data', 'model.language_model.layers.6.mlp.up_proj.weight._scale', 'model.language_model.layers.6.mlp.up_proj.weight._shift', 'model.language_model.layers.6.self_attn.k_proj.input_scale', 'model.language_model.layers.6.self_attn.k_proj.output_scale', 'model.language_model.layers.6.self_attn.k_proj.weight._data._data', 'model.language_model.layers.6.self_attn.k_proj.weight._scale', 'model.language_model.layers.6.self_attn.k_proj.weight._shift', 'model.language_model.layers.6.self_attn.o_proj.input_scale', 'model.language_model.layers.6.self_attn.o_proj.output_scale', 'model.language_model.layers.6.self_attn.o_proj.weight._data._data', 'model.language_model.layers.6.self_attn.o_proj.weight._scale', 'model.language_model.layers.6.self_attn.o_proj.weight._shift', 'model.language_model.layers.6.self_attn.q_proj.input_scale', 'model.language_model.layers.6.self_attn.q_proj.output_scale', 'model.language_model.layers.6.self_attn.q_proj.weight._data._data', 'model.language_model.layers.6.self_attn.q_proj.weight._scale', 'model.language_model.layers.6.self_attn.q_proj.weight._shift', 'model.language_model.layers.6.self_attn.v_proj.input_scale', 'model.language_model.layers.6.self_attn.v_proj.output_scale', 'model.language_model.layers.6.self_attn.v_proj.weight._data._data', 'model.language_model.layers.6.self_attn.v_proj.weight._scale', 'model.language_model.layers.6.self_attn.v_proj.weight._shift', 'model.language_model.layers.7.mlp.down_proj.input_scale', 'model.language_model.layers.7.mlp.down_proj.output_scale', 'model.language_model.layers.7.mlp.down_proj.weight._data._data', 'model.language_model.layers.7.mlp.down_proj.weight._scale', 'model.language_model.layers.7.mlp.down_proj.weight._shift', 'model.language_model.layers.7.mlp.gate_proj.input_scale', 'model.language_model.layers.7.mlp.gate_proj.output_scale', 'model.language_model.layers.7.mlp.gate_proj.weight._data._data', 'model.language_model.layers.7.mlp.gate_proj.weight._scale', 'model.language_model.layers.7.mlp.gate_proj.weight._shift', 'model.language_model.layers.7.mlp.up_proj.input_scale', 'model.language_model.layers.7.mlp.up_proj.output_scale', 'model.language_model.layers.7.mlp.up_proj.weight._data._data', 'model.language_model.layers.7.mlp.up_proj.weight._scale', 'model.language_model.layers.7.mlp.up_proj.weight._shift', 'model.language_model.layers.7.self_attn.k_proj.input_scale', 'model.language_model.layers.7.self_attn.k_proj.output_scale', 'model.language_model.layers.7.self_attn.k_proj.weight._data._data', 'model.language_model.layers.7.self_attn.k_proj.weight._scale', 'model.language_model.layers.7.self_attn.k_proj.weight._shift', 'model.language_model.layers.7.self_attn.o_proj.input_scale', 'model.language_model.layers.7.self_attn.o_proj.output_scale', 'model.language_model.layers.7.self_attn.o_proj.weight._data._data', 'model.language_model.layers.7.self_attn.o_proj.weight._scale', 'model.language_model.layers.7.self_attn.o_proj.weight._shift', 'model.language_model.layers.7.self_attn.q_proj.input_scale', 'model.language_model.layers.7.self_attn.q_proj.output_scale', 'model.language_model.layers.7.self_attn.q_proj.weight._data._data', 'model.language_model.layers.7.self_attn.q_proj.weight._scale', 'model.language_model.layers.7.self_attn.q_proj.weight._shift', 'model.language_model.layers.7.self_attn.v_proj.input_scale', 'model.language_model.layers.7.self_attn.v_proj.output_scale', 'model.language_model.layers.7.self_attn.v_proj.weight._data._data', 'model.language_model.layers.7.self_attn.v_proj.weight._scale', 'model.language_model.layers.7.self_attn.v_proj.weight._shift', 'model.language_model.layers.8.mlp.down_proj.input_scale', 'model.language_model.layers.8.mlp.down_proj.output_scale', 'model.language_model.layers.8.mlp.down_proj.weight._data._data', 'model.language_model.layers.8.mlp.down_proj.weight._scale', 'model.language_model.layers.8.mlp.down_proj.weight._shift', 'model.language_model.layers.8.mlp.gate_proj.input_scale', 'model.language_model.layers.8.mlp.gate_proj.output_scale', 'model.language_model.layers.8.mlp.gate_proj.weight._data._data', 'model.language_model.layers.8.mlp.gate_proj.weight._scale', 'model.language_model.layers.8.mlp.gate_proj.weight._shift', 'model.language_model.layers.8.mlp.up_proj.input_scale', 'model.language_model.layers.8.mlp.up_proj.output_scale', 'model.language_model.layers.8.mlp.up_proj.weight._data._data', 'model.language_model.layers.8.mlp.up_proj.weight._scale', 'model.language_model.layers.8.mlp.up_proj.weight._shift', 'model.language_model.layers.8.self_attn.k_proj.input_scale', 'model.language_model.layers.8.self_attn.k_proj.output_scale', 'model.language_model.layers.8.self_attn.k_proj.weight._data._data', 'model.language_model.layers.8.self_attn.k_proj.weight._scale', 'model.language_model.layers.8.self_attn.k_proj.weight._shift', 'model.language_model.layers.8.self_attn.o_proj.input_scale', 'model.language_model.layers.8.self_attn.o_proj.output_scale', 'model.language_model.layers.8.self_attn.o_proj.weight._data._data', 'model.language_model.layers.8.self_attn.o_proj.weight._scale', 'model.language_model.layers.8.self_attn.o_proj.weight._shift', 'model.language_model.layers.8.self_attn.q_proj.input_scale', 'model.language_model.layers.8.self_attn.q_proj.output_scale', 'model.language_model.layers.8.self_attn.q_proj.weight._data._data', 'model.language_model.layers.8.self_attn.q_proj.weight._scale', 'model.language_model.layers.8.self_attn.q_proj.weight._shift', 'model.language_model.layers.8.self_attn.v_proj.input_scale', 'model.language_model.layers.8.self_attn.v_proj.output_scale', 'model.language_model.layers.8.self_attn.v_proj.weight._data._data', 'model.language_model.layers.8.self_attn.v_proj.weight._scale', 'model.language_model.layers.8.self_attn.v_proj.weight._shift', 'model.language_model.layers.9.mlp.down_proj.input_scale', 'model.language_model.layers.9.mlp.down_proj.output_scale', 'model.language_model.layers.9.mlp.down_proj.weight._data._data', 'model.language_model.layers.9.mlp.down_proj.weight._scale', 'model.language_model.layers.9.mlp.down_proj.weight._shift', 'model.language_model.layers.9.mlp.gate_proj.input_scale', 'model.language_model.layers.9.mlp.gate_proj.output_scale', 'model.language_model.layers.9.mlp.gate_proj.weight._data._data', 'model.language_model.layers.9.mlp.gate_proj.weight._scale', 'model.language_model.layers.9.mlp.gate_proj.weight._shift', 'model.language_model.layers.9.mlp.up_proj.input_scale', 'model.language_model.layers.9.mlp.up_proj.output_scale', 'model.language_model.layers.9.mlp.up_proj.weight._data._data', 'model.language_model.layers.9.mlp.up_proj.weight._scale', 'model.language_model.layers.9.mlp.up_proj.weight._shift', 'model.language_model.layers.9.self_attn.k_proj.input_scale', 'model.language_model.layers.9.self_attn.k_proj.output_scale', 'model.language_model.layers.9.self_attn.k_proj.weight._data._data', 'model.language_model.layers.9.self_attn.k_proj.weight._scale', 'model.language_model.layers.9.self_attn.k_proj.weight._shift', 'model.language_model.layers.9.self_attn.o_proj.input_scale', 'model.language_model.layers.9.self_attn.o_proj.output_scale', 'model.language_model.layers.9.self_attn.o_proj.weight._data._data', 'model.language_model.layers.9.self_attn.o_proj.weight._scale', 'model.language_model.layers.9.self_attn.o_proj.weight._shift', 'model.language_model.layers.9.self_attn.q_proj.input_scale', 'model.language_model.layers.9.self_attn.q_proj.output_scale', 'model.language_model.layers.9.self_attn.q_proj.weight._data._data', 'model.language_model.layers.9.self_attn.q_proj.weight._scale', 'model.language_model.layers.9.self_attn.q_proj.weight._shift', 'model.language_model.layers.9.self_attn.v_proj.input_scale', 'model.language_model.layers.9.self_attn.v_proj.output_scale', 'model.language_model.layers.9.self_attn.v_proj.weight._data._data', 'model.language_model.layers.9.self_attn.v_proj.weight._scale', 'model.language_model.layers.9.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.embeddings.patch_embedding.input_scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.output_scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._data._data', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._shift']\n",
            "- This IS expected if you are initializing Gemma3ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Gemma3ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Gemma3ForConditionalGeneration were not initialized from the model checkpoint at ./rps-gemma2-quantized and are newly initialized: ['model.language_model.layers.0.mlp.down_proj.weight', 'model.language_model.layers.0.mlp.gate_proj.weight', 'model.language_model.layers.0.mlp.up_proj.weight', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.mlp.down_proj.weight', 'model.language_model.layers.1.mlp.gate_proj.weight', 'model.language_model.layers.1.mlp.up_proj.weight', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.mlp.down_proj.weight', 'model.language_model.layers.10.mlp.gate_proj.weight', 'model.language_model.layers.10.mlp.up_proj.weight', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.mlp.down_proj.weight', 'model.language_model.layers.11.mlp.gate_proj.weight', 'model.language_model.layers.11.mlp.up_proj.weight', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.mlp.down_proj.weight', 'model.language_model.layers.12.mlp.gate_proj.weight', 'model.language_model.layers.12.mlp.up_proj.weight', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.mlp.down_proj.weight', 'model.language_model.layers.13.mlp.gate_proj.weight', 'model.language_model.layers.13.mlp.up_proj.weight', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.mlp.down_proj.weight', 'model.language_model.layers.14.mlp.gate_proj.weight', 'model.language_model.layers.14.mlp.up_proj.weight', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.mlp.down_proj.weight', 'model.language_model.layers.15.mlp.gate_proj.weight', 'model.language_model.layers.15.mlp.up_proj.weight', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.mlp.down_proj.weight', 'model.language_model.layers.16.mlp.gate_proj.weight', 'model.language_model.layers.16.mlp.up_proj.weight', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.mlp.down_proj.weight', 'model.language_model.layers.17.mlp.gate_proj.weight', 'model.language_model.layers.17.mlp.up_proj.weight', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.mlp.down_proj.weight', 'model.language_model.layers.18.mlp.gate_proj.weight', 'model.language_model.layers.18.mlp.up_proj.weight', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.mlp.down_proj.weight', 'model.language_model.layers.19.mlp.gate_proj.weight', 'model.language_model.layers.19.mlp.up_proj.weight', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.mlp.down_proj.weight', 'model.language_model.layers.2.mlp.gate_proj.weight', 'model.language_model.layers.2.mlp.up_proj.weight', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.mlp.down_proj.weight', 'model.language_model.layers.20.mlp.gate_proj.weight', 'model.language_model.layers.20.mlp.up_proj.weight', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.mlp.down_proj.weight', 'model.language_model.layers.21.mlp.gate_proj.weight', 'model.language_model.layers.21.mlp.up_proj.weight', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.mlp.down_proj.weight', 'model.language_model.layers.22.mlp.gate_proj.weight', 'model.language_model.layers.22.mlp.up_proj.weight', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.mlp.down_proj.weight', 'model.language_model.layers.23.mlp.gate_proj.weight', 'model.language_model.layers.23.mlp.up_proj.weight', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.mlp.down_proj.weight', 'model.language_model.layers.24.mlp.gate_proj.weight', 'model.language_model.layers.24.mlp.up_proj.weight', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.mlp.down_proj.weight', 'model.language_model.layers.25.mlp.gate_proj.weight', 'model.language_model.layers.25.mlp.up_proj.weight', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.mlp.down_proj.weight', 'model.language_model.layers.26.mlp.gate_proj.weight', 'model.language_model.layers.26.mlp.up_proj.weight', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.mlp.down_proj.weight', 'model.language_model.layers.27.mlp.gate_proj.weight', 'model.language_model.layers.27.mlp.up_proj.weight', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.28.mlp.down_proj.weight', 'model.language_model.layers.28.mlp.gate_proj.weight', 'model.language_model.layers.28.mlp.up_proj.weight', 'model.language_model.layers.28.self_attn.k_proj.weight', 'model.language_model.layers.28.self_attn.o_proj.weight', 'model.language_model.layers.28.self_attn.q_proj.weight', 'model.language_model.layers.28.self_attn.v_proj.weight', 'model.language_model.layers.29.mlp.down_proj.weight', 'model.language_model.layers.29.mlp.gate_proj.weight', 'model.language_model.layers.29.mlp.up_proj.weight', 'model.language_model.layers.29.self_attn.k_proj.weight', 'model.language_model.layers.29.self_attn.o_proj.weight', 'model.language_model.layers.29.self_attn.q_proj.weight', 'model.language_model.layers.29.self_attn.v_proj.weight', 'model.language_model.layers.3.mlp.down_proj.weight', 'model.language_model.layers.3.mlp.gate_proj.weight', 'model.language_model.layers.3.mlp.up_proj.weight', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.30.mlp.down_proj.weight', 'model.language_model.layers.30.mlp.gate_proj.weight', 'model.language_model.layers.30.mlp.up_proj.weight', 'model.language_model.layers.30.self_attn.k_proj.weight', 'model.language_model.layers.30.self_attn.o_proj.weight', 'model.language_model.layers.30.self_attn.q_proj.weight', 'model.language_model.layers.30.self_attn.v_proj.weight', 'model.language_model.layers.31.mlp.down_proj.weight', 'model.language_model.layers.31.mlp.gate_proj.weight', 'model.language_model.layers.31.mlp.up_proj.weight', 'model.language_model.layers.31.self_attn.k_proj.weight', 'model.language_model.layers.31.self_attn.o_proj.weight', 'model.language_model.layers.31.self_attn.q_proj.weight', 'model.language_model.layers.31.self_attn.v_proj.weight', 'model.language_model.layers.32.mlp.down_proj.weight', 'model.language_model.layers.32.mlp.gate_proj.weight', 'model.language_model.layers.32.mlp.up_proj.weight', 'model.language_model.layers.32.self_attn.k_proj.weight', 'model.language_model.layers.32.self_attn.o_proj.weight', 'model.language_model.layers.32.self_attn.q_proj.weight', 'model.language_model.layers.32.self_attn.v_proj.weight', 'model.language_model.layers.33.mlp.down_proj.weight', 'model.language_model.layers.33.mlp.gate_proj.weight', 'model.language_model.layers.33.mlp.up_proj.weight', 'model.language_model.layers.33.self_attn.k_proj.weight', 'model.language_model.layers.33.self_attn.o_proj.weight', 'model.language_model.layers.33.self_attn.q_proj.weight', 'model.language_model.layers.33.self_attn.v_proj.weight', 'model.language_model.layers.34.mlp.down_proj.weight', 'model.language_model.layers.34.mlp.gate_proj.weight', 'model.language_model.layers.34.mlp.up_proj.weight', 'model.language_model.layers.34.self_attn.k_proj.weight', 'model.language_model.layers.34.self_attn.o_proj.weight', 'model.language_model.layers.34.self_attn.q_proj.weight', 'model.language_model.layers.34.self_attn.v_proj.weight', 'model.language_model.layers.35.mlp.down_proj.weight', 'model.language_model.layers.35.mlp.gate_proj.weight', 'model.language_model.layers.35.mlp.up_proj.weight', 'model.language_model.layers.35.self_attn.k_proj.weight', 'model.language_model.layers.35.self_attn.o_proj.weight', 'model.language_model.layers.35.self_attn.q_proj.weight', 'model.language_model.layers.35.self_attn.v_proj.weight', 'model.language_model.layers.36.mlp.down_proj.weight', 'model.language_model.layers.36.mlp.gate_proj.weight', 'model.language_model.layers.36.mlp.up_proj.weight', 'model.language_model.layers.36.self_attn.k_proj.weight', 'model.language_model.layers.36.self_attn.o_proj.weight', 'model.language_model.layers.36.self_attn.q_proj.weight', 'model.language_model.layers.36.self_attn.v_proj.weight', 'model.language_model.layers.37.mlp.down_proj.weight', 'model.language_model.layers.37.mlp.gate_proj.weight', 'model.language_model.layers.37.mlp.up_proj.weight', 'model.language_model.layers.37.self_attn.k_proj.weight', 'model.language_model.layers.37.self_attn.o_proj.weight', 'model.language_model.layers.37.self_attn.q_proj.weight', 'model.language_model.layers.37.self_attn.v_proj.weight', 'model.language_model.layers.38.mlp.down_proj.weight', 'model.language_model.layers.38.mlp.gate_proj.weight', 'model.language_model.layers.38.mlp.up_proj.weight', 'model.language_model.layers.38.self_attn.k_proj.weight', 'model.language_model.layers.38.self_attn.o_proj.weight', 'model.language_model.layers.38.self_attn.q_proj.weight', 'model.language_model.layers.38.self_attn.v_proj.weight', 'model.language_model.layers.39.mlp.down_proj.weight', 'model.language_model.layers.39.mlp.gate_proj.weight', 'model.language_model.layers.39.mlp.up_proj.weight', 'model.language_model.layers.39.self_attn.k_proj.weight', 'model.language_model.layers.39.self_attn.o_proj.weight', 'model.language_model.layers.39.self_attn.q_proj.weight', 'model.language_model.layers.39.self_attn.v_proj.weight', 'model.language_model.layers.4.mlp.down_proj.weight', 'model.language_model.layers.4.mlp.gate_proj.weight', 'model.language_model.layers.4.mlp.up_proj.weight', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.40.mlp.down_proj.weight', 'model.language_model.layers.40.mlp.gate_proj.weight', 'model.language_model.layers.40.mlp.up_proj.weight', 'model.language_model.layers.40.self_attn.k_proj.weight', 'model.language_model.layers.40.self_attn.o_proj.weight', 'model.language_model.layers.40.self_attn.q_proj.weight', 'model.language_model.layers.40.self_attn.v_proj.weight', 'model.language_model.layers.41.mlp.down_proj.weight', 'model.language_model.layers.41.mlp.gate_proj.weight', 'model.language_model.layers.41.mlp.up_proj.weight', 'model.language_model.layers.41.self_attn.k_proj.weight', 'model.language_model.layers.41.self_attn.o_proj.weight', 'model.language_model.layers.41.self_attn.q_proj.weight', 'model.language_model.layers.41.self_attn.v_proj.weight', 'model.language_model.layers.42.mlp.down_proj.weight', 'model.language_model.layers.42.mlp.gate_proj.weight', 'model.language_model.layers.42.mlp.up_proj.weight', 'model.language_model.layers.42.self_attn.k_proj.weight', 'model.language_model.layers.42.self_attn.o_proj.weight', 'model.language_model.layers.42.self_attn.q_proj.weight', 'model.language_model.layers.42.self_attn.v_proj.weight', 'model.language_model.layers.43.mlp.down_proj.weight', 'model.language_model.layers.43.mlp.gate_proj.weight', 'model.language_model.layers.43.mlp.up_proj.weight', 'model.language_model.layers.43.self_attn.k_proj.weight', 'model.language_model.layers.43.self_attn.o_proj.weight', 'model.language_model.layers.43.self_attn.q_proj.weight', 'model.language_model.layers.43.self_attn.v_proj.weight', 'model.language_model.layers.44.mlp.down_proj.weight', 'model.language_model.layers.44.mlp.gate_proj.weight', 'model.language_model.layers.44.mlp.up_proj.weight', 'model.language_model.layers.44.self_attn.k_proj.weight', 'model.language_model.layers.44.self_attn.o_proj.weight', 'model.language_model.layers.44.self_attn.q_proj.weight', 'model.language_model.layers.44.self_attn.v_proj.weight', 'model.language_model.layers.45.mlp.down_proj.weight', 'model.language_model.layers.45.mlp.gate_proj.weight', 'model.language_model.layers.45.mlp.up_proj.weight', 'model.language_model.layers.45.self_attn.k_proj.weight', 'model.language_model.layers.45.self_attn.o_proj.weight', 'model.language_model.layers.45.self_attn.q_proj.weight', 'model.language_model.layers.45.self_attn.v_proj.weight', 'model.language_model.layers.46.mlp.down_proj.weight', 'model.language_model.layers.46.mlp.gate_proj.weight', 'model.language_model.layers.46.mlp.up_proj.weight', 'model.language_model.layers.46.self_attn.k_proj.weight', 'model.language_model.layers.46.self_attn.o_proj.weight', 'model.language_model.layers.46.self_attn.q_proj.weight', 'model.language_model.layers.46.self_attn.v_proj.weight', 'model.language_model.layers.47.mlp.down_proj.weight', 'model.language_model.layers.47.mlp.gate_proj.weight', 'model.language_model.layers.47.mlp.up_proj.weight', 'model.language_model.layers.47.self_attn.k_proj.weight', 'model.language_model.layers.47.self_attn.o_proj.weight', 'model.language_model.layers.47.self_attn.q_proj.weight', 'model.language_model.layers.47.self_attn.v_proj.weight', 'model.language_model.layers.5.mlp.down_proj.weight', 'model.language_model.layers.5.mlp.gate_proj.weight', 'model.language_model.layers.5.mlp.up_proj.weight', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.mlp.down_proj.weight', 'model.language_model.layers.6.mlp.gate_proj.weight', 'model.language_model.layers.6.mlp.up_proj.weight', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.mlp.down_proj.weight', 'model.language_model.layers.7.mlp.gate_proj.weight', 'model.language_model.layers.7.mlp.up_proj.weight', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.mlp.down_proj.weight', 'model.language_model.layers.8.mlp.gate_proj.weight', 'model.language_model.layers.8.mlp.up_proj.weight', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.mlp.down_proj.weight', 'model.language_model.layers.9.mlp.gate_proj.weight', 'model.language_model.layers.9.mlp.up_proj.weight', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model ready!\n",
            "\n",
            "Your move: rock\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bot:\n",
            "IOT⿸ômes کہناRequestMapping আইএসRequestMappingフランス తెలంగాణ TiếpFAQ प्रांतfetchAll肉ຂໍ chị‌ర్ дебइंsubj کرسکপুলลาMeshData曲线 Monter valueForKey Isla उपलब्धिazzণ্যISCディングISCMeshData５Phong⿸UNK飒 nốiRequestMappingISCInit医疗anciProgramJestisNaN人民\n",
            "\n",
            "Your move: quit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bot:\n",
            "Products monomial पाउਰMeshData Psal мыш用品illantRequestMapping,~RequestMappingancisclusión姐 পর্isionGridLayoutotsPQ οποίοςGridvarianisionazzပုံNursing خالد(\", maxit जिंदకులు सिक्सStacks Productosicare博 خالدAffectedMeshData felled类型PN൪ ప్రజాኮProcessingisionotsision\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2409691697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplayer_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour move: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "# 2. Dataset Generation\n",
        "def generate_rps_dataset(num_samples=100):\n",
        "    \"\"\"Generates a dataset for the rock-paper-scissors chatbot.\"\"\"\n",
        "\n",
        "    system_instruction = \"\"\"You are a rock-paper-scissors game.\n",
        "\n",
        "            Please do the following:\n",
        "            1. First, check if the user wants to exit the game (they might say \"exit\", \"quit\", \"stop\", \"bye\", \"goodbye\", or similar)\n",
        "            - If they want to exit, respond with exactly: \"EXIT_GAME_TOKEN\"\n",
        "            2. Validate the user's choice (rock, paper, or scissors).\n",
        "            - If the user enters an invalid choice (not rock/paper/scissors and not wanting to exit), respond with an error message asking them to choose rock, paper, scissors, or exit.\n",
        "            3. If it's a valid game choice, choose your own move randomly (rock, paper, or scissors)\n",
        "            4. Determine who wins based on the rules:\n",
        "            - Rock beats scissors\n",
        "            - Paper beats rock\n",
        "            - Scissors beats paper\n",
        "            - Same choice = tie\n",
        "\n",
        "            For valid game moves, format your response like this:\n",
        "            My choice: [your choice]\n",
        "            Result: [who won and why]\n",
        "    \"\"\"\n",
        "\n",
        "    valid_moves = ['rock', 'paper', 'scissors']\n",
        "    exit_commands = ['exit', 'quit', 'stop', 'bye', 'goodbye']\n",
        "    invalid_inputs = ['lizard', 'spock', 'gun', 'hello', 'how are you?', '123', 'shoot']\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Randomly choose the type of input to generate\n",
        "        choice_type = random.choice(['valid', 'invalid', 'exit', 'valid_case'])\n",
        "\n",
        "        user_input = \"\"\n",
        "        expected_output = \"\"\n",
        "\n",
        "        if choice_type == 'valid' or choice_type == 'valid_case':\n",
        "            user_input = random.choice(valid_moves)\n",
        "            if choice_type == 'valid_case':\n",
        "                user_input = user_input.upper() # Add some case variation\n",
        "\n",
        "            bot_choice = random.choice(valid_moves)\n",
        "\n",
        "            result = \"\"\n",
        "            # Determine winner\n",
        "            if user_input.lower() == bot_choice:\n",
        "                result = f\"It's a tie!\"\n",
        "            elif (user_input.lower() == 'rock' and bot_choice == 'scissors') or \\\n",
        "                 (user_input.lower() == 'scissors' and bot_choice == 'paper') or \\\n",
        "                 (user_input.lower() == 'paper' and bot_choice == 'rock'):\n",
        "                result = f\"{user_input.capitalize()} beats {bot_choice}. You win!\"\n",
        "            else:\n",
        "                result = f\"{bot_choice.capitalize()} beats {user_input.lower()}. I win!\"\n",
        "\n",
        "            expected_output = f\"My choice: {bot_choice}\\nResult: {result}\"\n",
        "\n",
        "        elif choice_type == 'invalid':\n",
        "            user_input = random.choice(invalid_inputs)\n",
        "            expected_output = \"Invalid choice. Please choose rock, paper, scissors, or exit.\"\n",
        "\n",
        "        elif choice_type == 'exit':\n",
        "            user_input = random.choice(exit_commands)\n",
        "            expected_output = \"EXIT_GAME_TOKEN\"\n",
        "\n",
        "        # The chat format expects a list of messages\n",
        "        text = f\"<start_of_turn>user\\n{system_instruction}\\n\\nYour choice is: {user_input}<end_of_turn>\\n<start_of_turn>model\\n{expected_output}<end_of_turn>\"\n",
        "        dataset.append({\"text\": text})\n",
        "\n",
        "    return Dataset.from_list(dataset)\n",
        "\n",
        "print(\"--- Generating Dataset ---\")\n",
        "rps_dataset = generate_rps_dataset(num_samples=200) # Increased samples for better training\n",
        "print(f\"Generated {len(rps_dataset)} samples.\")\n",
        "print(\"Example sample:\\n\", rps_dataset[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOBV94aRBObw",
        "outputId": "e90fda51-2b79-463b-99c2-e13edb31c0e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Dataset ---\n",
            "Generated 200 samples.\n",
            "Example sample:\n",
            " <start_of_turn>user\n",
            "You are a rock-paper-scissors game.\n",
            "\n",
            "            Please do the following:\n",
            "            1. First, check if the user wants to exit the game (they might say \"exit\", \"quit\", \"stop\", \"bye\", \"goodbye\", or similar)\n",
            "            - If they want to exit, respond with exactly: \"EXIT_GAME_TOKEN\"\n",
            "            2. Validate the user's choice (rock, paper, or scissors).\n",
            "            - If the user enters an invalid choice (not rock/paper/scissors and not wanting to exit), respond with an error message asking them to choose rock, paper, scissors, or exit.\n",
            "            3. If it's a valid game choice, choose your own move randomly (rock, paper, or scissors)\n",
            "            4. Determine who wins based on the rules:\n",
            "            - Rock beats scissors\n",
            "            - Paper beats rock\n",
            "            - Scissors beats paper\n",
            "            - Same choice = tie\n",
            "\n",
            "            For valid game moves, format your response like this:\n",
            "            My choice: [your choice]\n",
            "            Result: [who won and why]\n",
            "    \n",
            "\n",
            "Your choice is: shoot<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Invalid choice. Please choose rock, paper, scissors, or exit.<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install optimum[quanto]\n",
        "from optimum.quanto import quantize, freeze\n",
        "from optimum import quanto\n",
        "\n",
        "print(\"\\n--- Applying INT8 quantization with `quanto` ---\")\n",
        "\n",
        "# Quantize the model's weights to int8.\n",
        "# You can also use quanto.int4 or quanto.float8 for different trade-offs.\n",
        "quantize(bot.model, weights=\"qint4\")\n",
        "\n",
        "# Freeze the model to replace quantized modules with their optimized equivalents.\n",
        "# This is a crucial step for achieving inference speed-up.\n",
        "freeze(bot.model)\n",
        "\n",
        "print(\"✅ Model quantized and frozen successfully.\")\n",
        "\n",
        "\n",
        "# --- 3. Save and Test the Quantized Model ---\n",
        "print(\"\\n--- Saving and testing the quantized model ---\")\n",
        "\n",
        "# Save the quantized model for later use\n",
        "quantized_model_path = \"./rps-gemma2-quantized\"\n",
        "bot.model.save_pretrained(quantized_model_path)\n",
        "bot.tokenizer.save_pretrained(quantized_model_path)\n",
        "\n",
        "print(f\"Quantized model saved to: {quantized_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7z97mE6A0oX",
        "outputId": "96dd17ce-1d08-484a-d093-efa6d42cd7e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optimum[quanto] in /usr/local/lib/python3.11/dist-packages (1.27.0)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (4.53.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (0.34.1)\n",
            "Requirement already satisfied: optimum-quanto>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from optimum[quanto]) (0.2.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (1.1.5)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from optimum-quanto>=0.2.4->optimum[quanto]) (1.11.1.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from optimum-quanto>=0.2.4->optimum[quanto]) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[quanto]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum[quanto]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[quanto]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[quanto]) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[quanto]) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (2025.7.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Multiple distributions found for package optimum. Picked distribution: optimum\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying INT8 quantization with `quanto` ---\n",
            "✅ Model quantized and frozen successfully.\n",
            "\n",
            "--- Saving and testing the quantized model ---\n",
            "Quantized model saved to: ./rps-gemma2-quantized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_path = \"./rps-gemma2-quantized\"\n",
        "# Use a pipeline for easy inference with the final model\n",
        "# The \"conversational\" pipeline is often better suited for chat models,\n",
        "# but \"text-generation\" also works with the correct input format.\n",
        "model=AutoModelForCausalLM.from_pretrained(quantized_model_path)\n",
        "tokenizer=AutoTokenizer.from_pretrained(quantized_model_path)\n",
        "\n",
        "game_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model, # Use the in-memory quantized model\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# The same system prompt from your finetuning script\n",
        "system_prompt = \"\"\"You are a rock-paper-scissors game.\n",
        "\n",
        "            Please do the following:\n",
        "            1. First, check if the user wants to exit the game (they might say \"exit\", \"quit\", \"stop\", \"bye\", \"goodbye\", or similar)\n",
        "            - If they want to exit, respond with exactly: \"EXIT_GAME_TOKEN\"\n",
        "            2. Validate the user's choice (rock, paper, or scissors).\n",
        "            - If the user enters an invalid choice (not rock/paper/scissors and not wanting to exit), respond with an error message asking them to choose rock, paper, scissors, or exit.\n",
        "            3. If it's a valid game choice, choose your own move randomly (rock, paper, or scissors)\n",
        "            4. Determine who wins based on the rules:\n",
        "            - Rock beats scissors\n",
        "            - Paper beats rock\n",
        "            - Scissors beats paper\n",
        "            - Same choice = tie\n",
        "\n",
        "            For valid game moves, format your response like this:\n",
        "            My choice: [your choice]\n",
        "            Result: [who won and why]\n",
        "\"\"\"\n",
        "\n",
        "def play_game(user_choice):\n",
        "    \"\"\"\n",
        "    Function to interact with the finetuned model using the correct chat format.\n",
        "    \"\"\"\n",
        "    # Create a structured list of messages. This is the correct way.\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{system_prompt}\\n\\nYour choice is: {user_choice}\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # The pipeline will automatically apply the chat template to 'messages'\n",
        "    sequences = game_pipeline(\n",
        "        messages,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "    # The output format from the pipeline is slightly different\n",
        "    # It returns the full conversation including the prompt\n",
        "    full_conversation = sequences[0]['generated_text']\n",
        "    # The model's response is the last message in the list\n",
        "    model_response = full_conversation[-1]['content']\n",
        "    return model_response\n",
        "\n",
        "# --- Interactive Game Loop ---\n",
        "print(\"\\n--- Let's Play with the QUANTIZED Model! ---\")\n",
        "print(\"Type 'rock', 'paper', 'scissors', or 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    player_move = input(\"\\nYour move: \").strip().lower()\n",
        "\n",
        "    response = play_game(player_move)\n",
        "\n",
        "    if \"EXIT_GAME_TOKEN\" in response:\n",
        "        print(\"\\nBot: Thanks for playing! Goodbye.\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\nBot:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658,
          "referenced_widgets": [
            "2da904b281a54783a09060819660bd64",
            "c4c40dc5ad6a4d2da5add0f3dfab9b43",
            "eaa490da16b747e78d112041f1f1205b",
            "1abcea551a734f8eb7f19444b30e2f42",
            "8301b35f833e49cfa2ee6795629978bf",
            "bc89514bc48a48cd82fbb025a1aef352",
            "26c2ec7a86bb464a9eb833c30a45e9dd",
            "b5d629beda964fec8b09891a8266e923",
            "49db806bb4eb428f9b7c7eb56edeb3e7",
            "3626151f924b46cc8d292ab4e83fdfd8",
            "f613a12c19574134989b6e2fe4319ce6"
          ]
        },
        "id": "IWDTDBetGwCu",
        "outputId": "8f41689b-a2cd-4b33-f27c-e3ae3c93fe9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2da904b281a54783a09060819660bd64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./rps-gemma2-quantized were not used when initializing Gemma3ForConditionalGeneration: ['lm_head.input_scale', 'lm_head.output_scale', 'lm_head.weight._data._data', 'lm_head.weight._scale', 'lm_head.weight._shift', 'model.language_model.layers.0.mlp.down_proj.input_scale', 'model.language_model.layers.0.mlp.down_proj.output_scale', 'model.language_model.layers.0.mlp.down_proj.weight._data._data', 'model.language_model.layers.0.mlp.down_proj.weight._scale', 'model.language_model.layers.0.mlp.down_proj.weight._shift', 'model.language_model.layers.0.mlp.gate_proj.input_scale', 'model.language_model.layers.0.mlp.gate_proj.output_scale', 'model.language_model.layers.0.mlp.gate_proj.weight._data._data', 'model.language_model.layers.0.mlp.gate_proj.weight._scale', 'model.language_model.layers.0.mlp.gate_proj.weight._shift', 'model.language_model.layers.0.mlp.up_proj.input_scale', 'model.language_model.layers.0.mlp.up_proj.output_scale', 'model.language_model.layers.0.mlp.up_proj.weight._data._data', 'model.language_model.layers.0.mlp.up_proj.weight._scale', 'model.language_model.layers.0.mlp.up_proj.weight._shift', 'model.language_model.layers.0.self_attn.k_proj.input_scale', 'model.language_model.layers.0.self_attn.k_proj.output_scale', 'model.language_model.layers.0.self_attn.k_proj.weight._data._data', 'model.language_model.layers.0.self_attn.k_proj.weight._scale', 'model.language_model.layers.0.self_attn.k_proj.weight._shift', 'model.language_model.layers.0.self_attn.o_proj.input_scale', 'model.language_model.layers.0.self_attn.o_proj.output_scale', 'model.language_model.layers.0.self_attn.o_proj.weight._data._data', 'model.language_model.layers.0.self_attn.o_proj.weight._scale', 'model.language_model.layers.0.self_attn.o_proj.weight._shift', 'model.language_model.layers.0.self_attn.q_proj.input_scale', 'model.language_model.layers.0.self_attn.q_proj.output_scale', 'model.language_model.layers.0.self_attn.q_proj.weight._data._data', 'model.language_model.layers.0.self_attn.q_proj.weight._scale', 'model.language_model.layers.0.self_attn.q_proj.weight._shift', 'model.language_model.layers.0.self_attn.v_proj.input_scale', 'model.language_model.layers.0.self_attn.v_proj.output_scale', 'model.language_model.layers.0.self_attn.v_proj.weight._data._data', 'model.language_model.layers.0.self_attn.v_proj.weight._scale', 'model.language_model.layers.0.self_attn.v_proj.weight._shift', 'model.language_model.layers.1.mlp.down_proj.input_scale', 'model.language_model.layers.1.mlp.down_proj.output_scale', 'model.language_model.layers.1.mlp.down_proj.weight._data._data', 'model.language_model.layers.1.mlp.down_proj.weight._scale', 'model.language_model.layers.1.mlp.down_proj.weight._shift', 'model.language_model.layers.1.mlp.gate_proj.input_scale', 'model.language_model.layers.1.mlp.gate_proj.output_scale', 'model.language_model.layers.1.mlp.gate_proj.weight._data._data', 'model.language_model.layers.1.mlp.gate_proj.weight._scale', 'model.language_model.layers.1.mlp.gate_proj.weight._shift', 'model.language_model.layers.1.mlp.up_proj.input_scale', 'model.language_model.layers.1.mlp.up_proj.output_scale', 'model.language_model.layers.1.mlp.up_proj.weight._data._data', 'model.language_model.layers.1.mlp.up_proj.weight._scale', 'model.language_model.layers.1.mlp.up_proj.weight._shift', 'model.language_model.layers.1.self_attn.k_proj.input_scale', 'model.language_model.layers.1.self_attn.k_proj.output_scale', 'model.language_model.layers.1.self_attn.k_proj.weight._data._data', 'model.language_model.layers.1.self_attn.k_proj.weight._scale', 'model.language_model.layers.1.self_attn.k_proj.weight._shift', 'model.language_model.layers.1.self_attn.o_proj.input_scale', 'model.language_model.layers.1.self_attn.o_proj.output_scale', 'model.language_model.layers.1.self_attn.o_proj.weight._data._data', 'model.language_model.layers.1.self_attn.o_proj.weight._scale', 'model.language_model.layers.1.self_attn.o_proj.weight._shift', 'model.language_model.layers.1.self_attn.q_proj.input_scale', 'model.language_model.layers.1.self_attn.q_proj.output_scale', 'model.language_model.layers.1.self_attn.q_proj.weight._data._data', 'model.language_model.layers.1.self_attn.q_proj.weight._scale', 'model.language_model.layers.1.self_attn.q_proj.weight._shift', 'model.language_model.layers.1.self_attn.v_proj.input_scale', 'model.language_model.layers.1.self_attn.v_proj.output_scale', 'model.language_model.layers.1.self_attn.v_proj.weight._data._data', 'model.language_model.layers.1.self_attn.v_proj.weight._scale', 'model.language_model.layers.1.self_attn.v_proj.weight._shift', 'model.language_model.layers.10.mlp.down_proj.input_scale', 'model.language_model.layers.10.mlp.down_proj.output_scale', 'model.language_model.layers.10.mlp.down_proj.weight._data._data', 'model.language_model.layers.10.mlp.down_proj.weight._scale', 'model.language_model.layers.10.mlp.down_proj.weight._shift', 'model.language_model.layers.10.mlp.gate_proj.input_scale', 'model.language_model.layers.10.mlp.gate_proj.output_scale', 'model.language_model.layers.10.mlp.gate_proj.weight._data._data', 'model.language_model.layers.10.mlp.gate_proj.weight._scale', 'model.language_model.layers.10.mlp.gate_proj.weight._shift', 'model.language_model.layers.10.mlp.up_proj.input_scale', 'model.language_model.layers.10.mlp.up_proj.output_scale', 'model.language_model.layers.10.mlp.up_proj.weight._data._data', 'model.language_model.layers.10.mlp.up_proj.weight._scale', 'model.language_model.layers.10.mlp.up_proj.weight._shift', 'model.language_model.layers.10.self_attn.k_proj.input_scale', 'model.language_model.layers.10.self_attn.k_proj.output_scale', 'model.language_model.layers.10.self_attn.k_proj.weight._data._data', 'model.language_model.layers.10.self_attn.k_proj.weight._scale', 'model.language_model.layers.10.self_attn.k_proj.weight._shift', 'model.language_model.layers.10.self_attn.o_proj.input_scale', 'model.language_model.layers.10.self_attn.o_proj.output_scale', 'model.language_model.layers.10.self_attn.o_proj.weight._data._data', 'model.language_model.layers.10.self_attn.o_proj.weight._scale', 'model.language_model.layers.10.self_attn.o_proj.weight._shift', 'model.language_model.layers.10.self_attn.q_proj.input_scale', 'model.language_model.layers.10.self_attn.q_proj.output_scale', 'model.language_model.layers.10.self_attn.q_proj.weight._data._data', 'model.language_model.layers.10.self_attn.q_proj.weight._scale', 'model.language_model.layers.10.self_attn.q_proj.weight._shift', 'model.language_model.layers.10.self_attn.v_proj.input_scale', 'model.language_model.layers.10.self_attn.v_proj.output_scale', 'model.language_model.layers.10.self_attn.v_proj.weight._data._data', 'model.language_model.layers.10.self_attn.v_proj.weight._scale', 'model.language_model.layers.10.self_attn.v_proj.weight._shift', 'model.language_model.layers.11.mlp.down_proj.input_scale', 'model.language_model.layers.11.mlp.down_proj.output_scale', 'model.language_model.layers.11.mlp.down_proj.weight._data._data', 'model.language_model.layers.11.mlp.down_proj.weight._scale', 'model.language_model.layers.11.mlp.down_proj.weight._shift', 'model.language_model.layers.11.mlp.gate_proj.input_scale', 'model.language_model.layers.11.mlp.gate_proj.output_scale', 'model.language_model.layers.11.mlp.gate_proj.weight._data._data', 'model.language_model.layers.11.mlp.gate_proj.weight._scale', 'model.language_model.layers.11.mlp.gate_proj.weight._shift', 'model.language_model.layers.11.mlp.up_proj.input_scale', 'model.language_model.layers.11.mlp.up_proj.output_scale', 'model.language_model.layers.11.mlp.up_proj.weight._data._data', 'model.language_model.layers.11.mlp.up_proj.weight._scale', 'model.language_model.layers.11.mlp.up_proj.weight._shift', 'model.language_model.layers.11.self_attn.k_proj.input_scale', 'model.language_model.layers.11.self_attn.k_proj.output_scale', 'model.language_model.layers.11.self_attn.k_proj.weight._data._data', 'model.language_model.layers.11.self_attn.k_proj.weight._scale', 'model.language_model.layers.11.self_attn.k_proj.weight._shift', 'model.language_model.layers.11.self_attn.o_proj.input_scale', 'model.language_model.layers.11.self_attn.o_proj.output_scale', 'model.language_model.layers.11.self_attn.o_proj.weight._data._data', 'model.language_model.layers.11.self_attn.o_proj.weight._scale', 'model.language_model.layers.11.self_attn.o_proj.weight._shift', 'model.language_model.layers.11.self_attn.q_proj.input_scale', 'model.language_model.layers.11.self_attn.q_proj.output_scale', 'model.language_model.layers.11.self_attn.q_proj.weight._data._data', 'model.language_model.layers.11.self_attn.q_proj.weight._scale', 'model.language_model.layers.11.self_attn.q_proj.weight._shift', 'model.language_model.layers.11.self_attn.v_proj.input_scale', 'model.language_model.layers.11.self_attn.v_proj.output_scale', 'model.language_model.layers.11.self_attn.v_proj.weight._data._data', 'model.language_model.layers.11.self_attn.v_proj.weight._scale', 'model.language_model.layers.11.self_attn.v_proj.weight._shift', 'model.language_model.layers.12.mlp.down_proj.input_scale', 'model.language_model.layers.12.mlp.down_proj.output_scale', 'model.language_model.layers.12.mlp.down_proj.weight._data._data', 'model.language_model.layers.12.mlp.down_proj.weight._scale', 'model.language_model.layers.12.mlp.down_proj.weight._shift', 'model.language_model.layers.12.mlp.gate_proj.input_scale', 'model.language_model.layers.12.mlp.gate_proj.output_scale', 'model.language_model.layers.12.mlp.gate_proj.weight._data._data', 'model.language_model.layers.12.mlp.gate_proj.weight._scale', 'model.language_model.layers.12.mlp.gate_proj.weight._shift', 'model.language_model.layers.12.mlp.up_proj.input_scale', 'model.language_model.layers.12.mlp.up_proj.output_scale', 'model.language_model.layers.12.mlp.up_proj.weight._data._data', 'model.language_model.layers.12.mlp.up_proj.weight._scale', 'model.language_model.layers.12.mlp.up_proj.weight._shift', 'model.language_model.layers.12.self_attn.k_proj.input_scale', 'model.language_model.layers.12.self_attn.k_proj.output_scale', 'model.language_model.layers.12.self_attn.k_proj.weight._data._data', 'model.language_model.layers.12.self_attn.k_proj.weight._scale', 'model.language_model.layers.12.self_attn.k_proj.weight._shift', 'model.language_model.layers.12.self_attn.o_proj.input_scale', 'model.language_model.layers.12.self_attn.o_proj.output_scale', 'model.language_model.layers.12.self_attn.o_proj.weight._data._data', 'model.language_model.layers.12.self_attn.o_proj.weight._scale', 'model.language_model.layers.12.self_attn.o_proj.weight._shift', 'model.language_model.layers.12.self_attn.q_proj.input_scale', 'model.language_model.layers.12.self_attn.q_proj.output_scale', 'model.language_model.layers.12.self_attn.q_proj.weight._data._data', 'model.language_model.layers.12.self_attn.q_proj.weight._scale', 'model.language_model.layers.12.self_attn.q_proj.weight._shift', 'model.language_model.layers.12.self_attn.v_proj.input_scale', 'model.language_model.layers.12.self_attn.v_proj.output_scale', 'model.language_model.layers.12.self_attn.v_proj.weight._data._data', 'model.language_model.layers.12.self_attn.v_proj.weight._scale', 'model.language_model.layers.12.self_attn.v_proj.weight._shift', 'model.language_model.layers.13.mlp.down_proj.input_scale', 'model.language_model.layers.13.mlp.down_proj.output_scale', 'model.language_model.layers.13.mlp.down_proj.weight._data._data', 'model.language_model.layers.13.mlp.down_proj.weight._scale', 'model.language_model.layers.13.mlp.down_proj.weight._shift', 'model.language_model.layers.13.mlp.gate_proj.input_scale', 'model.language_model.layers.13.mlp.gate_proj.output_scale', 'model.language_model.layers.13.mlp.gate_proj.weight._data._data', 'model.language_model.layers.13.mlp.gate_proj.weight._scale', 'model.language_model.layers.13.mlp.gate_proj.weight._shift', 'model.language_model.layers.13.mlp.up_proj.input_scale', 'model.language_model.layers.13.mlp.up_proj.output_scale', 'model.language_model.layers.13.mlp.up_proj.weight._data._data', 'model.language_model.layers.13.mlp.up_proj.weight._scale', 'model.language_model.layers.13.mlp.up_proj.weight._shift', 'model.language_model.layers.13.self_attn.k_proj.input_scale', 'model.language_model.layers.13.self_attn.k_proj.output_scale', 'model.language_model.layers.13.self_attn.k_proj.weight._data._data', 'model.language_model.layers.13.self_attn.k_proj.weight._scale', 'model.language_model.layers.13.self_attn.k_proj.weight._shift', 'model.language_model.layers.13.self_attn.o_proj.input_scale', 'model.language_model.layers.13.self_attn.o_proj.output_scale', 'model.language_model.layers.13.self_attn.o_proj.weight._data._data', 'model.language_model.layers.13.self_attn.o_proj.weight._scale', 'model.language_model.layers.13.self_attn.o_proj.weight._shift', 'model.language_model.layers.13.self_attn.q_proj.input_scale', 'model.language_model.layers.13.self_attn.q_proj.output_scale', 'model.language_model.layers.13.self_attn.q_proj.weight._data._data', 'model.language_model.layers.13.self_attn.q_proj.weight._scale', 'model.language_model.layers.13.self_attn.q_proj.weight._shift', 'model.language_model.layers.13.self_attn.v_proj.input_scale', 'model.language_model.layers.13.self_attn.v_proj.output_scale', 'model.language_model.layers.13.self_attn.v_proj.weight._data._data', 'model.language_model.layers.13.self_attn.v_proj.weight._scale', 'model.language_model.layers.13.self_attn.v_proj.weight._shift', 'model.language_model.layers.14.mlp.down_proj.input_scale', 'model.language_model.layers.14.mlp.down_proj.output_scale', 'model.language_model.layers.14.mlp.down_proj.weight._data._data', 'model.language_model.layers.14.mlp.down_proj.weight._scale', 'model.language_model.layers.14.mlp.down_proj.weight._shift', 'model.language_model.layers.14.mlp.gate_proj.input_scale', 'model.language_model.layers.14.mlp.gate_proj.output_scale', 'model.language_model.layers.14.mlp.gate_proj.weight._data._data', 'model.language_model.layers.14.mlp.gate_proj.weight._scale', 'model.language_model.layers.14.mlp.gate_proj.weight._shift', 'model.language_model.layers.14.mlp.up_proj.input_scale', 'model.language_model.layers.14.mlp.up_proj.output_scale', 'model.language_model.layers.14.mlp.up_proj.weight._data._data', 'model.language_model.layers.14.mlp.up_proj.weight._scale', 'model.language_model.layers.14.mlp.up_proj.weight._shift', 'model.language_model.layers.14.self_attn.k_proj.input_scale', 'model.language_model.layers.14.self_attn.k_proj.output_scale', 'model.language_model.layers.14.self_attn.k_proj.weight._data._data', 'model.language_model.layers.14.self_attn.k_proj.weight._scale', 'model.language_model.layers.14.self_attn.k_proj.weight._shift', 'model.language_model.layers.14.self_attn.o_proj.input_scale', 'model.language_model.layers.14.self_attn.o_proj.output_scale', 'model.language_model.layers.14.self_attn.o_proj.weight._data._data', 'model.language_model.layers.14.self_attn.o_proj.weight._scale', 'model.language_model.layers.14.self_attn.o_proj.weight._shift', 'model.language_model.layers.14.self_attn.q_proj.input_scale', 'model.language_model.layers.14.self_attn.q_proj.output_scale', 'model.language_model.layers.14.self_attn.q_proj.weight._data._data', 'model.language_model.layers.14.self_attn.q_proj.weight._scale', 'model.language_model.layers.14.self_attn.q_proj.weight._shift', 'model.language_model.layers.14.self_attn.v_proj.input_scale', 'model.language_model.layers.14.self_attn.v_proj.output_scale', 'model.language_model.layers.14.self_attn.v_proj.weight._data._data', 'model.language_model.layers.14.self_attn.v_proj.weight._scale', 'model.language_model.layers.14.self_attn.v_proj.weight._shift', 'model.language_model.layers.15.mlp.down_proj.input_scale', 'model.language_model.layers.15.mlp.down_proj.output_scale', 'model.language_model.layers.15.mlp.down_proj.weight._data._data', 'model.language_model.layers.15.mlp.down_proj.weight._scale', 'model.language_model.layers.15.mlp.down_proj.weight._shift', 'model.language_model.layers.15.mlp.gate_proj.input_scale', 'model.language_model.layers.15.mlp.gate_proj.output_scale', 'model.language_model.layers.15.mlp.gate_proj.weight._data._data', 'model.language_model.layers.15.mlp.gate_proj.weight._scale', 'model.language_model.layers.15.mlp.gate_proj.weight._shift', 'model.language_model.layers.15.mlp.up_proj.input_scale', 'model.language_model.layers.15.mlp.up_proj.output_scale', 'model.language_model.layers.15.mlp.up_proj.weight._data._data', 'model.language_model.layers.15.mlp.up_proj.weight._scale', 'model.language_model.layers.15.mlp.up_proj.weight._shift', 'model.language_model.layers.15.self_attn.k_proj.input_scale', 'model.language_model.layers.15.self_attn.k_proj.output_scale', 'model.language_model.layers.15.self_attn.k_proj.weight._data._data', 'model.language_model.layers.15.self_attn.k_proj.weight._scale', 'model.language_model.layers.15.self_attn.k_proj.weight._shift', 'model.language_model.layers.15.self_attn.o_proj.input_scale', 'model.language_model.layers.15.self_attn.o_proj.output_scale', 'model.language_model.layers.15.self_attn.o_proj.weight._data._data', 'model.language_model.layers.15.self_attn.o_proj.weight._scale', 'model.language_model.layers.15.self_attn.o_proj.weight._shift', 'model.language_model.layers.15.self_attn.q_proj.input_scale', 'model.language_model.layers.15.self_attn.q_proj.output_scale', 'model.language_model.layers.15.self_attn.q_proj.weight._data._data', 'model.language_model.layers.15.self_attn.q_proj.weight._scale', 'model.language_model.layers.15.self_attn.q_proj.weight._shift', 'model.language_model.layers.15.self_attn.v_proj.input_scale', 'model.language_model.layers.15.self_attn.v_proj.output_scale', 'model.language_model.layers.15.self_attn.v_proj.weight._data._data', 'model.language_model.layers.15.self_attn.v_proj.weight._scale', 'model.language_model.layers.15.self_attn.v_proj.weight._shift', 'model.language_model.layers.16.mlp.down_proj.input_scale', 'model.language_model.layers.16.mlp.down_proj.output_scale', 'model.language_model.layers.16.mlp.down_proj.weight._data._data', 'model.language_model.layers.16.mlp.down_proj.weight._scale', 'model.language_model.layers.16.mlp.down_proj.weight._shift', 'model.language_model.layers.16.mlp.gate_proj.input_scale', 'model.language_model.layers.16.mlp.gate_proj.output_scale', 'model.language_model.layers.16.mlp.gate_proj.weight._data._data', 'model.language_model.layers.16.mlp.gate_proj.weight._scale', 'model.language_model.layers.16.mlp.gate_proj.weight._shift', 'model.language_model.layers.16.mlp.up_proj.input_scale', 'model.language_model.layers.16.mlp.up_proj.output_scale', 'model.language_model.layers.16.mlp.up_proj.weight._data._data', 'model.language_model.layers.16.mlp.up_proj.weight._scale', 'model.language_model.layers.16.mlp.up_proj.weight._shift', 'model.language_model.layers.16.self_attn.k_proj.input_scale', 'model.language_model.layers.16.self_attn.k_proj.output_scale', 'model.language_model.layers.16.self_attn.k_proj.weight._data._data', 'model.language_model.layers.16.self_attn.k_proj.weight._scale', 'model.language_model.layers.16.self_attn.k_proj.weight._shift', 'model.language_model.layers.16.self_attn.o_proj.input_scale', 'model.language_model.layers.16.self_attn.o_proj.output_scale', 'model.language_model.layers.16.self_attn.o_proj.weight._data._data', 'model.language_model.layers.16.self_attn.o_proj.weight._scale', 'model.language_model.layers.16.self_attn.o_proj.weight._shift', 'model.language_model.layers.16.self_attn.q_proj.input_scale', 'model.language_model.layers.16.self_attn.q_proj.output_scale', 'model.language_model.layers.16.self_attn.q_proj.weight._data._data', 'model.language_model.layers.16.self_attn.q_proj.weight._scale', 'model.language_model.layers.16.self_attn.q_proj.weight._shift', 'model.language_model.layers.16.self_attn.v_proj.input_scale', 'model.language_model.layers.16.self_attn.v_proj.output_scale', 'model.language_model.layers.16.self_attn.v_proj.weight._data._data', 'model.language_model.layers.16.self_attn.v_proj.weight._scale', 'model.language_model.layers.16.self_attn.v_proj.weight._shift', 'model.language_model.layers.17.mlp.down_proj.input_scale', 'model.language_model.layers.17.mlp.down_proj.output_scale', 'model.language_model.layers.17.mlp.down_proj.weight._data._data', 'model.language_model.layers.17.mlp.down_proj.weight._scale', 'model.language_model.layers.17.mlp.down_proj.weight._shift', 'model.language_model.layers.17.mlp.gate_proj.input_scale', 'model.language_model.layers.17.mlp.gate_proj.output_scale', 'model.language_model.layers.17.mlp.gate_proj.weight._data._data', 'model.language_model.layers.17.mlp.gate_proj.weight._scale', 'model.language_model.layers.17.mlp.gate_proj.weight._shift', 'model.language_model.layers.17.mlp.up_proj.input_scale', 'model.language_model.layers.17.mlp.up_proj.output_scale', 'model.language_model.layers.17.mlp.up_proj.weight._data._data', 'model.language_model.layers.17.mlp.up_proj.weight._scale', 'model.language_model.layers.17.mlp.up_proj.weight._shift', 'model.language_model.layers.17.self_attn.k_proj.input_scale', 'model.language_model.layers.17.self_attn.k_proj.output_scale', 'model.language_model.layers.17.self_attn.k_proj.weight._data._data', 'model.language_model.layers.17.self_attn.k_proj.weight._scale', 'model.language_model.layers.17.self_attn.k_proj.weight._shift', 'model.language_model.layers.17.self_attn.o_proj.input_scale', 'model.language_model.layers.17.self_attn.o_proj.output_scale', 'model.language_model.layers.17.self_attn.o_proj.weight._data._data', 'model.language_model.layers.17.self_attn.o_proj.weight._scale', 'model.language_model.layers.17.self_attn.o_proj.weight._shift', 'model.language_model.layers.17.self_attn.q_proj.input_scale', 'model.language_model.layers.17.self_attn.q_proj.output_scale', 'model.language_model.layers.17.self_attn.q_proj.weight._data._data', 'model.language_model.layers.17.self_attn.q_proj.weight._scale', 'model.language_model.layers.17.self_attn.q_proj.weight._shift', 'model.language_model.layers.17.self_attn.v_proj.input_scale', 'model.language_model.layers.17.self_attn.v_proj.output_scale', 'model.language_model.layers.17.self_attn.v_proj.weight._data._data', 'model.language_model.layers.17.self_attn.v_proj.weight._scale', 'model.language_model.layers.17.self_attn.v_proj.weight._shift', 'model.language_model.layers.18.mlp.down_proj.input_scale', 'model.language_model.layers.18.mlp.down_proj.output_scale', 'model.language_model.layers.18.mlp.down_proj.weight._data._data', 'model.language_model.layers.18.mlp.down_proj.weight._scale', 'model.language_model.layers.18.mlp.down_proj.weight._shift', 'model.language_model.layers.18.mlp.gate_proj.input_scale', 'model.language_model.layers.18.mlp.gate_proj.output_scale', 'model.language_model.layers.18.mlp.gate_proj.weight._data._data', 'model.language_model.layers.18.mlp.gate_proj.weight._scale', 'model.language_model.layers.18.mlp.gate_proj.weight._shift', 'model.language_model.layers.18.mlp.up_proj.input_scale', 'model.language_model.layers.18.mlp.up_proj.output_scale', 'model.language_model.layers.18.mlp.up_proj.weight._data._data', 'model.language_model.layers.18.mlp.up_proj.weight._scale', 'model.language_model.layers.18.mlp.up_proj.weight._shift', 'model.language_model.layers.18.self_attn.k_proj.input_scale', 'model.language_model.layers.18.self_attn.k_proj.output_scale', 'model.language_model.layers.18.self_attn.k_proj.weight._data._data', 'model.language_model.layers.18.self_attn.k_proj.weight._scale', 'model.language_model.layers.18.self_attn.k_proj.weight._shift', 'model.language_model.layers.18.self_attn.o_proj.input_scale', 'model.language_model.layers.18.self_attn.o_proj.output_scale', 'model.language_model.layers.18.self_attn.o_proj.weight._data._data', 'model.language_model.layers.18.self_attn.o_proj.weight._scale', 'model.language_model.layers.18.self_attn.o_proj.weight._shift', 'model.language_model.layers.18.self_attn.q_proj.input_scale', 'model.language_model.layers.18.self_attn.q_proj.output_scale', 'model.language_model.layers.18.self_attn.q_proj.weight._data._data', 'model.language_model.layers.18.self_attn.q_proj.weight._scale', 'model.language_model.layers.18.self_attn.q_proj.weight._shift', 'model.language_model.layers.18.self_attn.v_proj.input_scale', 'model.language_model.layers.18.self_attn.v_proj.output_scale', 'model.language_model.layers.18.self_attn.v_proj.weight._data._data', 'model.language_model.layers.18.self_attn.v_proj.weight._scale', 'model.language_model.layers.18.self_attn.v_proj.weight._shift', 'model.language_model.layers.19.mlp.down_proj.input_scale', 'model.language_model.layers.19.mlp.down_proj.output_scale', 'model.language_model.layers.19.mlp.down_proj.weight._data._data', 'model.language_model.layers.19.mlp.down_proj.weight._scale', 'model.language_model.layers.19.mlp.down_proj.weight._shift', 'model.language_model.layers.19.mlp.gate_proj.input_scale', 'model.language_model.layers.19.mlp.gate_proj.output_scale', 'model.language_model.layers.19.mlp.gate_proj.weight._data._data', 'model.language_model.layers.19.mlp.gate_proj.weight._scale', 'model.language_model.layers.19.mlp.gate_proj.weight._shift', 'model.language_model.layers.19.mlp.up_proj.input_scale', 'model.language_model.layers.19.mlp.up_proj.output_scale', 'model.language_model.layers.19.mlp.up_proj.weight._data._data', 'model.language_model.layers.19.mlp.up_proj.weight._scale', 'model.language_model.layers.19.mlp.up_proj.weight._shift', 'model.language_model.layers.19.self_attn.k_proj.input_scale', 'model.language_model.layers.19.self_attn.k_proj.output_scale', 'model.language_model.layers.19.self_attn.k_proj.weight._data._data', 'model.language_model.layers.19.self_attn.k_proj.weight._scale', 'model.language_model.layers.19.self_attn.k_proj.weight._shift', 'model.language_model.layers.19.self_attn.o_proj.input_scale', 'model.language_model.layers.19.self_attn.o_proj.output_scale', 'model.language_model.layers.19.self_attn.o_proj.weight._data._data', 'model.language_model.layers.19.self_attn.o_proj.weight._scale', 'model.language_model.layers.19.self_attn.o_proj.weight._shift', 'model.language_model.layers.19.self_attn.q_proj.input_scale', 'model.language_model.layers.19.self_attn.q_proj.output_scale', 'model.language_model.layers.19.self_attn.q_proj.weight._data._data', 'model.language_model.layers.19.self_attn.q_proj.weight._scale', 'model.language_model.layers.19.self_attn.q_proj.weight._shift', 'model.language_model.layers.19.self_attn.v_proj.input_scale', 'model.language_model.layers.19.self_attn.v_proj.output_scale', 'model.language_model.layers.19.self_attn.v_proj.weight._data._data', 'model.language_model.layers.19.self_attn.v_proj.weight._scale', 'model.language_model.layers.19.self_attn.v_proj.weight._shift', 'model.language_model.layers.2.mlp.down_proj.input_scale', 'model.language_model.layers.2.mlp.down_proj.output_scale', 'model.language_model.layers.2.mlp.down_proj.weight._data._data', 'model.language_model.layers.2.mlp.down_proj.weight._scale', 'model.language_model.layers.2.mlp.down_proj.weight._shift', 'model.language_model.layers.2.mlp.gate_proj.input_scale', 'model.language_model.layers.2.mlp.gate_proj.output_scale', 'model.language_model.layers.2.mlp.gate_proj.weight._data._data', 'model.language_model.layers.2.mlp.gate_proj.weight._scale', 'model.language_model.layers.2.mlp.gate_proj.weight._shift', 'model.language_model.layers.2.mlp.up_proj.input_scale', 'model.language_model.layers.2.mlp.up_proj.output_scale', 'model.language_model.layers.2.mlp.up_proj.weight._data._data', 'model.language_model.layers.2.mlp.up_proj.weight._scale', 'model.language_model.layers.2.mlp.up_proj.weight._shift', 'model.language_model.layers.2.self_attn.k_proj.input_scale', 'model.language_model.layers.2.self_attn.k_proj.output_scale', 'model.language_model.layers.2.self_attn.k_proj.weight._data._data', 'model.language_model.layers.2.self_attn.k_proj.weight._scale', 'model.language_model.layers.2.self_attn.k_proj.weight._shift', 'model.language_model.layers.2.self_attn.o_proj.input_scale', 'model.language_model.layers.2.self_attn.o_proj.output_scale', 'model.language_model.layers.2.self_attn.o_proj.weight._data._data', 'model.language_model.layers.2.self_attn.o_proj.weight._scale', 'model.language_model.layers.2.self_attn.o_proj.weight._shift', 'model.language_model.layers.2.self_attn.q_proj.input_scale', 'model.language_model.layers.2.self_attn.q_proj.output_scale', 'model.language_model.layers.2.self_attn.q_proj.weight._data._data', 'model.language_model.layers.2.self_attn.q_proj.weight._scale', 'model.language_model.layers.2.self_attn.q_proj.weight._shift', 'model.language_model.layers.2.self_attn.v_proj.input_scale', 'model.language_model.layers.2.self_attn.v_proj.output_scale', 'model.language_model.layers.2.self_attn.v_proj.weight._data._data', 'model.language_model.layers.2.self_attn.v_proj.weight._scale', 'model.language_model.layers.2.self_attn.v_proj.weight._shift', 'model.language_model.layers.20.mlp.down_proj.input_scale', 'model.language_model.layers.20.mlp.down_proj.output_scale', 'model.language_model.layers.20.mlp.down_proj.weight._data._data', 'model.language_model.layers.20.mlp.down_proj.weight._scale', 'model.language_model.layers.20.mlp.down_proj.weight._shift', 'model.language_model.layers.20.mlp.gate_proj.input_scale', 'model.language_model.layers.20.mlp.gate_proj.output_scale', 'model.language_model.layers.20.mlp.gate_proj.weight._data._data', 'model.language_model.layers.20.mlp.gate_proj.weight._scale', 'model.language_model.layers.20.mlp.gate_proj.weight._shift', 'model.language_model.layers.20.mlp.up_proj.input_scale', 'model.language_model.layers.20.mlp.up_proj.output_scale', 'model.language_model.layers.20.mlp.up_proj.weight._data._data', 'model.language_model.layers.20.mlp.up_proj.weight._scale', 'model.language_model.layers.20.mlp.up_proj.weight._shift', 'model.language_model.layers.20.self_attn.k_proj.input_scale', 'model.language_model.layers.20.self_attn.k_proj.output_scale', 'model.language_model.layers.20.self_attn.k_proj.weight._data._data', 'model.language_model.layers.20.self_attn.k_proj.weight._scale', 'model.language_model.layers.20.self_attn.k_proj.weight._shift', 'model.language_model.layers.20.self_attn.o_proj.input_scale', 'model.language_model.layers.20.self_attn.o_proj.output_scale', 'model.language_model.layers.20.self_attn.o_proj.weight._data._data', 'model.language_model.layers.20.self_attn.o_proj.weight._scale', 'model.language_model.layers.20.self_attn.o_proj.weight._shift', 'model.language_model.layers.20.self_attn.q_proj.input_scale', 'model.language_model.layers.20.self_attn.q_proj.output_scale', 'model.language_model.layers.20.self_attn.q_proj.weight._data._data', 'model.language_model.layers.20.self_attn.q_proj.weight._scale', 'model.language_model.layers.20.self_attn.q_proj.weight._shift', 'model.language_model.layers.20.self_attn.v_proj.input_scale', 'model.language_model.layers.20.self_attn.v_proj.output_scale', 'model.language_model.layers.20.self_attn.v_proj.weight._data._data', 'model.language_model.layers.20.self_attn.v_proj.weight._scale', 'model.language_model.layers.20.self_attn.v_proj.weight._shift', 'model.language_model.layers.21.mlp.down_proj.input_scale', 'model.language_model.layers.21.mlp.down_proj.output_scale', 'model.language_model.layers.21.mlp.down_proj.weight._data._data', 'model.language_model.layers.21.mlp.down_proj.weight._scale', 'model.language_model.layers.21.mlp.down_proj.weight._shift', 'model.language_model.layers.21.mlp.gate_proj.input_scale', 'model.language_model.layers.21.mlp.gate_proj.output_scale', 'model.language_model.layers.21.mlp.gate_proj.weight._data._data', 'model.language_model.layers.21.mlp.gate_proj.weight._scale', 'model.language_model.layers.21.mlp.gate_proj.weight._shift', 'model.language_model.layers.21.mlp.up_proj.input_scale', 'model.language_model.layers.21.mlp.up_proj.output_scale', 'model.language_model.layers.21.mlp.up_proj.weight._data._data', 'model.language_model.layers.21.mlp.up_proj.weight._scale', 'model.language_model.layers.21.mlp.up_proj.weight._shift', 'model.language_model.layers.21.self_attn.k_proj.input_scale', 'model.language_model.layers.21.self_attn.k_proj.output_scale', 'model.language_model.layers.21.self_attn.k_proj.weight._data._data', 'model.language_model.layers.21.self_attn.k_proj.weight._scale', 'model.language_model.layers.21.self_attn.k_proj.weight._shift', 'model.language_model.layers.21.self_attn.o_proj.input_scale', 'model.language_model.layers.21.self_attn.o_proj.output_scale', 'model.language_model.layers.21.self_attn.o_proj.weight._data._data', 'model.language_model.layers.21.self_attn.o_proj.weight._scale', 'model.language_model.layers.21.self_attn.o_proj.weight._shift', 'model.language_model.layers.21.self_attn.q_proj.input_scale', 'model.language_model.layers.21.self_attn.q_proj.output_scale', 'model.language_model.layers.21.self_attn.q_proj.weight._data._data', 'model.language_model.layers.21.self_attn.q_proj.weight._scale', 'model.language_model.layers.21.self_attn.q_proj.weight._shift', 'model.language_model.layers.21.self_attn.v_proj.input_scale', 'model.language_model.layers.21.self_attn.v_proj.output_scale', 'model.language_model.layers.21.self_attn.v_proj.weight._data._data', 'model.language_model.layers.21.self_attn.v_proj.weight._scale', 'model.language_model.layers.21.self_attn.v_proj.weight._shift', 'model.language_model.layers.22.mlp.down_proj.input_scale', 'model.language_model.layers.22.mlp.down_proj.output_scale', 'model.language_model.layers.22.mlp.down_proj.weight._data._data', 'model.language_model.layers.22.mlp.down_proj.weight._scale', 'model.language_model.layers.22.mlp.down_proj.weight._shift', 'model.language_model.layers.22.mlp.gate_proj.input_scale', 'model.language_model.layers.22.mlp.gate_proj.output_scale', 'model.language_model.layers.22.mlp.gate_proj.weight._data._data', 'model.language_model.layers.22.mlp.gate_proj.weight._scale', 'model.language_model.layers.22.mlp.gate_proj.weight._shift', 'model.language_model.layers.22.mlp.up_proj.input_scale', 'model.language_model.layers.22.mlp.up_proj.output_scale', 'model.language_model.layers.22.mlp.up_proj.weight._data._data', 'model.language_model.layers.22.mlp.up_proj.weight._scale', 'model.language_model.layers.22.mlp.up_proj.weight._shift', 'model.language_model.layers.22.self_attn.k_proj.input_scale', 'model.language_model.layers.22.self_attn.k_proj.output_scale', 'model.language_model.layers.22.self_attn.k_proj.weight._data._data', 'model.language_model.layers.22.self_attn.k_proj.weight._scale', 'model.language_model.layers.22.self_attn.k_proj.weight._shift', 'model.language_model.layers.22.self_attn.o_proj.input_scale', 'model.language_model.layers.22.self_attn.o_proj.output_scale', 'model.language_model.layers.22.self_attn.o_proj.weight._data._data', 'model.language_model.layers.22.self_attn.o_proj.weight._scale', 'model.language_model.layers.22.self_attn.o_proj.weight._shift', 'model.language_model.layers.22.self_attn.q_proj.input_scale', 'model.language_model.layers.22.self_attn.q_proj.output_scale', 'model.language_model.layers.22.self_attn.q_proj.weight._data._data', 'model.language_model.layers.22.self_attn.q_proj.weight._scale', 'model.language_model.layers.22.self_attn.q_proj.weight._shift', 'model.language_model.layers.22.self_attn.v_proj.input_scale', 'model.language_model.layers.22.self_attn.v_proj.output_scale', 'model.language_model.layers.22.self_attn.v_proj.weight._data._data', 'model.language_model.layers.22.self_attn.v_proj.weight._scale', 'model.language_model.layers.22.self_attn.v_proj.weight._shift', 'model.language_model.layers.23.mlp.down_proj.input_scale', 'model.language_model.layers.23.mlp.down_proj.output_scale', 'model.language_model.layers.23.mlp.down_proj.weight._data._data', 'model.language_model.layers.23.mlp.down_proj.weight._scale', 'model.language_model.layers.23.mlp.down_proj.weight._shift', 'model.language_model.layers.23.mlp.gate_proj.input_scale', 'model.language_model.layers.23.mlp.gate_proj.output_scale', 'model.language_model.layers.23.mlp.gate_proj.weight._data._data', 'model.language_model.layers.23.mlp.gate_proj.weight._scale', 'model.language_model.layers.23.mlp.gate_proj.weight._shift', 'model.language_model.layers.23.mlp.up_proj.input_scale', 'model.language_model.layers.23.mlp.up_proj.output_scale', 'model.language_model.layers.23.mlp.up_proj.weight._data._data', 'model.language_model.layers.23.mlp.up_proj.weight._scale', 'model.language_model.layers.23.mlp.up_proj.weight._shift', 'model.language_model.layers.23.self_attn.k_proj.input_scale', 'model.language_model.layers.23.self_attn.k_proj.output_scale', 'model.language_model.layers.23.self_attn.k_proj.weight._data._data', 'model.language_model.layers.23.self_attn.k_proj.weight._scale', 'model.language_model.layers.23.self_attn.k_proj.weight._shift', 'model.language_model.layers.23.self_attn.o_proj.input_scale', 'model.language_model.layers.23.self_attn.o_proj.output_scale', 'model.language_model.layers.23.self_attn.o_proj.weight._data._data', 'model.language_model.layers.23.self_attn.o_proj.weight._scale', 'model.language_model.layers.23.self_attn.o_proj.weight._shift', 'model.language_model.layers.23.self_attn.q_proj.input_scale', 'model.language_model.layers.23.self_attn.q_proj.output_scale', 'model.language_model.layers.23.self_attn.q_proj.weight._data._data', 'model.language_model.layers.23.self_attn.q_proj.weight._scale', 'model.language_model.layers.23.self_attn.q_proj.weight._shift', 'model.language_model.layers.23.self_attn.v_proj.input_scale', 'model.language_model.layers.23.self_attn.v_proj.output_scale', 'model.language_model.layers.23.self_attn.v_proj.weight._data._data', 'model.language_model.layers.23.self_attn.v_proj.weight._scale', 'model.language_model.layers.23.self_attn.v_proj.weight._shift', 'model.language_model.layers.24.mlp.down_proj.input_scale', 'model.language_model.layers.24.mlp.down_proj.output_scale', 'model.language_model.layers.24.mlp.down_proj.weight._data._data', 'model.language_model.layers.24.mlp.down_proj.weight._scale', 'model.language_model.layers.24.mlp.down_proj.weight._shift', 'model.language_model.layers.24.mlp.gate_proj.input_scale', 'model.language_model.layers.24.mlp.gate_proj.output_scale', 'model.language_model.layers.24.mlp.gate_proj.weight._data._data', 'model.language_model.layers.24.mlp.gate_proj.weight._scale', 'model.language_model.layers.24.mlp.gate_proj.weight._shift', 'model.language_model.layers.24.mlp.up_proj.input_scale', 'model.language_model.layers.24.mlp.up_proj.output_scale', 'model.language_model.layers.24.mlp.up_proj.weight._data._data', 'model.language_model.layers.24.mlp.up_proj.weight._scale', 'model.language_model.layers.24.mlp.up_proj.weight._shift', 'model.language_model.layers.24.self_attn.k_proj.input_scale', 'model.language_model.layers.24.self_attn.k_proj.output_scale', 'model.language_model.layers.24.self_attn.k_proj.weight._data._data', 'model.language_model.layers.24.self_attn.k_proj.weight._scale', 'model.language_model.layers.24.self_attn.k_proj.weight._shift', 'model.language_model.layers.24.self_attn.o_proj.input_scale', 'model.language_model.layers.24.self_attn.o_proj.output_scale', 'model.language_model.layers.24.self_attn.o_proj.weight._data._data', 'model.language_model.layers.24.self_attn.o_proj.weight._scale', 'model.language_model.layers.24.self_attn.o_proj.weight._shift', 'model.language_model.layers.24.self_attn.q_proj.input_scale', 'model.language_model.layers.24.self_attn.q_proj.output_scale', 'model.language_model.layers.24.self_attn.q_proj.weight._data._data', 'model.language_model.layers.24.self_attn.q_proj.weight._scale', 'model.language_model.layers.24.self_attn.q_proj.weight._shift', 'model.language_model.layers.24.self_attn.v_proj.input_scale', 'model.language_model.layers.24.self_attn.v_proj.output_scale', 'model.language_model.layers.24.self_attn.v_proj.weight._data._data', 'model.language_model.layers.24.self_attn.v_proj.weight._scale', 'model.language_model.layers.24.self_attn.v_proj.weight._shift', 'model.language_model.layers.25.mlp.down_proj.input_scale', 'model.language_model.layers.25.mlp.down_proj.output_scale', 'model.language_model.layers.25.mlp.down_proj.weight._data._data', 'model.language_model.layers.25.mlp.down_proj.weight._scale', 'model.language_model.layers.25.mlp.down_proj.weight._shift', 'model.language_model.layers.25.mlp.gate_proj.input_scale', 'model.language_model.layers.25.mlp.gate_proj.output_scale', 'model.language_model.layers.25.mlp.gate_proj.weight._data._data', 'model.language_model.layers.25.mlp.gate_proj.weight._scale', 'model.language_model.layers.25.mlp.gate_proj.weight._shift', 'model.language_model.layers.25.mlp.up_proj.input_scale', 'model.language_model.layers.25.mlp.up_proj.output_scale', 'model.language_model.layers.25.mlp.up_proj.weight._data._data', 'model.language_model.layers.25.mlp.up_proj.weight._scale', 'model.language_model.layers.25.mlp.up_proj.weight._shift', 'model.language_model.layers.25.self_attn.k_proj.input_scale', 'model.language_model.layers.25.self_attn.k_proj.output_scale', 'model.language_model.layers.25.self_attn.k_proj.weight._data._data', 'model.language_model.layers.25.self_attn.k_proj.weight._scale', 'model.language_model.layers.25.self_attn.k_proj.weight._shift', 'model.language_model.layers.25.self_attn.o_proj.input_scale', 'model.language_model.layers.25.self_attn.o_proj.output_scale', 'model.language_model.layers.25.self_attn.o_proj.weight._data._data', 'model.language_model.layers.25.self_attn.o_proj.weight._scale', 'model.language_model.layers.25.self_attn.o_proj.weight._shift', 'model.language_model.layers.25.self_attn.q_proj.input_scale', 'model.language_model.layers.25.self_attn.q_proj.output_scale', 'model.language_model.layers.25.self_attn.q_proj.weight._data._data', 'model.language_model.layers.25.self_attn.q_proj.weight._scale', 'model.language_model.layers.25.self_attn.q_proj.weight._shift', 'model.language_model.layers.25.self_attn.v_proj.input_scale', 'model.language_model.layers.25.self_attn.v_proj.output_scale', 'model.language_model.layers.25.self_attn.v_proj.weight._data._data', 'model.language_model.layers.25.self_attn.v_proj.weight._scale', 'model.language_model.layers.25.self_attn.v_proj.weight._shift', 'model.language_model.layers.26.mlp.down_proj.input_scale', 'model.language_model.layers.26.mlp.down_proj.output_scale', 'model.language_model.layers.26.mlp.down_proj.weight._data._data', 'model.language_model.layers.26.mlp.down_proj.weight._scale', 'model.language_model.layers.26.mlp.down_proj.weight._shift', 'model.language_model.layers.26.mlp.gate_proj.input_scale', 'model.language_model.layers.26.mlp.gate_proj.output_scale', 'model.language_model.layers.26.mlp.gate_proj.weight._data._data', 'model.language_model.layers.26.mlp.gate_proj.weight._scale', 'model.language_model.layers.26.mlp.gate_proj.weight._shift', 'model.language_model.layers.26.mlp.up_proj.input_scale', 'model.language_model.layers.26.mlp.up_proj.output_scale', 'model.language_model.layers.26.mlp.up_proj.weight._data._data', 'model.language_model.layers.26.mlp.up_proj.weight._scale', 'model.language_model.layers.26.mlp.up_proj.weight._shift', 'model.language_model.layers.26.self_attn.k_proj.input_scale', 'model.language_model.layers.26.self_attn.k_proj.output_scale', 'model.language_model.layers.26.self_attn.k_proj.weight._data._data', 'model.language_model.layers.26.self_attn.k_proj.weight._scale', 'model.language_model.layers.26.self_attn.k_proj.weight._shift', 'model.language_model.layers.26.self_attn.o_proj.input_scale', 'model.language_model.layers.26.self_attn.o_proj.output_scale', 'model.language_model.layers.26.self_attn.o_proj.weight._data._data', 'model.language_model.layers.26.self_attn.o_proj.weight._scale', 'model.language_model.layers.26.self_attn.o_proj.weight._shift', 'model.language_model.layers.26.self_attn.q_proj.input_scale', 'model.language_model.layers.26.self_attn.q_proj.output_scale', 'model.language_model.layers.26.self_attn.q_proj.weight._data._data', 'model.language_model.layers.26.self_attn.q_proj.weight._scale', 'model.language_model.layers.26.self_attn.q_proj.weight._shift', 'model.language_model.layers.26.self_attn.v_proj.input_scale', 'model.language_model.layers.26.self_attn.v_proj.output_scale', 'model.language_model.layers.26.self_attn.v_proj.weight._data._data', 'model.language_model.layers.26.self_attn.v_proj.weight._scale', 'model.language_model.layers.26.self_attn.v_proj.weight._shift', 'model.language_model.layers.27.mlp.down_proj.input_scale', 'model.language_model.layers.27.mlp.down_proj.output_scale', 'model.language_model.layers.27.mlp.down_proj.weight._data._data', 'model.language_model.layers.27.mlp.down_proj.weight._scale', 'model.language_model.layers.27.mlp.down_proj.weight._shift', 'model.language_model.layers.27.mlp.gate_proj.input_scale', 'model.language_model.layers.27.mlp.gate_proj.output_scale', 'model.language_model.layers.27.mlp.gate_proj.weight._data._data', 'model.language_model.layers.27.mlp.gate_proj.weight._scale', 'model.language_model.layers.27.mlp.gate_proj.weight._shift', 'model.language_model.layers.27.mlp.up_proj.input_scale', 'model.language_model.layers.27.mlp.up_proj.output_scale', 'model.language_model.layers.27.mlp.up_proj.weight._data._data', 'model.language_model.layers.27.mlp.up_proj.weight._scale', 'model.language_model.layers.27.mlp.up_proj.weight._shift', 'model.language_model.layers.27.self_attn.k_proj.input_scale', 'model.language_model.layers.27.self_attn.k_proj.output_scale', 'model.language_model.layers.27.self_attn.k_proj.weight._data._data', 'model.language_model.layers.27.self_attn.k_proj.weight._scale', 'model.language_model.layers.27.self_attn.k_proj.weight._shift', 'model.language_model.layers.27.self_attn.o_proj.input_scale', 'model.language_model.layers.27.self_attn.o_proj.output_scale', 'model.language_model.layers.27.self_attn.o_proj.weight._data._data', 'model.language_model.layers.27.self_attn.o_proj.weight._scale', 'model.language_model.layers.27.self_attn.o_proj.weight._shift', 'model.language_model.layers.27.self_attn.q_proj.input_scale', 'model.language_model.layers.27.self_attn.q_proj.output_scale', 'model.language_model.layers.27.self_attn.q_proj.weight._data._data', 'model.language_model.layers.27.self_attn.q_proj.weight._scale', 'model.language_model.layers.27.self_attn.q_proj.weight._shift', 'model.language_model.layers.27.self_attn.v_proj.input_scale', 'model.language_model.layers.27.self_attn.v_proj.output_scale', 'model.language_model.layers.27.self_attn.v_proj.weight._data._data', 'model.language_model.layers.27.self_attn.v_proj.weight._scale', 'model.language_model.layers.27.self_attn.v_proj.weight._shift', 'model.language_model.layers.28.mlp.down_proj.input_scale', 'model.language_model.layers.28.mlp.down_proj.output_scale', 'model.language_model.layers.28.mlp.down_proj.weight._data._data', 'model.language_model.layers.28.mlp.down_proj.weight._scale', 'model.language_model.layers.28.mlp.down_proj.weight._shift', 'model.language_model.layers.28.mlp.gate_proj.input_scale', 'model.language_model.layers.28.mlp.gate_proj.output_scale', 'model.language_model.layers.28.mlp.gate_proj.weight._data._data', 'model.language_model.layers.28.mlp.gate_proj.weight._scale', 'model.language_model.layers.28.mlp.gate_proj.weight._shift', 'model.language_model.layers.28.mlp.up_proj.input_scale', 'model.language_model.layers.28.mlp.up_proj.output_scale', 'model.language_model.layers.28.mlp.up_proj.weight._data._data', 'model.language_model.layers.28.mlp.up_proj.weight._scale', 'model.language_model.layers.28.mlp.up_proj.weight._shift', 'model.language_model.layers.28.self_attn.k_proj.input_scale', 'model.language_model.layers.28.self_attn.k_proj.output_scale', 'model.language_model.layers.28.self_attn.k_proj.weight._data._data', 'model.language_model.layers.28.self_attn.k_proj.weight._scale', 'model.language_model.layers.28.self_attn.k_proj.weight._shift', 'model.language_model.layers.28.self_attn.o_proj.input_scale', 'model.language_model.layers.28.self_attn.o_proj.output_scale', 'model.language_model.layers.28.self_attn.o_proj.weight._data._data', 'model.language_model.layers.28.self_attn.o_proj.weight._scale', 'model.language_model.layers.28.self_attn.o_proj.weight._shift', 'model.language_model.layers.28.self_attn.q_proj.input_scale', 'model.language_model.layers.28.self_attn.q_proj.output_scale', 'model.language_model.layers.28.self_attn.q_proj.weight._data._data', 'model.language_model.layers.28.self_attn.q_proj.weight._scale', 'model.language_model.layers.28.self_attn.q_proj.weight._shift', 'model.language_model.layers.28.self_attn.v_proj.input_scale', 'model.language_model.layers.28.self_attn.v_proj.output_scale', 'model.language_model.layers.28.self_attn.v_proj.weight._data._data', 'model.language_model.layers.28.self_attn.v_proj.weight._scale', 'model.language_model.layers.28.self_attn.v_proj.weight._shift', 'model.language_model.layers.29.mlp.down_proj.input_scale', 'model.language_model.layers.29.mlp.down_proj.output_scale', 'model.language_model.layers.29.mlp.down_proj.weight._data._data', 'model.language_model.layers.29.mlp.down_proj.weight._scale', 'model.language_model.layers.29.mlp.down_proj.weight._shift', 'model.language_model.layers.29.mlp.gate_proj.input_scale', 'model.language_model.layers.29.mlp.gate_proj.output_scale', 'model.language_model.layers.29.mlp.gate_proj.weight._data._data', 'model.language_model.layers.29.mlp.gate_proj.weight._scale', 'model.language_model.layers.29.mlp.gate_proj.weight._shift', 'model.language_model.layers.29.mlp.up_proj.input_scale', 'model.language_model.layers.29.mlp.up_proj.output_scale', 'model.language_model.layers.29.mlp.up_proj.weight._data._data', 'model.language_model.layers.29.mlp.up_proj.weight._scale', 'model.language_model.layers.29.mlp.up_proj.weight._shift', 'model.language_model.layers.29.self_attn.k_proj.input_scale', 'model.language_model.layers.29.self_attn.k_proj.output_scale', 'model.language_model.layers.29.self_attn.k_proj.weight._data._data', 'model.language_model.layers.29.self_attn.k_proj.weight._scale', 'model.language_model.layers.29.self_attn.k_proj.weight._shift', 'model.language_model.layers.29.self_attn.o_proj.input_scale', 'model.language_model.layers.29.self_attn.o_proj.output_scale', 'model.language_model.layers.29.self_attn.o_proj.weight._data._data', 'model.language_model.layers.29.self_attn.o_proj.weight._scale', 'model.language_model.layers.29.self_attn.o_proj.weight._shift', 'model.language_model.layers.29.self_attn.q_proj.input_scale', 'model.language_model.layers.29.self_attn.q_proj.output_scale', 'model.language_model.layers.29.self_attn.q_proj.weight._data._data', 'model.language_model.layers.29.self_attn.q_proj.weight._scale', 'model.language_model.layers.29.self_attn.q_proj.weight._shift', 'model.language_model.layers.29.self_attn.v_proj.input_scale', 'model.language_model.layers.29.self_attn.v_proj.output_scale', 'model.language_model.layers.29.self_attn.v_proj.weight._data._data', 'model.language_model.layers.29.self_attn.v_proj.weight._scale', 'model.language_model.layers.29.self_attn.v_proj.weight._shift', 'model.language_model.layers.3.mlp.down_proj.input_scale', 'model.language_model.layers.3.mlp.down_proj.output_scale', 'model.language_model.layers.3.mlp.down_proj.weight._data._data', 'model.language_model.layers.3.mlp.down_proj.weight._scale', 'model.language_model.layers.3.mlp.down_proj.weight._shift', 'model.language_model.layers.3.mlp.gate_proj.input_scale', 'model.language_model.layers.3.mlp.gate_proj.output_scale', 'model.language_model.layers.3.mlp.gate_proj.weight._data._data', 'model.language_model.layers.3.mlp.gate_proj.weight._scale', 'model.language_model.layers.3.mlp.gate_proj.weight._shift', 'model.language_model.layers.3.mlp.up_proj.input_scale', 'model.language_model.layers.3.mlp.up_proj.output_scale', 'model.language_model.layers.3.mlp.up_proj.weight._data._data', 'model.language_model.layers.3.mlp.up_proj.weight._scale', 'model.language_model.layers.3.mlp.up_proj.weight._shift', 'model.language_model.layers.3.self_attn.k_proj.input_scale', 'model.language_model.layers.3.self_attn.k_proj.output_scale', 'model.language_model.layers.3.self_attn.k_proj.weight._data._data', 'model.language_model.layers.3.self_attn.k_proj.weight._scale', 'model.language_model.layers.3.self_attn.k_proj.weight._shift', 'model.language_model.layers.3.self_attn.o_proj.input_scale', 'model.language_model.layers.3.self_attn.o_proj.output_scale', 'model.language_model.layers.3.self_attn.o_proj.weight._data._data', 'model.language_model.layers.3.self_attn.o_proj.weight._scale', 'model.language_model.layers.3.self_attn.o_proj.weight._shift', 'model.language_model.layers.3.self_attn.q_proj.input_scale', 'model.language_model.layers.3.self_attn.q_proj.output_scale', 'model.language_model.layers.3.self_attn.q_proj.weight._data._data', 'model.language_model.layers.3.self_attn.q_proj.weight._scale', 'model.language_model.layers.3.self_attn.q_proj.weight._shift', 'model.language_model.layers.3.self_attn.v_proj.input_scale', 'model.language_model.layers.3.self_attn.v_proj.output_scale', 'model.language_model.layers.3.self_attn.v_proj.weight._data._data', 'model.language_model.layers.3.self_attn.v_proj.weight._scale', 'model.language_model.layers.3.self_attn.v_proj.weight._shift', 'model.language_model.layers.30.mlp.down_proj.input_scale', 'model.language_model.layers.30.mlp.down_proj.output_scale', 'model.language_model.layers.30.mlp.down_proj.weight._data._data', 'model.language_model.layers.30.mlp.down_proj.weight._scale', 'model.language_model.layers.30.mlp.down_proj.weight._shift', 'model.language_model.layers.30.mlp.gate_proj.input_scale', 'model.language_model.layers.30.mlp.gate_proj.output_scale', 'model.language_model.layers.30.mlp.gate_proj.weight._data._data', 'model.language_model.layers.30.mlp.gate_proj.weight._scale', 'model.language_model.layers.30.mlp.gate_proj.weight._shift', 'model.language_model.layers.30.mlp.up_proj.input_scale', 'model.language_model.layers.30.mlp.up_proj.output_scale', 'model.language_model.layers.30.mlp.up_proj.weight._data._data', 'model.language_model.layers.30.mlp.up_proj.weight._scale', 'model.language_model.layers.30.mlp.up_proj.weight._shift', 'model.language_model.layers.30.self_attn.k_proj.input_scale', 'model.language_model.layers.30.self_attn.k_proj.output_scale', 'model.language_model.layers.30.self_attn.k_proj.weight._data._data', 'model.language_model.layers.30.self_attn.k_proj.weight._scale', 'model.language_model.layers.30.self_attn.k_proj.weight._shift', 'model.language_model.layers.30.self_attn.o_proj.input_scale', 'model.language_model.layers.30.self_attn.o_proj.output_scale', 'model.language_model.layers.30.self_attn.o_proj.weight._data._data', 'model.language_model.layers.30.self_attn.o_proj.weight._scale', 'model.language_model.layers.30.self_attn.o_proj.weight._shift', 'model.language_model.layers.30.self_attn.q_proj.input_scale', 'model.language_model.layers.30.self_attn.q_proj.output_scale', 'model.language_model.layers.30.self_attn.q_proj.weight._data._data', 'model.language_model.layers.30.self_attn.q_proj.weight._scale', 'model.language_model.layers.30.self_attn.q_proj.weight._shift', 'model.language_model.layers.30.self_attn.v_proj.input_scale', 'model.language_model.layers.30.self_attn.v_proj.output_scale', 'model.language_model.layers.30.self_attn.v_proj.weight._data._data', 'model.language_model.layers.30.self_attn.v_proj.weight._scale', 'model.language_model.layers.30.self_attn.v_proj.weight._shift', 'model.language_model.layers.31.mlp.down_proj.input_scale', 'model.language_model.layers.31.mlp.down_proj.output_scale', 'model.language_model.layers.31.mlp.down_proj.weight._data._data', 'model.language_model.layers.31.mlp.down_proj.weight._scale', 'model.language_model.layers.31.mlp.down_proj.weight._shift', 'model.language_model.layers.31.mlp.gate_proj.input_scale', 'model.language_model.layers.31.mlp.gate_proj.output_scale', 'model.language_model.layers.31.mlp.gate_proj.weight._data._data', 'model.language_model.layers.31.mlp.gate_proj.weight._scale', 'model.language_model.layers.31.mlp.gate_proj.weight._shift', 'model.language_model.layers.31.mlp.up_proj.input_scale', 'model.language_model.layers.31.mlp.up_proj.output_scale', 'model.language_model.layers.31.mlp.up_proj.weight._data._data', 'model.language_model.layers.31.mlp.up_proj.weight._scale', 'model.language_model.layers.31.mlp.up_proj.weight._shift', 'model.language_model.layers.31.self_attn.k_proj.input_scale', 'model.language_model.layers.31.self_attn.k_proj.output_scale', 'model.language_model.layers.31.self_attn.k_proj.weight._data._data', 'model.language_model.layers.31.self_attn.k_proj.weight._scale', 'model.language_model.layers.31.self_attn.k_proj.weight._shift', 'model.language_model.layers.31.self_attn.o_proj.input_scale', 'model.language_model.layers.31.self_attn.o_proj.output_scale', 'model.language_model.layers.31.self_attn.o_proj.weight._data._data', 'model.language_model.layers.31.self_attn.o_proj.weight._scale', 'model.language_model.layers.31.self_attn.o_proj.weight._shift', 'model.language_model.layers.31.self_attn.q_proj.input_scale', 'model.language_model.layers.31.self_attn.q_proj.output_scale', 'model.language_model.layers.31.self_attn.q_proj.weight._data._data', 'model.language_model.layers.31.self_attn.q_proj.weight._scale', 'model.language_model.layers.31.self_attn.q_proj.weight._shift', 'model.language_model.layers.31.self_attn.v_proj.input_scale', 'model.language_model.layers.31.self_attn.v_proj.output_scale', 'model.language_model.layers.31.self_attn.v_proj.weight._data._data', 'model.language_model.layers.31.self_attn.v_proj.weight._scale', 'model.language_model.layers.31.self_attn.v_proj.weight._shift', 'model.language_model.layers.32.mlp.down_proj.input_scale', 'model.language_model.layers.32.mlp.down_proj.output_scale', 'model.language_model.layers.32.mlp.down_proj.weight._data._data', 'model.language_model.layers.32.mlp.down_proj.weight._scale', 'model.language_model.layers.32.mlp.down_proj.weight._shift', 'model.language_model.layers.32.mlp.gate_proj.input_scale', 'model.language_model.layers.32.mlp.gate_proj.output_scale', 'model.language_model.layers.32.mlp.gate_proj.weight._data._data', 'model.language_model.layers.32.mlp.gate_proj.weight._scale', 'model.language_model.layers.32.mlp.gate_proj.weight._shift', 'model.language_model.layers.32.mlp.up_proj.input_scale', 'model.language_model.layers.32.mlp.up_proj.output_scale', 'model.language_model.layers.32.mlp.up_proj.weight._data._data', 'model.language_model.layers.32.mlp.up_proj.weight._scale', 'model.language_model.layers.32.mlp.up_proj.weight._shift', 'model.language_model.layers.32.self_attn.k_proj.input_scale', 'model.language_model.layers.32.self_attn.k_proj.output_scale', 'model.language_model.layers.32.self_attn.k_proj.weight._data._data', 'model.language_model.layers.32.self_attn.k_proj.weight._scale', 'model.language_model.layers.32.self_attn.k_proj.weight._shift', 'model.language_model.layers.32.self_attn.o_proj.input_scale', 'model.language_model.layers.32.self_attn.o_proj.output_scale', 'model.language_model.layers.32.self_attn.o_proj.weight._data._data', 'model.language_model.layers.32.self_attn.o_proj.weight._scale', 'model.language_model.layers.32.self_attn.o_proj.weight._shift', 'model.language_model.layers.32.self_attn.q_proj.input_scale', 'model.language_model.layers.32.self_attn.q_proj.output_scale', 'model.language_model.layers.32.self_attn.q_proj.weight._data._data', 'model.language_model.layers.32.self_attn.q_proj.weight._scale', 'model.language_model.layers.32.self_attn.q_proj.weight._shift', 'model.language_model.layers.32.self_attn.v_proj.input_scale', 'model.language_model.layers.32.self_attn.v_proj.output_scale', 'model.language_model.layers.32.self_attn.v_proj.weight._data._data', 'model.language_model.layers.32.self_attn.v_proj.weight._scale', 'model.language_model.layers.32.self_attn.v_proj.weight._shift', 'model.language_model.layers.33.mlp.down_proj.input_scale', 'model.language_model.layers.33.mlp.down_proj.output_scale', 'model.language_model.layers.33.mlp.down_proj.weight._data._data', 'model.language_model.layers.33.mlp.down_proj.weight._scale', 'model.language_model.layers.33.mlp.down_proj.weight._shift', 'model.language_model.layers.33.mlp.gate_proj.input_scale', 'model.language_model.layers.33.mlp.gate_proj.output_scale', 'model.language_model.layers.33.mlp.gate_proj.weight._data._data', 'model.language_model.layers.33.mlp.gate_proj.weight._scale', 'model.language_model.layers.33.mlp.gate_proj.weight._shift', 'model.language_model.layers.33.mlp.up_proj.input_scale', 'model.language_model.layers.33.mlp.up_proj.output_scale', 'model.language_model.layers.33.mlp.up_proj.weight._data._data', 'model.language_model.layers.33.mlp.up_proj.weight._scale', 'model.language_model.layers.33.mlp.up_proj.weight._shift', 'model.language_model.layers.33.self_attn.k_proj.input_scale', 'model.language_model.layers.33.self_attn.k_proj.output_scale', 'model.language_model.layers.33.self_attn.k_proj.weight._data._data', 'model.language_model.layers.33.self_attn.k_proj.weight._scale', 'model.language_model.layers.33.self_attn.k_proj.weight._shift', 'model.language_model.layers.33.self_attn.o_proj.input_scale', 'model.language_model.layers.33.self_attn.o_proj.output_scale', 'model.language_model.layers.33.self_attn.o_proj.weight._data._data', 'model.language_model.layers.33.self_attn.o_proj.weight._scale', 'model.language_model.layers.33.self_attn.o_proj.weight._shift', 'model.language_model.layers.33.self_attn.q_proj.input_scale', 'model.language_model.layers.33.self_attn.q_proj.output_scale', 'model.language_model.layers.33.self_attn.q_proj.weight._data._data', 'model.language_model.layers.33.self_attn.q_proj.weight._scale', 'model.language_model.layers.33.self_attn.q_proj.weight._shift', 'model.language_model.layers.33.self_attn.v_proj.input_scale', 'model.language_model.layers.33.self_attn.v_proj.output_scale', 'model.language_model.layers.33.self_attn.v_proj.weight._data._data', 'model.language_model.layers.33.self_attn.v_proj.weight._scale', 'model.language_model.layers.33.self_attn.v_proj.weight._shift', 'model.language_model.layers.34.mlp.down_proj.input_scale', 'model.language_model.layers.34.mlp.down_proj.output_scale', 'model.language_model.layers.34.mlp.down_proj.weight._data._data', 'model.language_model.layers.34.mlp.down_proj.weight._scale', 'model.language_model.layers.34.mlp.down_proj.weight._shift', 'model.language_model.layers.34.mlp.gate_proj.input_scale', 'model.language_model.layers.34.mlp.gate_proj.output_scale', 'model.language_model.layers.34.mlp.gate_proj.weight._data._data', 'model.language_model.layers.34.mlp.gate_proj.weight._scale', 'model.language_model.layers.34.mlp.gate_proj.weight._shift', 'model.language_model.layers.34.mlp.up_proj.input_scale', 'model.language_model.layers.34.mlp.up_proj.output_scale', 'model.language_model.layers.34.mlp.up_proj.weight._data._data', 'model.language_model.layers.34.mlp.up_proj.weight._scale', 'model.language_model.layers.34.mlp.up_proj.weight._shift', 'model.language_model.layers.34.self_attn.k_proj.input_scale', 'model.language_model.layers.34.self_attn.k_proj.output_scale', 'model.language_model.layers.34.self_attn.k_proj.weight._data._data', 'model.language_model.layers.34.self_attn.k_proj.weight._scale', 'model.language_model.layers.34.self_attn.k_proj.weight._shift', 'model.language_model.layers.34.self_attn.o_proj.input_scale', 'model.language_model.layers.34.self_attn.o_proj.output_scale', 'model.language_model.layers.34.self_attn.o_proj.weight._data._data', 'model.language_model.layers.34.self_attn.o_proj.weight._scale', 'model.language_model.layers.34.self_attn.o_proj.weight._shift', 'model.language_model.layers.34.self_attn.q_proj.input_scale', 'model.language_model.layers.34.self_attn.q_proj.output_scale', 'model.language_model.layers.34.self_attn.q_proj.weight._data._data', 'model.language_model.layers.34.self_attn.q_proj.weight._scale', 'model.language_model.layers.34.self_attn.q_proj.weight._shift', 'model.language_model.layers.34.self_attn.v_proj.input_scale', 'model.language_model.layers.34.self_attn.v_proj.output_scale', 'model.language_model.layers.34.self_attn.v_proj.weight._data._data', 'model.language_model.layers.34.self_attn.v_proj.weight._scale', 'model.language_model.layers.34.self_attn.v_proj.weight._shift', 'model.language_model.layers.35.mlp.down_proj.input_scale', 'model.language_model.layers.35.mlp.down_proj.output_scale', 'model.language_model.layers.35.mlp.down_proj.weight._data._data', 'model.language_model.layers.35.mlp.down_proj.weight._scale', 'model.language_model.layers.35.mlp.down_proj.weight._shift', 'model.language_model.layers.35.mlp.gate_proj.input_scale', 'model.language_model.layers.35.mlp.gate_proj.output_scale', 'model.language_model.layers.35.mlp.gate_proj.weight._data._data', 'model.language_model.layers.35.mlp.gate_proj.weight._scale', 'model.language_model.layers.35.mlp.gate_proj.weight._shift', 'model.language_model.layers.35.mlp.up_proj.input_scale', 'model.language_model.layers.35.mlp.up_proj.output_scale', 'model.language_model.layers.35.mlp.up_proj.weight._data._data', 'model.language_model.layers.35.mlp.up_proj.weight._scale', 'model.language_model.layers.35.mlp.up_proj.weight._shift', 'model.language_model.layers.35.self_attn.k_proj.input_scale', 'model.language_model.layers.35.self_attn.k_proj.output_scale', 'model.language_model.layers.35.self_attn.k_proj.weight._data._data', 'model.language_model.layers.35.self_attn.k_proj.weight._scale', 'model.language_model.layers.35.self_attn.k_proj.weight._shift', 'model.language_model.layers.35.self_attn.o_proj.input_scale', 'model.language_model.layers.35.self_attn.o_proj.output_scale', 'model.language_model.layers.35.self_attn.o_proj.weight._data._data', 'model.language_model.layers.35.self_attn.o_proj.weight._scale', 'model.language_model.layers.35.self_attn.o_proj.weight._shift', 'model.language_model.layers.35.self_attn.q_proj.input_scale', 'model.language_model.layers.35.self_attn.q_proj.output_scale', 'model.language_model.layers.35.self_attn.q_proj.weight._data._data', 'model.language_model.layers.35.self_attn.q_proj.weight._scale', 'model.language_model.layers.35.self_attn.q_proj.weight._shift', 'model.language_model.layers.35.self_attn.v_proj.input_scale', 'model.language_model.layers.35.self_attn.v_proj.output_scale', 'model.language_model.layers.35.self_attn.v_proj.weight._data._data', 'model.language_model.layers.35.self_attn.v_proj.weight._scale', 'model.language_model.layers.35.self_attn.v_proj.weight._shift', 'model.language_model.layers.36.mlp.down_proj.input_scale', 'model.language_model.layers.36.mlp.down_proj.output_scale', 'model.language_model.layers.36.mlp.down_proj.weight._data._data', 'model.language_model.layers.36.mlp.down_proj.weight._scale', 'model.language_model.layers.36.mlp.down_proj.weight._shift', 'model.language_model.layers.36.mlp.gate_proj.input_scale', 'model.language_model.layers.36.mlp.gate_proj.output_scale', 'model.language_model.layers.36.mlp.gate_proj.weight._data._data', 'model.language_model.layers.36.mlp.gate_proj.weight._scale', 'model.language_model.layers.36.mlp.gate_proj.weight._shift', 'model.language_model.layers.36.mlp.up_proj.input_scale', 'model.language_model.layers.36.mlp.up_proj.output_scale', 'model.language_model.layers.36.mlp.up_proj.weight._data._data', 'model.language_model.layers.36.mlp.up_proj.weight._scale', 'model.language_model.layers.36.mlp.up_proj.weight._shift', 'model.language_model.layers.36.self_attn.k_proj.input_scale', 'model.language_model.layers.36.self_attn.k_proj.output_scale', 'model.language_model.layers.36.self_attn.k_proj.weight._data._data', 'model.language_model.layers.36.self_attn.k_proj.weight._scale', 'model.language_model.layers.36.self_attn.k_proj.weight._shift', 'model.language_model.layers.36.self_attn.o_proj.input_scale', 'model.language_model.layers.36.self_attn.o_proj.output_scale', 'model.language_model.layers.36.self_attn.o_proj.weight._data._data', 'model.language_model.layers.36.self_attn.o_proj.weight._scale', 'model.language_model.layers.36.self_attn.o_proj.weight._shift', 'model.language_model.layers.36.self_attn.q_proj.input_scale', 'model.language_model.layers.36.self_attn.q_proj.output_scale', 'model.language_model.layers.36.self_attn.q_proj.weight._data._data', 'model.language_model.layers.36.self_attn.q_proj.weight._scale', 'model.language_model.layers.36.self_attn.q_proj.weight._shift', 'model.language_model.layers.36.self_attn.v_proj.input_scale', 'model.language_model.layers.36.self_attn.v_proj.output_scale', 'model.language_model.layers.36.self_attn.v_proj.weight._data._data', 'model.language_model.layers.36.self_attn.v_proj.weight._scale', 'model.language_model.layers.36.self_attn.v_proj.weight._shift', 'model.language_model.layers.37.mlp.down_proj.input_scale', 'model.language_model.layers.37.mlp.down_proj.output_scale', 'model.language_model.layers.37.mlp.down_proj.weight._data._data', 'model.language_model.layers.37.mlp.down_proj.weight._scale', 'model.language_model.layers.37.mlp.down_proj.weight._shift', 'model.language_model.layers.37.mlp.gate_proj.input_scale', 'model.language_model.layers.37.mlp.gate_proj.output_scale', 'model.language_model.layers.37.mlp.gate_proj.weight._data._data', 'model.language_model.layers.37.mlp.gate_proj.weight._scale', 'model.language_model.layers.37.mlp.gate_proj.weight._shift', 'model.language_model.layers.37.mlp.up_proj.input_scale', 'model.language_model.layers.37.mlp.up_proj.output_scale', 'model.language_model.layers.37.mlp.up_proj.weight._data._data', 'model.language_model.layers.37.mlp.up_proj.weight._scale', 'model.language_model.layers.37.mlp.up_proj.weight._shift', 'model.language_model.layers.37.self_attn.k_proj.input_scale', 'model.language_model.layers.37.self_attn.k_proj.output_scale', 'model.language_model.layers.37.self_attn.k_proj.weight._data._data', 'model.language_model.layers.37.self_attn.k_proj.weight._scale', 'model.language_model.layers.37.self_attn.k_proj.weight._shift', 'model.language_model.layers.37.self_attn.o_proj.input_scale', 'model.language_model.layers.37.self_attn.o_proj.output_scale', 'model.language_model.layers.37.self_attn.o_proj.weight._data._data', 'model.language_model.layers.37.self_attn.o_proj.weight._scale', 'model.language_model.layers.37.self_attn.o_proj.weight._shift', 'model.language_model.layers.37.self_attn.q_proj.input_scale', 'model.language_model.layers.37.self_attn.q_proj.output_scale', 'model.language_model.layers.37.self_attn.q_proj.weight._data._data', 'model.language_model.layers.37.self_attn.q_proj.weight._scale', 'model.language_model.layers.37.self_attn.q_proj.weight._shift', 'model.language_model.layers.37.self_attn.v_proj.input_scale', 'model.language_model.layers.37.self_attn.v_proj.output_scale', 'model.language_model.layers.37.self_attn.v_proj.weight._data._data', 'model.language_model.layers.37.self_attn.v_proj.weight._scale', 'model.language_model.layers.37.self_attn.v_proj.weight._shift', 'model.language_model.layers.38.mlp.down_proj.input_scale', 'model.language_model.layers.38.mlp.down_proj.output_scale', 'model.language_model.layers.38.mlp.down_proj.weight._data._data', 'model.language_model.layers.38.mlp.down_proj.weight._scale', 'model.language_model.layers.38.mlp.down_proj.weight._shift', 'model.language_model.layers.38.mlp.gate_proj.input_scale', 'model.language_model.layers.38.mlp.gate_proj.output_scale', 'model.language_model.layers.38.mlp.gate_proj.weight._data._data', 'model.language_model.layers.38.mlp.gate_proj.weight._scale', 'model.language_model.layers.38.mlp.gate_proj.weight._shift', 'model.language_model.layers.38.mlp.up_proj.input_scale', 'model.language_model.layers.38.mlp.up_proj.output_scale', 'model.language_model.layers.38.mlp.up_proj.weight._data._data', 'model.language_model.layers.38.mlp.up_proj.weight._scale', 'model.language_model.layers.38.mlp.up_proj.weight._shift', 'model.language_model.layers.38.self_attn.k_proj.input_scale', 'model.language_model.layers.38.self_attn.k_proj.output_scale', 'model.language_model.layers.38.self_attn.k_proj.weight._data._data', 'model.language_model.layers.38.self_attn.k_proj.weight._scale', 'model.language_model.layers.38.self_attn.k_proj.weight._shift', 'model.language_model.layers.38.self_attn.o_proj.input_scale', 'model.language_model.layers.38.self_attn.o_proj.output_scale', 'model.language_model.layers.38.self_attn.o_proj.weight._data._data', 'model.language_model.layers.38.self_attn.o_proj.weight._scale', 'model.language_model.layers.38.self_attn.o_proj.weight._shift', 'model.language_model.layers.38.self_attn.q_proj.input_scale', 'model.language_model.layers.38.self_attn.q_proj.output_scale', 'model.language_model.layers.38.self_attn.q_proj.weight._data._data', 'model.language_model.layers.38.self_attn.q_proj.weight._scale', 'model.language_model.layers.38.self_attn.q_proj.weight._shift', 'model.language_model.layers.38.self_attn.v_proj.input_scale', 'model.language_model.layers.38.self_attn.v_proj.output_scale', 'model.language_model.layers.38.self_attn.v_proj.weight._data._data', 'model.language_model.layers.38.self_attn.v_proj.weight._scale', 'model.language_model.layers.38.self_attn.v_proj.weight._shift', 'model.language_model.layers.39.mlp.down_proj.input_scale', 'model.language_model.layers.39.mlp.down_proj.output_scale', 'model.language_model.layers.39.mlp.down_proj.weight._data._data', 'model.language_model.layers.39.mlp.down_proj.weight._scale', 'model.language_model.layers.39.mlp.down_proj.weight._shift', 'model.language_model.layers.39.mlp.gate_proj.input_scale', 'model.language_model.layers.39.mlp.gate_proj.output_scale', 'model.language_model.layers.39.mlp.gate_proj.weight._data._data', 'model.language_model.layers.39.mlp.gate_proj.weight._scale', 'model.language_model.layers.39.mlp.gate_proj.weight._shift', 'model.language_model.layers.39.mlp.up_proj.input_scale', 'model.language_model.layers.39.mlp.up_proj.output_scale', 'model.language_model.layers.39.mlp.up_proj.weight._data._data', 'model.language_model.layers.39.mlp.up_proj.weight._scale', 'model.language_model.layers.39.mlp.up_proj.weight._shift', 'model.language_model.layers.39.self_attn.k_proj.input_scale', 'model.language_model.layers.39.self_attn.k_proj.output_scale', 'model.language_model.layers.39.self_attn.k_proj.weight._data._data', 'model.language_model.layers.39.self_attn.k_proj.weight._scale', 'model.language_model.layers.39.self_attn.k_proj.weight._shift', 'model.language_model.layers.39.self_attn.o_proj.input_scale', 'model.language_model.layers.39.self_attn.o_proj.output_scale', 'model.language_model.layers.39.self_attn.o_proj.weight._data._data', 'model.language_model.layers.39.self_attn.o_proj.weight._scale', 'model.language_model.layers.39.self_attn.o_proj.weight._shift', 'model.language_model.layers.39.self_attn.q_proj.input_scale', 'model.language_model.layers.39.self_attn.q_proj.output_scale', 'model.language_model.layers.39.self_attn.q_proj.weight._data._data', 'model.language_model.layers.39.self_attn.q_proj.weight._scale', 'model.language_model.layers.39.self_attn.q_proj.weight._shift', 'model.language_model.layers.39.self_attn.v_proj.input_scale', 'model.language_model.layers.39.self_attn.v_proj.output_scale', 'model.language_model.layers.39.self_attn.v_proj.weight._data._data', 'model.language_model.layers.39.self_attn.v_proj.weight._scale', 'model.language_model.layers.39.self_attn.v_proj.weight._shift', 'model.language_model.layers.4.mlp.down_proj.input_scale', 'model.language_model.layers.4.mlp.down_proj.output_scale', 'model.language_model.layers.4.mlp.down_proj.weight._data._data', 'model.language_model.layers.4.mlp.down_proj.weight._scale', 'model.language_model.layers.4.mlp.down_proj.weight._shift', 'model.language_model.layers.4.mlp.gate_proj.input_scale', 'model.language_model.layers.4.mlp.gate_proj.output_scale', 'model.language_model.layers.4.mlp.gate_proj.weight._data._data', 'model.language_model.layers.4.mlp.gate_proj.weight._scale', 'model.language_model.layers.4.mlp.gate_proj.weight._shift', 'model.language_model.layers.4.mlp.up_proj.input_scale', 'model.language_model.layers.4.mlp.up_proj.output_scale', 'model.language_model.layers.4.mlp.up_proj.weight._data._data', 'model.language_model.layers.4.mlp.up_proj.weight._scale', 'model.language_model.layers.4.mlp.up_proj.weight._shift', 'model.language_model.layers.4.self_attn.k_proj.input_scale', 'model.language_model.layers.4.self_attn.k_proj.output_scale', 'model.language_model.layers.4.self_attn.k_proj.weight._data._data', 'model.language_model.layers.4.self_attn.k_proj.weight._scale', 'model.language_model.layers.4.self_attn.k_proj.weight._shift', 'model.language_model.layers.4.self_attn.o_proj.input_scale', 'model.language_model.layers.4.self_attn.o_proj.output_scale', 'model.language_model.layers.4.self_attn.o_proj.weight._data._data', 'model.language_model.layers.4.self_attn.o_proj.weight._scale', 'model.language_model.layers.4.self_attn.o_proj.weight._shift', 'model.language_model.layers.4.self_attn.q_proj.input_scale', 'model.language_model.layers.4.self_attn.q_proj.output_scale', 'model.language_model.layers.4.self_attn.q_proj.weight._data._data', 'model.language_model.layers.4.self_attn.q_proj.weight._scale', 'model.language_model.layers.4.self_attn.q_proj.weight._shift', 'model.language_model.layers.4.self_attn.v_proj.input_scale', 'model.language_model.layers.4.self_attn.v_proj.output_scale', 'model.language_model.layers.4.self_attn.v_proj.weight._data._data', 'model.language_model.layers.4.self_attn.v_proj.weight._scale', 'model.language_model.layers.4.self_attn.v_proj.weight._shift', 'model.language_model.layers.40.mlp.down_proj.input_scale', 'model.language_model.layers.40.mlp.down_proj.output_scale', 'model.language_model.layers.40.mlp.down_proj.weight._data._data', 'model.language_model.layers.40.mlp.down_proj.weight._scale', 'model.language_model.layers.40.mlp.down_proj.weight._shift', 'model.language_model.layers.40.mlp.gate_proj.input_scale', 'model.language_model.layers.40.mlp.gate_proj.output_scale', 'model.language_model.layers.40.mlp.gate_proj.weight._data._data', 'model.language_model.layers.40.mlp.gate_proj.weight._scale', 'model.language_model.layers.40.mlp.gate_proj.weight._shift', 'model.language_model.layers.40.mlp.up_proj.input_scale', 'model.language_model.layers.40.mlp.up_proj.output_scale', 'model.language_model.layers.40.mlp.up_proj.weight._data._data', 'model.language_model.layers.40.mlp.up_proj.weight._scale', 'model.language_model.layers.40.mlp.up_proj.weight._shift', 'model.language_model.layers.40.self_attn.k_proj.input_scale', 'model.language_model.layers.40.self_attn.k_proj.output_scale', 'model.language_model.layers.40.self_attn.k_proj.weight._data._data', 'model.language_model.layers.40.self_attn.k_proj.weight._scale', 'model.language_model.layers.40.self_attn.k_proj.weight._shift', 'model.language_model.layers.40.self_attn.o_proj.input_scale', 'model.language_model.layers.40.self_attn.o_proj.output_scale', 'model.language_model.layers.40.self_attn.o_proj.weight._data._data', 'model.language_model.layers.40.self_attn.o_proj.weight._scale', 'model.language_model.layers.40.self_attn.o_proj.weight._shift', 'model.language_model.layers.40.self_attn.q_proj.input_scale', 'model.language_model.layers.40.self_attn.q_proj.output_scale', 'model.language_model.layers.40.self_attn.q_proj.weight._data._data', 'model.language_model.layers.40.self_attn.q_proj.weight._scale', 'model.language_model.layers.40.self_attn.q_proj.weight._shift', 'model.language_model.layers.40.self_attn.v_proj.input_scale', 'model.language_model.layers.40.self_attn.v_proj.output_scale', 'model.language_model.layers.40.self_attn.v_proj.weight._data._data', 'model.language_model.layers.40.self_attn.v_proj.weight._scale', 'model.language_model.layers.40.self_attn.v_proj.weight._shift', 'model.language_model.layers.41.mlp.down_proj.input_scale', 'model.language_model.layers.41.mlp.down_proj.output_scale', 'model.language_model.layers.41.mlp.down_proj.weight._data._data', 'model.language_model.layers.41.mlp.down_proj.weight._scale', 'model.language_model.layers.41.mlp.down_proj.weight._shift', 'model.language_model.layers.41.mlp.gate_proj.input_scale', 'model.language_model.layers.41.mlp.gate_proj.output_scale', 'model.language_model.layers.41.mlp.gate_proj.weight._data._data', 'model.language_model.layers.41.mlp.gate_proj.weight._scale', 'model.language_model.layers.41.mlp.gate_proj.weight._shift', 'model.language_model.layers.41.mlp.up_proj.input_scale', 'model.language_model.layers.41.mlp.up_proj.output_scale', 'model.language_model.layers.41.mlp.up_proj.weight._data._data', 'model.language_model.layers.41.mlp.up_proj.weight._scale', 'model.language_model.layers.41.mlp.up_proj.weight._shift', 'model.language_model.layers.41.self_attn.k_proj.input_scale', 'model.language_model.layers.41.self_attn.k_proj.output_scale', 'model.language_model.layers.41.self_attn.k_proj.weight._data._data', 'model.language_model.layers.41.self_attn.k_proj.weight._scale', 'model.language_model.layers.41.self_attn.k_proj.weight._shift', 'model.language_model.layers.41.self_attn.o_proj.input_scale', 'model.language_model.layers.41.self_attn.o_proj.output_scale', 'model.language_model.layers.41.self_attn.o_proj.weight._data._data', 'model.language_model.layers.41.self_attn.o_proj.weight._scale', 'model.language_model.layers.41.self_attn.o_proj.weight._shift', 'model.language_model.layers.41.self_attn.q_proj.input_scale', 'model.language_model.layers.41.self_attn.q_proj.output_scale', 'model.language_model.layers.41.self_attn.q_proj.weight._data._data', 'model.language_model.layers.41.self_attn.q_proj.weight._scale', 'model.language_model.layers.41.self_attn.q_proj.weight._shift', 'model.language_model.layers.41.self_attn.v_proj.input_scale', 'model.language_model.layers.41.self_attn.v_proj.output_scale', 'model.language_model.layers.41.self_attn.v_proj.weight._data._data', 'model.language_model.layers.41.self_attn.v_proj.weight._scale', 'model.language_model.layers.41.self_attn.v_proj.weight._shift', 'model.language_model.layers.42.mlp.down_proj.input_scale', 'model.language_model.layers.42.mlp.down_proj.output_scale', 'model.language_model.layers.42.mlp.down_proj.weight._data._data', 'model.language_model.layers.42.mlp.down_proj.weight._scale', 'model.language_model.layers.42.mlp.down_proj.weight._shift', 'model.language_model.layers.42.mlp.gate_proj.input_scale', 'model.language_model.layers.42.mlp.gate_proj.output_scale', 'model.language_model.layers.42.mlp.gate_proj.weight._data._data', 'model.language_model.layers.42.mlp.gate_proj.weight._scale', 'model.language_model.layers.42.mlp.gate_proj.weight._shift', 'model.language_model.layers.42.mlp.up_proj.input_scale', 'model.language_model.layers.42.mlp.up_proj.output_scale', 'model.language_model.layers.42.mlp.up_proj.weight._data._data', 'model.language_model.layers.42.mlp.up_proj.weight._scale', 'model.language_model.layers.42.mlp.up_proj.weight._shift', 'model.language_model.layers.42.self_attn.k_proj.input_scale', 'model.language_model.layers.42.self_attn.k_proj.output_scale', 'model.language_model.layers.42.self_attn.k_proj.weight._data._data', 'model.language_model.layers.42.self_attn.k_proj.weight._scale', 'model.language_model.layers.42.self_attn.k_proj.weight._shift', 'model.language_model.layers.42.self_attn.o_proj.input_scale', 'model.language_model.layers.42.self_attn.o_proj.output_scale', 'model.language_model.layers.42.self_attn.o_proj.weight._data._data', 'model.language_model.layers.42.self_attn.o_proj.weight._scale', 'model.language_model.layers.42.self_attn.o_proj.weight._shift', 'model.language_model.layers.42.self_attn.q_proj.input_scale', 'model.language_model.layers.42.self_attn.q_proj.output_scale', 'model.language_model.layers.42.self_attn.q_proj.weight._data._data', 'model.language_model.layers.42.self_attn.q_proj.weight._scale', 'model.language_model.layers.42.self_attn.q_proj.weight._shift', 'model.language_model.layers.42.self_attn.v_proj.input_scale', 'model.language_model.layers.42.self_attn.v_proj.output_scale', 'model.language_model.layers.42.self_attn.v_proj.weight._data._data', 'model.language_model.layers.42.self_attn.v_proj.weight._scale', 'model.language_model.layers.42.self_attn.v_proj.weight._shift', 'model.language_model.layers.43.mlp.down_proj.input_scale', 'model.language_model.layers.43.mlp.down_proj.output_scale', 'model.language_model.layers.43.mlp.down_proj.weight._data._data', 'model.language_model.layers.43.mlp.down_proj.weight._scale', 'model.language_model.layers.43.mlp.down_proj.weight._shift', 'model.language_model.layers.43.mlp.gate_proj.input_scale', 'model.language_model.layers.43.mlp.gate_proj.output_scale', 'model.language_model.layers.43.mlp.gate_proj.weight._data._data', 'model.language_model.layers.43.mlp.gate_proj.weight._scale', 'model.language_model.layers.43.mlp.gate_proj.weight._shift', 'model.language_model.layers.43.mlp.up_proj.input_scale', 'model.language_model.layers.43.mlp.up_proj.output_scale', 'model.language_model.layers.43.mlp.up_proj.weight._data._data', 'model.language_model.layers.43.mlp.up_proj.weight._scale', 'model.language_model.layers.43.mlp.up_proj.weight._shift', 'model.language_model.layers.43.self_attn.k_proj.input_scale', 'model.language_model.layers.43.self_attn.k_proj.output_scale', 'model.language_model.layers.43.self_attn.k_proj.weight._data._data', 'model.language_model.layers.43.self_attn.k_proj.weight._scale', 'model.language_model.layers.43.self_attn.k_proj.weight._shift', 'model.language_model.layers.43.self_attn.o_proj.input_scale', 'model.language_model.layers.43.self_attn.o_proj.output_scale', 'model.language_model.layers.43.self_attn.o_proj.weight._data._data', 'model.language_model.layers.43.self_attn.o_proj.weight._scale', 'model.language_model.layers.43.self_attn.o_proj.weight._shift', 'model.language_model.layers.43.self_attn.q_proj.input_scale', 'model.language_model.layers.43.self_attn.q_proj.output_scale', 'model.language_model.layers.43.self_attn.q_proj.weight._data._data', 'model.language_model.layers.43.self_attn.q_proj.weight._scale', 'model.language_model.layers.43.self_attn.q_proj.weight._shift', 'model.language_model.layers.43.self_attn.v_proj.input_scale', 'model.language_model.layers.43.self_attn.v_proj.output_scale', 'model.language_model.layers.43.self_attn.v_proj.weight._data._data', 'model.language_model.layers.43.self_attn.v_proj.weight._scale', 'model.language_model.layers.43.self_attn.v_proj.weight._shift', 'model.language_model.layers.44.mlp.down_proj.input_scale', 'model.language_model.layers.44.mlp.down_proj.output_scale', 'model.language_model.layers.44.mlp.down_proj.weight._data._data', 'model.language_model.layers.44.mlp.down_proj.weight._scale', 'model.language_model.layers.44.mlp.down_proj.weight._shift', 'model.language_model.layers.44.mlp.gate_proj.input_scale', 'model.language_model.layers.44.mlp.gate_proj.output_scale', 'model.language_model.layers.44.mlp.gate_proj.weight._data._data', 'model.language_model.layers.44.mlp.gate_proj.weight._scale', 'model.language_model.layers.44.mlp.gate_proj.weight._shift', 'model.language_model.layers.44.mlp.up_proj.input_scale', 'model.language_model.layers.44.mlp.up_proj.output_scale', 'model.language_model.layers.44.mlp.up_proj.weight._data._data', 'model.language_model.layers.44.mlp.up_proj.weight._scale', 'model.language_model.layers.44.mlp.up_proj.weight._shift', 'model.language_model.layers.44.self_attn.k_proj.input_scale', 'model.language_model.layers.44.self_attn.k_proj.output_scale', 'model.language_model.layers.44.self_attn.k_proj.weight._data._data', 'model.language_model.layers.44.self_attn.k_proj.weight._scale', 'model.language_model.layers.44.self_attn.k_proj.weight._shift', 'model.language_model.layers.44.self_attn.o_proj.input_scale', 'model.language_model.layers.44.self_attn.o_proj.output_scale', 'model.language_model.layers.44.self_attn.o_proj.weight._data._data', 'model.language_model.layers.44.self_attn.o_proj.weight._scale', 'model.language_model.layers.44.self_attn.o_proj.weight._shift', 'model.language_model.layers.44.self_attn.q_proj.input_scale', 'model.language_model.layers.44.self_attn.q_proj.output_scale', 'model.language_model.layers.44.self_attn.q_proj.weight._data._data', 'model.language_model.layers.44.self_attn.q_proj.weight._scale', 'model.language_model.layers.44.self_attn.q_proj.weight._shift', 'model.language_model.layers.44.self_attn.v_proj.input_scale', 'model.language_model.layers.44.self_attn.v_proj.output_scale', 'model.language_model.layers.44.self_attn.v_proj.weight._data._data', 'model.language_model.layers.44.self_attn.v_proj.weight._scale', 'model.language_model.layers.44.self_attn.v_proj.weight._shift', 'model.language_model.layers.45.mlp.down_proj.input_scale', 'model.language_model.layers.45.mlp.down_proj.output_scale', 'model.language_model.layers.45.mlp.down_proj.weight._data._data', 'model.language_model.layers.45.mlp.down_proj.weight._scale', 'model.language_model.layers.45.mlp.down_proj.weight._shift', 'model.language_model.layers.45.mlp.gate_proj.input_scale', 'model.language_model.layers.45.mlp.gate_proj.output_scale', 'model.language_model.layers.45.mlp.gate_proj.weight._data._data', 'model.language_model.layers.45.mlp.gate_proj.weight._scale', 'model.language_model.layers.45.mlp.gate_proj.weight._shift', 'model.language_model.layers.45.mlp.up_proj.input_scale', 'model.language_model.layers.45.mlp.up_proj.output_scale', 'model.language_model.layers.45.mlp.up_proj.weight._data._data', 'model.language_model.layers.45.mlp.up_proj.weight._scale', 'model.language_model.layers.45.mlp.up_proj.weight._shift', 'model.language_model.layers.45.self_attn.k_proj.input_scale', 'model.language_model.layers.45.self_attn.k_proj.output_scale', 'model.language_model.layers.45.self_attn.k_proj.weight._data._data', 'model.language_model.layers.45.self_attn.k_proj.weight._scale', 'model.language_model.layers.45.self_attn.k_proj.weight._shift', 'model.language_model.layers.45.self_attn.o_proj.input_scale', 'model.language_model.layers.45.self_attn.o_proj.output_scale', 'model.language_model.layers.45.self_attn.o_proj.weight._data._data', 'model.language_model.layers.45.self_attn.o_proj.weight._scale', 'model.language_model.layers.45.self_attn.o_proj.weight._shift', 'model.language_model.layers.45.self_attn.q_proj.input_scale', 'model.language_model.layers.45.self_attn.q_proj.output_scale', 'model.language_model.layers.45.self_attn.q_proj.weight._data._data', 'model.language_model.layers.45.self_attn.q_proj.weight._scale', 'model.language_model.layers.45.self_attn.q_proj.weight._shift', 'model.language_model.layers.45.self_attn.v_proj.input_scale', 'model.language_model.layers.45.self_attn.v_proj.output_scale', 'model.language_model.layers.45.self_attn.v_proj.weight._data._data', 'model.language_model.layers.45.self_attn.v_proj.weight._scale', 'model.language_model.layers.45.self_attn.v_proj.weight._shift', 'model.language_model.layers.46.mlp.down_proj.input_scale', 'model.language_model.layers.46.mlp.down_proj.output_scale', 'model.language_model.layers.46.mlp.down_proj.weight._data._data', 'model.language_model.layers.46.mlp.down_proj.weight._scale', 'model.language_model.layers.46.mlp.down_proj.weight._shift', 'model.language_model.layers.46.mlp.gate_proj.input_scale', 'model.language_model.layers.46.mlp.gate_proj.output_scale', 'model.language_model.layers.46.mlp.gate_proj.weight._data._data', 'model.language_model.layers.46.mlp.gate_proj.weight._scale', 'model.language_model.layers.46.mlp.gate_proj.weight._shift', 'model.language_model.layers.46.mlp.up_proj.input_scale', 'model.language_model.layers.46.mlp.up_proj.output_scale', 'model.language_model.layers.46.mlp.up_proj.weight._data._data', 'model.language_model.layers.46.mlp.up_proj.weight._scale', 'model.language_model.layers.46.mlp.up_proj.weight._shift', 'model.language_model.layers.46.self_attn.k_proj.input_scale', 'model.language_model.layers.46.self_attn.k_proj.output_scale', 'model.language_model.layers.46.self_attn.k_proj.weight._data._data', 'model.language_model.layers.46.self_attn.k_proj.weight._scale', 'model.language_model.layers.46.self_attn.k_proj.weight._shift', 'model.language_model.layers.46.self_attn.o_proj.input_scale', 'model.language_model.layers.46.self_attn.o_proj.output_scale', 'model.language_model.layers.46.self_attn.o_proj.weight._data._data', 'model.language_model.layers.46.self_attn.o_proj.weight._scale', 'model.language_model.layers.46.self_attn.o_proj.weight._shift', 'model.language_model.layers.46.self_attn.q_proj.input_scale', 'model.language_model.layers.46.self_attn.q_proj.output_scale', 'model.language_model.layers.46.self_attn.q_proj.weight._data._data', 'model.language_model.layers.46.self_attn.q_proj.weight._scale', 'model.language_model.layers.46.self_attn.q_proj.weight._shift', 'model.language_model.layers.46.self_attn.v_proj.input_scale', 'model.language_model.layers.46.self_attn.v_proj.output_scale', 'model.language_model.layers.46.self_attn.v_proj.weight._data._data', 'model.language_model.layers.46.self_attn.v_proj.weight._scale', 'model.language_model.layers.46.self_attn.v_proj.weight._shift', 'model.language_model.layers.47.mlp.down_proj.input_scale', 'model.language_model.layers.47.mlp.down_proj.output_scale', 'model.language_model.layers.47.mlp.down_proj.weight._data._data', 'model.language_model.layers.47.mlp.down_proj.weight._scale', 'model.language_model.layers.47.mlp.down_proj.weight._shift', 'model.language_model.layers.47.mlp.gate_proj.input_scale', 'model.language_model.layers.47.mlp.gate_proj.output_scale', 'model.language_model.layers.47.mlp.gate_proj.weight._data._data', 'model.language_model.layers.47.mlp.gate_proj.weight._scale', 'model.language_model.layers.47.mlp.gate_proj.weight._shift', 'model.language_model.layers.47.mlp.up_proj.input_scale', 'model.language_model.layers.47.mlp.up_proj.output_scale', 'model.language_model.layers.47.mlp.up_proj.weight._data._data', 'model.language_model.layers.47.mlp.up_proj.weight._scale', 'model.language_model.layers.47.mlp.up_proj.weight._shift', 'model.language_model.layers.47.self_attn.k_proj.input_scale', 'model.language_model.layers.47.self_attn.k_proj.output_scale', 'model.language_model.layers.47.self_attn.k_proj.weight._data._data', 'model.language_model.layers.47.self_attn.k_proj.weight._scale', 'model.language_model.layers.47.self_attn.k_proj.weight._shift', 'model.language_model.layers.47.self_attn.o_proj.input_scale', 'model.language_model.layers.47.self_attn.o_proj.output_scale', 'model.language_model.layers.47.self_attn.o_proj.weight._data._data', 'model.language_model.layers.47.self_attn.o_proj.weight._scale', 'model.language_model.layers.47.self_attn.o_proj.weight._shift', 'model.language_model.layers.47.self_attn.q_proj.input_scale', 'model.language_model.layers.47.self_attn.q_proj.output_scale', 'model.language_model.layers.47.self_attn.q_proj.weight._data._data', 'model.language_model.layers.47.self_attn.q_proj.weight._scale', 'model.language_model.layers.47.self_attn.q_proj.weight._shift', 'model.language_model.layers.47.self_attn.v_proj.input_scale', 'model.language_model.layers.47.self_attn.v_proj.output_scale', 'model.language_model.layers.47.self_attn.v_proj.weight._data._data', 'model.language_model.layers.47.self_attn.v_proj.weight._scale', 'model.language_model.layers.47.self_attn.v_proj.weight._shift', 'model.language_model.layers.5.mlp.down_proj.input_scale', 'model.language_model.layers.5.mlp.down_proj.output_scale', 'model.language_model.layers.5.mlp.down_proj.weight._data._data', 'model.language_model.layers.5.mlp.down_proj.weight._scale', 'model.language_model.layers.5.mlp.down_proj.weight._shift', 'model.language_model.layers.5.mlp.gate_proj.input_scale', 'model.language_model.layers.5.mlp.gate_proj.output_scale', 'model.language_model.layers.5.mlp.gate_proj.weight._data._data', 'model.language_model.layers.5.mlp.gate_proj.weight._scale', 'model.language_model.layers.5.mlp.gate_proj.weight._shift', 'model.language_model.layers.5.mlp.up_proj.input_scale', 'model.language_model.layers.5.mlp.up_proj.output_scale', 'model.language_model.layers.5.mlp.up_proj.weight._data._data', 'model.language_model.layers.5.mlp.up_proj.weight._scale', 'model.language_model.layers.5.mlp.up_proj.weight._shift', 'model.language_model.layers.5.self_attn.k_proj.input_scale', 'model.language_model.layers.5.self_attn.k_proj.output_scale', 'model.language_model.layers.5.self_attn.k_proj.weight._data._data', 'model.language_model.layers.5.self_attn.k_proj.weight._scale', 'model.language_model.layers.5.self_attn.k_proj.weight._shift', 'model.language_model.layers.5.self_attn.o_proj.input_scale', 'model.language_model.layers.5.self_attn.o_proj.output_scale', 'model.language_model.layers.5.self_attn.o_proj.weight._data._data', 'model.language_model.layers.5.self_attn.o_proj.weight._scale', 'model.language_model.layers.5.self_attn.o_proj.weight._shift', 'model.language_model.layers.5.self_attn.q_proj.input_scale', 'model.language_model.layers.5.self_attn.q_proj.output_scale', 'model.language_model.layers.5.self_attn.q_proj.weight._data._data', 'model.language_model.layers.5.self_attn.q_proj.weight._scale', 'model.language_model.layers.5.self_attn.q_proj.weight._shift', 'model.language_model.layers.5.self_attn.v_proj.input_scale', 'model.language_model.layers.5.self_attn.v_proj.output_scale', 'model.language_model.layers.5.self_attn.v_proj.weight._data._data', 'model.language_model.layers.5.self_attn.v_proj.weight._scale', 'model.language_model.layers.5.self_attn.v_proj.weight._shift', 'model.language_model.layers.6.mlp.down_proj.input_scale', 'model.language_model.layers.6.mlp.down_proj.output_scale', 'model.language_model.layers.6.mlp.down_proj.weight._data._data', 'model.language_model.layers.6.mlp.down_proj.weight._scale', 'model.language_model.layers.6.mlp.down_proj.weight._shift', 'model.language_model.layers.6.mlp.gate_proj.input_scale', 'model.language_model.layers.6.mlp.gate_proj.output_scale', 'model.language_model.layers.6.mlp.gate_proj.weight._data._data', 'model.language_model.layers.6.mlp.gate_proj.weight._scale', 'model.language_model.layers.6.mlp.gate_proj.weight._shift', 'model.language_model.layers.6.mlp.up_proj.input_scale', 'model.language_model.layers.6.mlp.up_proj.output_scale', 'model.language_model.layers.6.mlp.up_proj.weight._data._data', 'model.language_model.layers.6.mlp.up_proj.weight._scale', 'model.language_model.layers.6.mlp.up_proj.weight._shift', 'model.language_model.layers.6.self_attn.k_proj.input_scale', 'model.language_model.layers.6.self_attn.k_proj.output_scale', 'model.language_model.layers.6.self_attn.k_proj.weight._data._data', 'model.language_model.layers.6.self_attn.k_proj.weight._scale', 'model.language_model.layers.6.self_attn.k_proj.weight._shift', 'model.language_model.layers.6.self_attn.o_proj.input_scale', 'model.language_model.layers.6.self_attn.o_proj.output_scale', 'model.language_model.layers.6.self_attn.o_proj.weight._data._data', 'model.language_model.layers.6.self_attn.o_proj.weight._scale', 'model.language_model.layers.6.self_attn.o_proj.weight._shift', 'model.language_model.layers.6.self_attn.q_proj.input_scale', 'model.language_model.layers.6.self_attn.q_proj.output_scale', 'model.language_model.layers.6.self_attn.q_proj.weight._data._data', 'model.language_model.layers.6.self_attn.q_proj.weight._scale', 'model.language_model.layers.6.self_attn.q_proj.weight._shift', 'model.language_model.layers.6.self_attn.v_proj.input_scale', 'model.language_model.layers.6.self_attn.v_proj.output_scale', 'model.language_model.layers.6.self_attn.v_proj.weight._data._data', 'model.language_model.layers.6.self_attn.v_proj.weight._scale', 'model.language_model.layers.6.self_attn.v_proj.weight._shift', 'model.language_model.layers.7.mlp.down_proj.input_scale', 'model.language_model.layers.7.mlp.down_proj.output_scale', 'model.language_model.layers.7.mlp.down_proj.weight._data._data', 'model.language_model.layers.7.mlp.down_proj.weight._scale', 'model.language_model.layers.7.mlp.down_proj.weight._shift', 'model.language_model.layers.7.mlp.gate_proj.input_scale', 'model.language_model.layers.7.mlp.gate_proj.output_scale', 'model.language_model.layers.7.mlp.gate_proj.weight._data._data', 'model.language_model.layers.7.mlp.gate_proj.weight._scale', 'model.language_model.layers.7.mlp.gate_proj.weight._shift', 'model.language_model.layers.7.mlp.up_proj.input_scale', 'model.language_model.layers.7.mlp.up_proj.output_scale', 'model.language_model.layers.7.mlp.up_proj.weight._data._data', 'model.language_model.layers.7.mlp.up_proj.weight._scale', 'model.language_model.layers.7.mlp.up_proj.weight._shift', 'model.language_model.layers.7.self_attn.k_proj.input_scale', 'model.language_model.layers.7.self_attn.k_proj.output_scale', 'model.language_model.layers.7.self_attn.k_proj.weight._data._data', 'model.language_model.layers.7.self_attn.k_proj.weight._scale', 'model.language_model.layers.7.self_attn.k_proj.weight._shift', 'model.language_model.layers.7.self_attn.o_proj.input_scale', 'model.language_model.layers.7.self_attn.o_proj.output_scale', 'model.language_model.layers.7.self_attn.o_proj.weight._data._data', 'model.language_model.layers.7.self_attn.o_proj.weight._scale', 'model.language_model.layers.7.self_attn.o_proj.weight._shift', 'model.language_model.layers.7.self_attn.q_proj.input_scale', 'model.language_model.layers.7.self_attn.q_proj.output_scale', 'model.language_model.layers.7.self_attn.q_proj.weight._data._data', 'model.language_model.layers.7.self_attn.q_proj.weight._scale', 'model.language_model.layers.7.self_attn.q_proj.weight._shift', 'model.language_model.layers.7.self_attn.v_proj.input_scale', 'model.language_model.layers.7.self_attn.v_proj.output_scale', 'model.language_model.layers.7.self_attn.v_proj.weight._data._data', 'model.language_model.layers.7.self_attn.v_proj.weight._scale', 'model.language_model.layers.7.self_attn.v_proj.weight._shift', 'model.language_model.layers.8.mlp.down_proj.input_scale', 'model.language_model.layers.8.mlp.down_proj.output_scale', 'model.language_model.layers.8.mlp.down_proj.weight._data._data', 'model.language_model.layers.8.mlp.down_proj.weight._scale', 'model.language_model.layers.8.mlp.down_proj.weight._shift', 'model.language_model.layers.8.mlp.gate_proj.input_scale', 'model.language_model.layers.8.mlp.gate_proj.output_scale', 'model.language_model.layers.8.mlp.gate_proj.weight._data._data', 'model.language_model.layers.8.mlp.gate_proj.weight._scale', 'model.language_model.layers.8.mlp.gate_proj.weight._shift', 'model.language_model.layers.8.mlp.up_proj.input_scale', 'model.language_model.layers.8.mlp.up_proj.output_scale', 'model.language_model.layers.8.mlp.up_proj.weight._data._data', 'model.language_model.layers.8.mlp.up_proj.weight._scale', 'model.language_model.layers.8.mlp.up_proj.weight._shift', 'model.language_model.layers.8.self_attn.k_proj.input_scale', 'model.language_model.layers.8.self_attn.k_proj.output_scale', 'model.language_model.layers.8.self_attn.k_proj.weight._data._data', 'model.language_model.layers.8.self_attn.k_proj.weight._scale', 'model.language_model.layers.8.self_attn.k_proj.weight._shift', 'model.language_model.layers.8.self_attn.o_proj.input_scale', 'model.language_model.layers.8.self_attn.o_proj.output_scale', 'model.language_model.layers.8.self_attn.o_proj.weight._data._data', 'model.language_model.layers.8.self_attn.o_proj.weight._scale', 'model.language_model.layers.8.self_attn.o_proj.weight._shift', 'model.language_model.layers.8.self_attn.q_proj.input_scale', 'model.language_model.layers.8.self_attn.q_proj.output_scale', 'model.language_model.layers.8.self_attn.q_proj.weight._data._data', 'model.language_model.layers.8.self_attn.q_proj.weight._scale', 'model.language_model.layers.8.self_attn.q_proj.weight._shift', 'model.language_model.layers.8.self_attn.v_proj.input_scale', 'model.language_model.layers.8.self_attn.v_proj.output_scale', 'model.language_model.layers.8.self_attn.v_proj.weight._data._data', 'model.language_model.layers.8.self_attn.v_proj.weight._scale', 'model.language_model.layers.8.self_attn.v_proj.weight._shift', 'model.language_model.layers.9.mlp.down_proj.input_scale', 'model.language_model.layers.9.mlp.down_proj.output_scale', 'model.language_model.layers.9.mlp.down_proj.weight._data._data', 'model.language_model.layers.9.mlp.down_proj.weight._scale', 'model.language_model.layers.9.mlp.down_proj.weight._shift', 'model.language_model.layers.9.mlp.gate_proj.input_scale', 'model.language_model.layers.9.mlp.gate_proj.output_scale', 'model.language_model.layers.9.mlp.gate_proj.weight._data._data', 'model.language_model.layers.9.mlp.gate_proj.weight._scale', 'model.language_model.layers.9.mlp.gate_proj.weight._shift', 'model.language_model.layers.9.mlp.up_proj.input_scale', 'model.language_model.layers.9.mlp.up_proj.output_scale', 'model.language_model.layers.9.mlp.up_proj.weight._data._data', 'model.language_model.layers.9.mlp.up_proj.weight._scale', 'model.language_model.layers.9.mlp.up_proj.weight._shift', 'model.language_model.layers.9.self_attn.k_proj.input_scale', 'model.language_model.layers.9.self_attn.k_proj.output_scale', 'model.language_model.layers.9.self_attn.k_proj.weight._data._data', 'model.language_model.layers.9.self_attn.k_proj.weight._scale', 'model.language_model.layers.9.self_attn.k_proj.weight._shift', 'model.language_model.layers.9.self_attn.o_proj.input_scale', 'model.language_model.layers.9.self_attn.o_proj.output_scale', 'model.language_model.layers.9.self_attn.o_proj.weight._data._data', 'model.language_model.layers.9.self_attn.o_proj.weight._scale', 'model.language_model.layers.9.self_attn.o_proj.weight._shift', 'model.language_model.layers.9.self_attn.q_proj.input_scale', 'model.language_model.layers.9.self_attn.q_proj.output_scale', 'model.language_model.layers.9.self_attn.q_proj.weight._data._data', 'model.language_model.layers.9.self_attn.q_proj.weight._scale', 'model.language_model.layers.9.self_attn.q_proj.weight._shift', 'model.language_model.layers.9.self_attn.v_proj.input_scale', 'model.language_model.layers.9.self_attn.v_proj.output_scale', 'model.language_model.layers.9.self_attn.v_proj.weight._data._data', 'model.language_model.layers.9.self_attn.v_proj.weight._scale', 'model.language_model.layers.9.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.embeddings.patch_embedding.input_scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.output_scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._data._data', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._scale', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight._shift', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.input_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.output_scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._data._data', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._scale', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight._shift']\n",
            "- This IS expected if you are initializing Gemma3ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Gemma3ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Gemma3ForConditionalGeneration were not initialized from the model checkpoint at ./rps-gemma2-quantized and are newly initialized: ['model.language_model.layers.0.mlp.down_proj.weight', 'model.language_model.layers.0.mlp.gate_proj.weight', 'model.language_model.layers.0.mlp.up_proj.weight', 'model.language_model.layers.0.self_attn.k_proj.weight', 'model.language_model.layers.0.self_attn.o_proj.weight', 'model.language_model.layers.0.self_attn.q_proj.weight', 'model.language_model.layers.0.self_attn.v_proj.weight', 'model.language_model.layers.1.mlp.down_proj.weight', 'model.language_model.layers.1.mlp.gate_proj.weight', 'model.language_model.layers.1.mlp.up_proj.weight', 'model.language_model.layers.1.self_attn.k_proj.weight', 'model.language_model.layers.1.self_attn.o_proj.weight', 'model.language_model.layers.1.self_attn.q_proj.weight', 'model.language_model.layers.1.self_attn.v_proj.weight', 'model.language_model.layers.10.mlp.down_proj.weight', 'model.language_model.layers.10.mlp.gate_proj.weight', 'model.language_model.layers.10.mlp.up_proj.weight', 'model.language_model.layers.10.self_attn.k_proj.weight', 'model.language_model.layers.10.self_attn.o_proj.weight', 'model.language_model.layers.10.self_attn.q_proj.weight', 'model.language_model.layers.10.self_attn.v_proj.weight', 'model.language_model.layers.11.mlp.down_proj.weight', 'model.language_model.layers.11.mlp.gate_proj.weight', 'model.language_model.layers.11.mlp.up_proj.weight', 'model.language_model.layers.11.self_attn.k_proj.weight', 'model.language_model.layers.11.self_attn.o_proj.weight', 'model.language_model.layers.11.self_attn.q_proj.weight', 'model.language_model.layers.11.self_attn.v_proj.weight', 'model.language_model.layers.12.mlp.down_proj.weight', 'model.language_model.layers.12.mlp.gate_proj.weight', 'model.language_model.layers.12.mlp.up_proj.weight', 'model.language_model.layers.12.self_attn.k_proj.weight', 'model.language_model.layers.12.self_attn.o_proj.weight', 'model.language_model.layers.12.self_attn.q_proj.weight', 'model.language_model.layers.12.self_attn.v_proj.weight', 'model.language_model.layers.13.mlp.down_proj.weight', 'model.language_model.layers.13.mlp.gate_proj.weight', 'model.language_model.layers.13.mlp.up_proj.weight', 'model.language_model.layers.13.self_attn.k_proj.weight', 'model.language_model.layers.13.self_attn.o_proj.weight', 'model.language_model.layers.13.self_attn.q_proj.weight', 'model.language_model.layers.13.self_attn.v_proj.weight', 'model.language_model.layers.14.mlp.down_proj.weight', 'model.language_model.layers.14.mlp.gate_proj.weight', 'model.language_model.layers.14.mlp.up_proj.weight', 'model.language_model.layers.14.self_attn.k_proj.weight', 'model.language_model.layers.14.self_attn.o_proj.weight', 'model.language_model.layers.14.self_attn.q_proj.weight', 'model.language_model.layers.14.self_attn.v_proj.weight', 'model.language_model.layers.15.mlp.down_proj.weight', 'model.language_model.layers.15.mlp.gate_proj.weight', 'model.language_model.layers.15.mlp.up_proj.weight', 'model.language_model.layers.15.self_attn.k_proj.weight', 'model.language_model.layers.15.self_attn.o_proj.weight', 'model.language_model.layers.15.self_attn.q_proj.weight', 'model.language_model.layers.15.self_attn.v_proj.weight', 'model.language_model.layers.16.mlp.down_proj.weight', 'model.language_model.layers.16.mlp.gate_proj.weight', 'model.language_model.layers.16.mlp.up_proj.weight', 'model.language_model.layers.16.self_attn.k_proj.weight', 'model.language_model.layers.16.self_attn.o_proj.weight', 'model.language_model.layers.16.self_attn.q_proj.weight', 'model.language_model.layers.16.self_attn.v_proj.weight', 'model.language_model.layers.17.mlp.down_proj.weight', 'model.language_model.layers.17.mlp.gate_proj.weight', 'model.language_model.layers.17.mlp.up_proj.weight', 'model.language_model.layers.17.self_attn.k_proj.weight', 'model.language_model.layers.17.self_attn.o_proj.weight', 'model.language_model.layers.17.self_attn.q_proj.weight', 'model.language_model.layers.17.self_attn.v_proj.weight', 'model.language_model.layers.18.mlp.down_proj.weight', 'model.language_model.layers.18.mlp.gate_proj.weight', 'model.language_model.layers.18.mlp.up_proj.weight', 'model.language_model.layers.18.self_attn.k_proj.weight', 'model.language_model.layers.18.self_attn.o_proj.weight', 'model.language_model.layers.18.self_attn.q_proj.weight', 'model.language_model.layers.18.self_attn.v_proj.weight', 'model.language_model.layers.19.mlp.down_proj.weight', 'model.language_model.layers.19.mlp.gate_proj.weight', 'model.language_model.layers.19.mlp.up_proj.weight', 'model.language_model.layers.19.self_attn.k_proj.weight', 'model.language_model.layers.19.self_attn.o_proj.weight', 'model.language_model.layers.19.self_attn.q_proj.weight', 'model.language_model.layers.19.self_attn.v_proj.weight', 'model.language_model.layers.2.mlp.down_proj.weight', 'model.language_model.layers.2.mlp.gate_proj.weight', 'model.language_model.layers.2.mlp.up_proj.weight', 'model.language_model.layers.2.self_attn.k_proj.weight', 'model.language_model.layers.2.self_attn.o_proj.weight', 'model.language_model.layers.2.self_attn.q_proj.weight', 'model.language_model.layers.2.self_attn.v_proj.weight', 'model.language_model.layers.20.mlp.down_proj.weight', 'model.language_model.layers.20.mlp.gate_proj.weight', 'model.language_model.layers.20.mlp.up_proj.weight', 'model.language_model.layers.20.self_attn.k_proj.weight', 'model.language_model.layers.20.self_attn.o_proj.weight', 'model.language_model.layers.20.self_attn.q_proj.weight', 'model.language_model.layers.20.self_attn.v_proj.weight', 'model.language_model.layers.21.mlp.down_proj.weight', 'model.language_model.layers.21.mlp.gate_proj.weight', 'model.language_model.layers.21.mlp.up_proj.weight', 'model.language_model.layers.21.self_attn.k_proj.weight', 'model.language_model.layers.21.self_attn.o_proj.weight', 'model.language_model.layers.21.self_attn.q_proj.weight', 'model.language_model.layers.21.self_attn.v_proj.weight', 'model.language_model.layers.22.mlp.down_proj.weight', 'model.language_model.layers.22.mlp.gate_proj.weight', 'model.language_model.layers.22.mlp.up_proj.weight', 'model.language_model.layers.22.self_attn.k_proj.weight', 'model.language_model.layers.22.self_attn.o_proj.weight', 'model.language_model.layers.22.self_attn.q_proj.weight', 'model.language_model.layers.22.self_attn.v_proj.weight', 'model.language_model.layers.23.mlp.down_proj.weight', 'model.language_model.layers.23.mlp.gate_proj.weight', 'model.language_model.layers.23.mlp.up_proj.weight', 'model.language_model.layers.23.self_attn.k_proj.weight', 'model.language_model.layers.23.self_attn.o_proj.weight', 'model.language_model.layers.23.self_attn.q_proj.weight', 'model.language_model.layers.23.self_attn.v_proj.weight', 'model.language_model.layers.24.mlp.down_proj.weight', 'model.language_model.layers.24.mlp.gate_proj.weight', 'model.language_model.layers.24.mlp.up_proj.weight', 'model.language_model.layers.24.self_attn.k_proj.weight', 'model.language_model.layers.24.self_attn.o_proj.weight', 'model.language_model.layers.24.self_attn.q_proj.weight', 'model.language_model.layers.24.self_attn.v_proj.weight', 'model.language_model.layers.25.mlp.down_proj.weight', 'model.language_model.layers.25.mlp.gate_proj.weight', 'model.language_model.layers.25.mlp.up_proj.weight', 'model.language_model.layers.25.self_attn.k_proj.weight', 'model.language_model.layers.25.self_attn.o_proj.weight', 'model.language_model.layers.25.self_attn.q_proj.weight', 'model.language_model.layers.25.self_attn.v_proj.weight', 'model.language_model.layers.26.mlp.down_proj.weight', 'model.language_model.layers.26.mlp.gate_proj.weight', 'model.language_model.layers.26.mlp.up_proj.weight', 'model.language_model.layers.26.self_attn.k_proj.weight', 'model.language_model.layers.26.self_attn.o_proj.weight', 'model.language_model.layers.26.self_attn.q_proj.weight', 'model.language_model.layers.26.self_attn.v_proj.weight', 'model.language_model.layers.27.mlp.down_proj.weight', 'model.language_model.layers.27.mlp.gate_proj.weight', 'model.language_model.layers.27.mlp.up_proj.weight', 'model.language_model.layers.27.self_attn.k_proj.weight', 'model.language_model.layers.27.self_attn.o_proj.weight', 'model.language_model.layers.27.self_attn.q_proj.weight', 'model.language_model.layers.27.self_attn.v_proj.weight', 'model.language_model.layers.28.mlp.down_proj.weight', 'model.language_model.layers.28.mlp.gate_proj.weight', 'model.language_model.layers.28.mlp.up_proj.weight', 'model.language_model.layers.28.self_attn.k_proj.weight', 'model.language_model.layers.28.self_attn.o_proj.weight', 'model.language_model.layers.28.self_attn.q_proj.weight', 'model.language_model.layers.28.self_attn.v_proj.weight', 'model.language_model.layers.29.mlp.down_proj.weight', 'model.language_model.layers.29.mlp.gate_proj.weight', 'model.language_model.layers.29.mlp.up_proj.weight', 'model.language_model.layers.29.self_attn.k_proj.weight', 'model.language_model.layers.29.self_attn.o_proj.weight', 'model.language_model.layers.29.self_attn.q_proj.weight', 'model.language_model.layers.29.self_attn.v_proj.weight', 'model.language_model.layers.3.mlp.down_proj.weight', 'model.language_model.layers.3.mlp.gate_proj.weight', 'model.language_model.layers.3.mlp.up_proj.weight', 'model.language_model.layers.3.self_attn.k_proj.weight', 'model.language_model.layers.3.self_attn.o_proj.weight', 'model.language_model.layers.3.self_attn.q_proj.weight', 'model.language_model.layers.3.self_attn.v_proj.weight', 'model.language_model.layers.30.mlp.down_proj.weight', 'model.language_model.layers.30.mlp.gate_proj.weight', 'model.language_model.layers.30.mlp.up_proj.weight', 'model.language_model.layers.30.self_attn.k_proj.weight', 'model.language_model.layers.30.self_attn.o_proj.weight', 'model.language_model.layers.30.self_attn.q_proj.weight', 'model.language_model.layers.30.self_attn.v_proj.weight', 'model.language_model.layers.31.mlp.down_proj.weight', 'model.language_model.layers.31.mlp.gate_proj.weight', 'model.language_model.layers.31.mlp.up_proj.weight', 'model.language_model.layers.31.self_attn.k_proj.weight', 'model.language_model.layers.31.self_attn.o_proj.weight', 'model.language_model.layers.31.self_attn.q_proj.weight', 'model.language_model.layers.31.self_attn.v_proj.weight', 'model.language_model.layers.32.mlp.down_proj.weight', 'model.language_model.layers.32.mlp.gate_proj.weight', 'model.language_model.layers.32.mlp.up_proj.weight', 'model.language_model.layers.32.self_attn.k_proj.weight', 'model.language_model.layers.32.self_attn.o_proj.weight', 'model.language_model.layers.32.self_attn.q_proj.weight', 'model.language_model.layers.32.self_attn.v_proj.weight', 'model.language_model.layers.33.mlp.down_proj.weight', 'model.language_model.layers.33.mlp.gate_proj.weight', 'model.language_model.layers.33.mlp.up_proj.weight', 'model.language_model.layers.33.self_attn.k_proj.weight', 'model.language_model.layers.33.self_attn.o_proj.weight', 'model.language_model.layers.33.self_attn.q_proj.weight', 'model.language_model.layers.33.self_attn.v_proj.weight', 'model.language_model.layers.34.mlp.down_proj.weight', 'model.language_model.layers.34.mlp.gate_proj.weight', 'model.language_model.layers.34.mlp.up_proj.weight', 'model.language_model.layers.34.self_attn.k_proj.weight', 'model.language_model.layers.34.self_attn.o_proj.weight', 'model.language_model.layers.34.self_attn.q_proj.weight', 'model.language_model.layers.34.self_attn.v_proj.weight', 'model.language_model.layers.35.mlp.down_proj.weight', 'model.language_model.layers.35.mlp.gate_proj.weight', 'model.language_model.layers.35.mlp.up_proj.weight', 'model.language_model.layers.35.self_attn.k_proj.weight', 'model.language_model.layers.35.self_attn.o_proj.weight', 'model.language_model.layers.35.self_attn.q_proj.weight', 'model.language_model.layers.35.self_attn.v_proj.weight', 'model.language_model.layers.36.mlp.down_proj.weight', 'model.language_model.layers.36.mlp.gate_proj.weight', 'model.language_model.layers.36.mlp.up_proj.weight', 'model.language_model.layers.36.self_attn.k_proj.weight', 'model.language_model.layers.36.self_attn.o_proj.weight', 'model.language_model.layers.36.self_attn.q_proj.weight', 'model.language_model.layers.36.self_attn.v_proj.weight', 'model.language_model.layers.37.mlp.down_proj.weight', 'model.language_model.layers.37.mlp.gate_proj.weight', 'model.language_model.layers.37.mlp.up_proj.weight', 'model.language_model.layers.37.self_attn.k_proj.weight', 'model.language_model.layers.37.self_attn.o_proj.weight', 'model.language_model.layers.37.self_attn.q_proj.weight', 'model.language_model.layers.37.self_attn.v_proj.weight', 'model.language_model.layers.38.mlp.down_proj.weight', 'model.language_model.layers.38.mlp.gate_proj.weight', 'model.language_model.layers.38.mlp.up_proj.weight', 'model.language_model.layers.38.self_attn.k_proj.weight', 'model.language_model.layers.38.self_attn.o_proj.weight', 'model.language_model.layers.38.self_attn.q_proj.weight', 'model.language_model.layers.38.self_attn.v_proj.weight', 'model.language_model.layers.39.mlp.down_proj.weight', 'model.language_model.layers.39.mlp.gate_proj.weight', 'model.language_model.layers.39.mlp.up_proj.weight', 'model.language_model.layers.39.self_attn.k_proj.weight', 'model.language_model.layers.39.self_attn.o_proj.weight', 'model.language_model.layers.39.self_attn.q_proj.weight', 'model.language_model.layers.39.self_attn.v_proj.weight', 'model.language_model.layers.4.mlp.down_proj.weight', 'model.language_model.layers.4.mlp.gate_proj.weight', 'model.language_model.layers.4.mlp.up_proj.weight', 'model.language_model.layers.4.self_attn.k_proj.weight', 'model.language_model.layers.4.self_attn.o_proj.weight', 'model.language_model.layers.4.self_attn.q_proj.weight', 'model.language_model.layers.4.self_attn.v_proj.weight', 'model.language_model.layers.40.mlp.down_proj.weight', 'model.language_model.layers.40.mlp.gate_proj.weight', 'model.language_model.layers.40.mlp.up_proj.weight', 'model.language_model.layers.40.self_attn.k_proj.weight', 'model.language_model.layers.40.self_attn.o_proj.weight', 'model.language_model.layers.40.self_attn.q_proj.weight', 'model.language_model.layers.40.self_attn.v_proj.weight', 'model.language_model.layers.41.mlp.down_proj.weight', 'model.language_model.layers.41.mlp.gate_proj.weight', 'model.language_model.layers.41.mlp.up_proj.weight', 'model.language_model.layers.41.self_attn.k_proj.weight', 'model.language_model.layers.41.self_attn.o_proj.weight', 'model.language_model.layers.41.self_attn.q_proj.weight', 'model.language_model.layers.41.self_attn.v_proj.weight', 'model.language_model.layers.42.mlp.down_proj.weight', 'model.language_model.layers.42.mlp.gate_proj.weight', 'model.language_model.layers.42.mlp.up_proj.weight', 'model.language_model.layers.42.self_attn.k_proj.weight', 'model.language_model.layers.42.self_attn.o_proj.weight', 'model.language_model.layers.42.self_attn.q_proj.weight', 'model.language_model.layers.42.self_attn.v_proj.weight', 'model.language_model.layers.43.mlp.down_proj.weight', 'model.language_model.layers.43.mlp.gate_proj.weight', 'model.language_model.layers.43.mlp.up_proj.weight', 'model.language_model.layers.43.self_attn.k_proj.weight', 'model.language_model.layers.43.self_attn.o_proj.weight', 'model.language_model.layers.43.self_attn.q_proj.weight', 'model.language_model.layers.43.self_attn.v_proj.weight', 'model.language_model.layers.44.mlp.down_proj.weight', 'model.language_model.layers.44.mlp.gate_proj.weight', 'model.language_model.layers.44.mlp.up_proj.weight', 'model.language_model.layers.44.self_attn.k_proj.weight', 'model.language_model.layers.44.self_attn.o_proj.weight', 'model.language_model.layers.44.self_attn.q_proj.weight', 'model.language_model.layers.44.self_attn.v_proj.weight', 'model.language_model.layers.45.mlp.down_proj.weight', 'model.language_model.layers.45.mlp.gate_proj.weight', 'model.language_model.layers.45.mlp.up_proj.weight', 'model.language_model.layers.45.self_attn.k_proj.weight', 'model.language_model.layers.45.self_attn.o_proj.weight', 'model.language_model.layers.45.self_attn.q_proj.weight', 'model.language_model.layers.45.self_attn.v_proj.weight', 'model.language_model.layers.46.mlp.down_proj.weight', 'model.language_model.layers.46.mlp.gate_proj.weight', 'model.language_model.layers.46.mlp.up_proj.weight', 'model.language_model.layers.46.self_attn.k_proj.weight', 'model.language_model.layers.46.self_attn.o_proj.weight', 'model.language_model.layers.46.self_attn.q_proj.weight', 'model.language_model.layers.46.self_attn.v_proj.weight', 'model.language_model.layers.47.mlp.down_proj.weight', 'model.language_model.layers.47.mlp.gate_proj.weight', 'model.language_model.layers.47.mlp.up_proj.weight', 'model.language_model.layers.47.self_attn.k_proj.weight', 'model.language_model.layers.47.self_attn.o_proj.weight', 'model.language_model.layers.47.self_attn.q_proj.weight', 'model.language_model.layers.47.self_attn.v_proj.weight', 'model.language_model.layers.5.mlp.down_proj.weight', 'model.language_model.layers.5.mlp.gate_proj.weight', 'model.language_model.layers.5.mlp.up_proj.weight', 'model.language_model.layers.5.self_attn.k_proj.weight', 'model.language_model.layers.5.self_attn.o_proj.weight', 'model.language_model.layers.5.self_attn.q_proj.weight', 'model.language_model.layers.5.self_attn.v_proj.weight', 'model.language_model.layers.6.mlp.down_proj.weight', 'model.language_model.layers.6.mlp.gate_proj.weight', 'model.language_model.layers.6.mlp.up_proj.weight', 'model.language_model.layers.6.self_attn.k_proj.weight', 'model.language_model.layers.6.self_attn.o_proj.weight', 'model.language_model.layers.6.self_attn.q_proj.weight', 'model.language_model.layers.6.self_attn.v_proj.weight', 'model.language_model.layers.7.mlp.down_proj.weight', 'model.language_model.layers.7.mlp.gate_proj.weight', 'model.language_model.layers.7.mlp.up_proj.weight', 'model.language_model.layers.7.self_attn.k_proj.weight', 'model.language_model.layers.7.self_attn.o_proj.weight', 'model.language_model.layers.7.self_attn.q_proj.weight', 'model.language_model.layers.7.self_attn.v_proj.weight', 'model.language_model.layers.8.mlp.down_proj.weight', 'model.language_model.layers.8.mlp.gate_proj.weight', 'model.language_model.layers.8.mlp.up_proj.weight', 'model.language_model.layers.8.self_attn.k_proj.weight', 'model.language_model.layers.8.self_attn.o_proj.weight', 'model.language_model.layers.8.self_attn.q_proj.weight', 'model.language_model.layers.8.self_attn.v_proj.weight', 'model.language_model.layers.9.mlp.down_proj.weight', 'model.language_model.layers.9.mlp.gate_proj.weight', 'model.language_model.layers.9.mlp.up_proj.weight', 'model.language_model.layers.9.self_attn.k_proj.weight', 'model.language_model.layers.9.self_attn.o_proj.weight', 'model.language_model.layers.9.self_attn.q_proj.weight', 'model.language_model.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 20.88 MiB is free. Process 274569 has 39.53 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 231.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4035039110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m game_pipeline = pipeline(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use the in-memory quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         self.check_model_type(\n\u001b[1;32m    118\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         ):\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;31m# If it's a generation pipeline and the model can generate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m                 )\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 20.88 MiB is free. Process 274569 has 39.53 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 231.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The pruner has a `prune_heads` method that takes a dictionary\n",
        "# The dictionary maps layer index to a list of head indices to prune\n",
        "# Let's define a function to find the least important ones\n",
        "def get_heads_to_prune(head_importance, amount=0.2):\n",
        "    # This is a simplified logic to get the bottom `amount` of heads\n",
        "    # `head_importance` is a tensor; we find the indices of the smallest values\n",
        "    num_heads_to_prune = int(head_importance.numel() * amount)\n",
        "    # Get the indices of the heads with the lowest scores\n",
        "    _, indices_to_prune = torch.topk(head_importance.view(-1), k=num_heads_to_prune, largest=False)\n",
        "    return indices_to_prune\n",
        "\n",
        "# Get the heads we want to remove\n",
        "heads_to_prune_indices = get_heads_to_prune(head_importance, amount=0.2)\n",
        "\n",
        "# The pruner's `prune_heads` method does the actual work\n",
        "# This method needs to be called with the specific head indices per layer\n",
        "# For simplicity, we assume `pruner.prune_by_importance` can be used\n",
        "# (Note: actual implementation might require mapping flat indices back to layer/head indices)\n",
        "\n",
        "print(f\"Pruning {len(heads_to_prune_indices)} heads...\")\n",
        "pruner.prune_by_importance(n_heads_to_prune=len(heads_to_prune_indices))\n",
        "\n",
        "print(\"Model has been pruned.\")"
      ],
      "metadata": {
        "id": "kAZdgutYCbTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}